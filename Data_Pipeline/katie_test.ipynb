{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4e2c18-cdd4-4603-b3ad-0b2f4dee2815",
   "metadata": {},
   "source": [
    "### testing fill missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8927dcb9-c531-4aad-826d-17b3c7c09655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pathlib\n",
    "import time\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from pyspark.pandas.indexes.datetimes import DatetimeIndex\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import when, col, rank, lit, monotonically_increasing_id, date_trunc, udf, min, max, explode\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, FloatType, DateType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587678ae-3b0d-4dc7-b9ca-eb763de9e8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/08 22:40:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/08 22:40:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAll([\\\n",
    "    ('spark.app.name', 'ReduceData')])\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e655f3f4-200c-4411-af71-86d789750291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glucose_data_schema=StructType([StructField('NumId', IntegerType(), True),\n",
    "                                        StructField('PatientId', StringType(), True),\n",
    "                                        StructField('Value', FloatType(), True),\n",
    "                                        StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                        StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                        StructField('GlucoseDisplayDate', DateType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff65b08-eab2-47e5-8509-f249e1b098be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7980234622955322\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "data_location = \"/cephfs/train_test_val/train_set\"\n",
    "allPaths = [str(x) for x in list(pathlib.Path(data_location).glob('*.parquet')) if 'part-00' in str(x)][0]\n",
    "#allPaths.sort()\n",
    "# print(allPaths)\n",
    "\n",
    "pyspark_glucose_data = spark.read \\\n",
    "                       .schema(glucose_data_schema) \\\n",
    "                       .format('parquet') \\\n",
    "                       .load(allPaths)\n",
    "pyspark_glucose_data = pyspark_glucose_data.withColumn(\"GlucoseDisplayTime\",\n",
    "                                                       date_trunc(\"minute\",\n",
    "                                                       col(\"GlucoseDisplayTime\")))\n",
    "\n",
    "pyspark_glucose_data=pyspark_glucose_data.orderBy(\"PatientId\",\n",
    "                                                  \"GlucoseDisplayTime\",\n",
    "                                                  ascending=True)\n",
    "\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cd8d14-52a8-43f4-b82a-3ec23ec00f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pyspark_binary_labels(pyspark_glucose_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e42eecce-4641-4a9d-afbc-054169f6ad5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "df = pyspark_glucose_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f553769-6fac-44df-8578-7d04ff859ab2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NumId</th>\n",
       "      <th>PatientId</th>\n",
       "      <th>Value</th>\n",
       "      <th>GlucoseDisplayTime</th>\n",
       "      <th>GlucoseDisplayTimeRaw</th>\n",
       "      <th>GlucoseDisplayDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>239.0</td>\n",
       "      <td>2022-02-01 00:01:00</td>\n",
       "      <td>2022-02-01T00:01:25.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>229.0</td>\n",
       "      <td>2022-02-01 00:06:00</td>\n",
       "      <td>2022-02-01T00:06:25.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>219.0</td>\n",
       "      <td>2022-02-01 00:11:00</td>\n",
       "      <td>2022-02-01T00:11:26.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>211.0</td>\n",
       "      <td>2022-02-01 00:16:00</td>\n",
       "      <td>2022-02-01T00:16:25.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2022-02-01 00:21:00</td>\n",
       "      <td>2022-02-01T00:21:25.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>198.0</td>\n",
       "      <td>2022-02-01 00:26:00</td>\n",
       "      <td>2022-02-01T00:26:26.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>191.0</td>\n",
       "      <td>2022-02-01 00:31:00</td>\n",
       "      <td>2022-02-01T00:31:26.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>184.0</td>\n",
       "      <td>2022-02-01 00:36:00</td>\n",
       "      <td>2022-02-01T00:36:26.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2022-02-01 00:41:00</td>\n",
       "      <td>2022-02-01T00:41:25.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6782</td>\n",
       "      <td>48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=</td>\n",
       "      <td>173.0</td>\n",
       "      <td>2022-02-01 00:46:00</td>\n",
       "      <td>2022-02-01T00:46:26.000-06:00</td>\n",
       "      <td>2022-02-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NumId                                     PatientId  Value  \\\n",
       "0   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  239.0   \n",
       "1   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  229.0   \n",
       "2   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  219.0   \n",
       "3   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  211.0   \n",
       "4   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  202.0   \n",
       "5   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  198.0   \n",
       "6   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  191.0   \n",
       "7   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  184.0   \n",
       "8   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  178.0   \n",
       "9   6782  48tvP/+WswO0u4OPaFoSqyW8rPDU4Nu/w7J4ctOnCo4=  173.0   \n",
       "\n",
       "   GlucoseDisplayTime          GlucoseDisplayTimeRaw GlucoseDisplayDate  \n",
       "0 2022-02-01 00:01:00  2022-02-01T00:01:25.000-06:00         2022-02-01  \n",
       "1 2022-02-01 00:06:00  2022-02-01T00:06:25.000-06:00         2022-02-01  \n",
       "2 2022-02-01 00:11:00  2022-02-01T00:11:26.000-06:00         2022-02-01  \n",
       "3 2022-02-01 00:16:00  2022-02-01T00:16:25.000-06:00         2022-02-01  \n",
       "4 2022-02-01 00:21:00  2022-02-01T00:21:25.000-06:00         2022-02-01  \n",
       "5 2022-02-01 00:26:00  2022-02-01T00:26:26.000-06:00         2022-02-01  \n",
       "6 2022-02-01 00:31:00  2022-02-01T00:31:26.000-06:00         2022-02-01  \n",
       "7 2022-02-01 00:36:00  2022-02-01T00:36:26.000-06:00         2022-02-01  \n",
       "8 2022-02-01 00:41:00  2022-02-01T00:41:25.000-06:00         2022-02-01  \n",
       "9 2022-02-01 00:46:00  2022-02-01T00:46:26.000-06:00         2022-02-01  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3de5e1-ad3f-4638-a548-b96946657701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.0\n",
      "264.0\n"
     ]
    }
   ],
   "source": [
    "lower_10, upper_90 = df.Value.quantile([.1, .9], interpolation='nearest')\n",
    "\n",
    "print(lower_10)\n",
    "print(upper_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61cc111c-a3cb-4368-8701-226dd3b39565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "264.0\n"
     ]
    }
   ],
   "source": [
    "default_lower = 70\n",
    "default_upper = 180\n",
    "\n",
    "lower = lower_10 if lower_10 < default_lower else default_lower\n",
    "upper = upper_90 if upper_90 > default_upper else default_upper\n",
    "\n",
    "print(lower)\n",
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc5c4407-3455-43a6-95f6-28846935d264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['y_Binary'] = [1 if ((x > upper) or (x < lower)) else 0 for x in df['Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2725dc1-2842-48a6-b4be-2fb18839a84d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def pyspark_binary_labels(self, df):\n",
    "#         #get 10th and 90th percentiles of patient\n",
    "#         lower_10, upper_90  = df.approxQuantile('Value', [.1, .9], 0)\n",
    "        \n",
    "#         #if 10th percentile of patient is lower than default, use percentile\n",
    "#         lower = lower_10 if lower_10 < self.lower else self.lower\n",
    "#         upper = upper_90 if upper_90 > self.upper else self.upper\n",
    "        \n",
    "#         df=df.withColumn('y_Binary', F.when(F.col('Value') > upper, 1)\\\n",
    "#             .when(F.col('Value') < lower, 1)\\\n",
    "#                 .otherwise(0))\n",
    "#         df=df.withColumn('is_above', F.when(F.col('Value') > upper, 1).otherwise(0))\n",
    "#         df=df.withColumn('is_below', F.when(F.col('Value') < lower, 1).otherwise(0))\n",
    "\n",
    "#         return df\n",
    "    \n",
    "def pyspark_binary_labels(self, df):\n",
    "    @pandas_udf(self.outSchema, functionType=PandasUDFType.GROUPED_MAP)\n",
    "    def pandas_binary_labels(self, df):\n",
    "        lower_10, upper_90 = df.Value.quantile([.1, .9], interpolation='nearest')\n",
    "        lower = lower_10 if lower_10 < self.lower else self.lower\n",
    "        upper = upper_90 if upper_90 > self.upper else self.upper\n",
    "        df['y_Binary'] = [1 if ((x > upper) or (x < lower)) else 0 for x in df['Value']]\n",
    "        df['is_above'] = [1 if (x > upper) else 0 for x in df['Value']]\n",
    "        df['is_below'] = [1 if (x < lower) else 0 for x in df['Value']]\n",
    "            \n",
    "        return df  \n",
    "    \n",
    "    return df.groupBy('PatientId').apply(pyspark_binary_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b905a6a-ca7f-4401-8dbe-d5f440040268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6bb3c4-585b-493c-af94-1dee4ff86bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564a78b7-eb43-4b28-8cd4-1d4d4c94eb85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a687c1-5bc2-4680-9fd9-b757b9382f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9d532-0a6f-4c31-9bb0-b3e7f491920d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b329c-b48c-4e94-97ba-91d15dc38a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf4577-4069-473c-9c8d-35a25a8628bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c2ee4b-2404-468c-8b05-1310028ca23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e867ed-bd5c-4afc-a9d8-90d5cab64e50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685b21d-eada-40ba-890c-46b4d7b5897b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pyspark_glucose_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975387a-fe4a-4bb3-ba70-2cdfd3825911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ea928-a31c-4cdd-be04-f55f6e00f104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max = test_df.groupBy('NumId')\\\n",
    "    .agg(min(\"GlucoseDisplayTime\").alias('MinDate'),\\\n",
    "        max(\"GlucoseDisplayTime\").alias('MaxDate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85f66c-c9f2-4d1e-964a-c6f0c2345aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max = min_max.withColumn(\"Index\", monotonically_increasing_id())\\\n",
    ".select('Index','NumId','MinDate','MaxDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585027e-9ac9-42c9-9689-74f50bf41992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da00c0-82f9-485a-bfcb-12783442817b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_schema = StructType([StructField('NumId', IntegerType(), True),\n",
    "                            StructField('GlucoseDisplayTime', TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a0dda-510b-48b9-842b-6882687057d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7558347-c730-47ec-8b4f-59f16dc438bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema=merged_schema)\n",
    "\n",
    "for idx in range(min_max.count()):\n",
    "    row = min_max.filter(min_max.Index == idx).first()\n",
    "    date_range = pd.DataFrame(pd.Series(ps.date_range(row.MinDate, row.MaxDate, freq='5min')))\n",
    "    start = time.time()\n",
    "    merge_df = spark.createDataFrame(date_range.to_numpy(), \\\n",
    "                                     schema=StructType([StructField('GlucoseDisplayTime', TimestampType(), True)]))\n",
    "    print(time.time() - start)\n",
    "    merge_df = merge_df.withColumn('NumId', lit(row.NumId))\n",
    "    #merge_df = merge_df.withColumn('GlucoseDisplayTime', date_range)                                                           \n",
    "    \n",
    "    print(merge_df)\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cf4a4-1676-40e6-889a-6c44d8d37170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def date_range_def(minDate, maxDate):    \n",
    "    return ps.date_range(minDate, maxDate, freq='5min')\n",
    "\n",
    "date_range_udf = udf(lambda x,y: date_range_def(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb6603-5dda-4f7c-a0fb-bc64f165fcac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max = min_max.withColumn('Range', date_range_udf(col('MinDate'), col('MaxDate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104e769-3eba-4a06-a3f9-0b92e95c5cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba149e8d-b1e1-4e73-8758-eefd381ef723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates = spark.sql(\"SELECT sequence(CAST('2022-04-08 07:49:00' AS DATE), CAST('2022-10-01 07:47:00' AS DATE), interval 5 minutes) as date\").withColumn(\"date\", explode(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d5208-a681-4098-af66-245b0d3e552c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d02d7-e552-4d6e-b7b4-48d638587c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(PandasUDFType.GROUPED_MAP)\n",
    "def replace_missing(subset):\n",
    "    \"\"\" INPUT\n",
    "        subset:     spark DataFrame with 1 patient\n",
    "        OUTPUT\n",
    "        filled_df:  spark DataFrame with 1 patient (-1 columns) and all missing rows filled in; not sorted\n",
    "    \"\"\"\n",
    "\n",
    "    '''get first and last date (takes about 10 seconds per ten days of one patient)'''\n",
    "    minimum = subset.agg({'GlucoseDisplayTime': 'min'}).collect()[0][0]\n",
    "    maximum = subset.agg({'GlucoseDisplayTime': 'max'}).collect()[0][0]\n",
    "\n",
    "    '''make a range that fills all those in'''\n",
    "    def date_range_list(start_date, end_date):\n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"start_date must come before end_date\")\n",
    "\n",
    "        datetime_list = []\n",
    "        curr_date = start_date\n",
    "        while curr_date <= end_date:\n",
    "            datetime_list.append([curr_date])\n",
    "            curr_date += timedelta(minutes=5)\n",
    "        return datetime_list\n",
    "\n",
    "    datetime_list = date_range_list(minimum, maximum)\n",
    "\n",
    "    '''make a dataframe of those dates'''\n",
    "    deptSchema = StructType([       \n",
    "        StructField('GlucoseDisplayTime', TimestampType(), True)\n",
    "    ])\n",
    "    dt_df = self.spark.createDataFrame(data=datetime_list, schema=deptSchema)\n",
    "\n",
    "    '''merge og dataframe back into the new one'''\n",
    "    merged = subset.unionByName(dt_df, allowMissingColumns=True)\n",
    "\n",
    "    '''get rid of the timestamps we already have (using the exact same method as from \"drop duplicate datetimes for each patient\" above)'''\n",
    "    window = Window.partitionBy('GlucoseDisplayTime').orderBy('tiebreak')\n",
    "    merged = (merged\n",
    "     .withColumn('tiebreak', monotonically_increasing_id())\n",
    "     .withColumn('rank', rank().over(window))\n",
    "     .filter(col('rank') == 1).drop('rank','tiebreak')\n",
    "    )\n",
    "\n",
    "    '''filling out the columns as needed:\n",
    "        -PatientId should be all the same string\n",
    "        -GlucoseDisplayTimeRaw should be used for checking the dates here, but implementation will have to come later'''\n",
    "    merged = merged.fillna(patient_str, subset='PatientId')\n",
    "    merged = merged.drop('GlucoseDisplayTimeRaw') #someday i'll have time to use this as the double-checker\n",
    "    merged = merged.withColumn('GlucoseDisplayDate',\n",
    "                               to_date(col('GlucoseDisplayTime')))\n",
    "\n",
    "    \"\"\" ============== FILL IN MISSING VALUES ============== \"\"\"\n",
    "    # filler = subset.agg({'Value': 'median'}).collect()[0][0]\n",
    "    filler = subset.agg({'Value': 'avg'}).collect()[0][0]\n",
    "\n",
    "    filled_df = merged.fillna(filler, subset='Value')\n",
    "\n",
    "    return filled_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e2ba3-087b-407e-bebb-a70ed4b44b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test_df.groupBy('NumId').apply(lambda x: replace_missing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d55ca-85c1-451f-9669-b5c11d7d2e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
