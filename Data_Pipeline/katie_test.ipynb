{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4e2c18-cdd4-4603-b3ad-0b2f4dee2815",
   "metadata": {},
   "source": [
    "### Fill In Missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8927dcb9-c531-4aad-826d-17b3c7c09655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pathlib\n",
    "import time\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from pyspark.pandas.indexes.datetimes import DatetimeIndex\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col,isnan,when,count, when, col, rank, lit, monotonically_increasing_id, date_trunc, udf, min, max, explode\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, FloatType, DateType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587678ae-3b0d-4dc7-b9ca-eb763de9e8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/23 01:55:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAll([\\\n",
    "    ('spark.app.name', 'ReduceData')])\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e655f3f4-200c-4411-af71-86d789750291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glucose_data_schema=StructType([StructField('NumId', IntegerType(), True),\n",
    "                                        StructField('PatientId', StringType(), True),\n",
    "                                        StructField('Value', FloatType(), True),\n",
    "                                        StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                        StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                        StructField('GlucoseDisplayDate', DateType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d37681c9-56da-440a-9a18-ee67003528fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train done\n",
      "test done\n",
      "val done\n"
     ]
    }
   ],
   "source": [
    "read_in_types = ['training_set', 'test_set', 'val_set']\n",
    "data_types = ['train', 'test', 'val']\n",
    "\n",
    "# for data_type in data_types: \n",
    "for j in range(0,3): \n",
    "    data_location = \"/cephfs/train_test_val/\" + read_in_types[j]\n",
    "    allPaths = [str(x) for x in list(pathlib.Path(data_location).glob('*.parquet')) if 'part-00' in str(x)]\n",
    "    path_counter = 0\n",
    "    \n",
    "    for path in allPaths:\n",
    "        gluc = pd.read_parquet(path, columns=['NumId','GlucoseDisplayTime', 'Value'])\n",
    "        gluc['GlucoseDisplayTime'] = gluc['GlucoseDisplayTime'].dt.floor('Min')\n",
    "        gluc = gluc.sort_values(by=['NumId', 'GlucoseDisplayTime'])\n",
    "\n",
    "        min_max = gluc.groupby('NumId')\\\n",
    "                        .agg({'GlucoseDisplayTime' : ['min','max']})\n",
    "\n",
    "        merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "        starttime = time.time()\n",
    "        last_idx = len(min_max)-1\n",
    "\n",
    "        index_counter = 0\n",
    "        for idx, row in min_max.iterrows():\n",
    "            #grab all potential dates in range\n",
    "\n",
    "            min_val = row['GlucoseDisplayTime']['min']\n",
    "            max_val = row['GlucoseDisplayTime']['max']\n",
    "\n",
    "            date_df = pd.DataFrame(pd.date_range(min_val, max_val, freq='5min'),\\\n",
    "                                   columns=['GlucoseDisplayTime'])  \n",
    "\n",
    "            # merge dates with big pypsark df\n",
    "            id_df = gluc[gluc['NumId'] == idx]\n",
    "\n",
    "            mean = id_df.Value.mean().round()\n",
    "\n",
    "            id_df.set_index('GlucoseDisplayTime', inplace=True)    \n",
    "\n",
    "            date_df.set_index('GlucoseDisplayTime', inplace=True)\n",
    "\n",
    "            merged = id_df.join(date_df, how='outer',\\\n",
    "                                on='GlucoseDisplayTime', sort=True)\n",
    "\n",
    "            merged['IsFilledIn'] = 0\n",
    "            merged.loc[merged.Value.isna(), 'IsFilledIn'] = 1        \n",
    "            merged.loc[merged.Value.isna(), 'Value'] = mean\n",
    "\n",
    "            merged['NumId'] = idx\n",
    "\n",
    "            merged.reset_index(inplace=True)\n",
    "\n",
    "            merged = merged.drop(columns=['index'])\n",
    "\n",
    "            merged['TimeLag'] = np.concatenate((merged['GlucoseDisplayTime'].iloc[0],\\\n",
    "                                                np.array(merged['GlucoseDisplayTime'].iloc[:-1].values)), axis=None)\\\n",
    "                                .astype('datetime64[ns]')\n",
    "\n",
    "            merged['Diff'] = (merged['TimeLag'] - merged['GlucoseDisplayTime']).dt.seconds\n",
    "\n",
    "            len_merged = len(merged)\n",
    "\n",
    "            # get all index of rows with diff less than 5 mins, add 1 to remove next row, \n",
    "            # dont include last row to delete\n",
    "            indexes_to_remove = [x for x in merged[merged['Diff'] < 300].index + 1 if x < len_merged & x != 0]\n",
    "\n",
    "            if len(indexes_to_remove) > 0:\n",
    "                merged = merged.drop(indexes_to_remove)\n",
    "\n",
    "            # its ready freddy for some interpoletty\n",
    "            # merged DF is the dataframe ready to go into interpolation function\n",
    "\n",
    "            # fill with mean\n",
    "\n",
    "            merged = merged.drop(columns=['TimeLag', 'Diff'])\n",
    "\n",
    "            if ((index_counter % 25 != 0) and index_counter != last_idx) or (index_counter == 0):\n",
    "                merge_df = pd.concat([merge_df, merged])\n",
    "            elif (index_counter % 25 == 0) or (index_counter == last_idx):\n",
    "                merge_df = merge_df.astype({'GlucoseDisplayTime': 'datetime64[ns]'})\n",
    "                \n",
    "                merge_df.to_parquet('/cephfs/interpolation/' + data_types[j] + '/parquet_' + str(path_counter) + '_' + str(index_counter) + '.parquet')\n",
    "                merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "               \n",
    "            \n",
    "            index_counter += 1\n",
    "\n",
    "        path_counter += 1\n",
    "    \n",
    "    print(data_types[j] + ' done')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c2b712-edc8-4305-a1b5-60dccdc326ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "786cea3b-adff-4af2-a2d3-dd146db19e4e",
   "metadata": {},
   "source": [
    "testing if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4416190e-51fd-4df2-a98b-5956938c54c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_sample = spark.read.format('parquet').load('/cephfs/interpolation/train/parquet_0_25.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ecca149-13e3-4b08-9977-8d800be4a1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sample = spark.read.format('parquet').load('/cephfs/interpolation/test/parquet_0_25.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b61cc1-dd08-41e2-ad24-cdb460f365e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sample = spark.read.format('parquet').load('/cephfs/interpolation/test/parquet_0_25.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6c58c3c-9282-4b20-9a9a-eced957ba583",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allPaths = [str(x) for x in list(pathlib.Path('/cephfs/summary_stats/test/').glob('*.parquet'))]\n",
    "\n",
    "len(allPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca1e7cab-7cc3-4276-8dd7-79372516dfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('parquet').load(allPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66e1b5a9-0f46-4114-8e4f-290fc76c4aba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NumId',\n",
       " 'Chunk',\n",
       " 'Mean',\n",
       " 'StdDev',\n",
       " 'Median',\n",
       " 'Min',\n",
       " 'Max',\n",
       " 'AvgFirstDiff',\n",
       " 'AvgSecDiff',\n",
       " 'StdFirstDiff',\n",
       " 'StdSecDiff',\n",
       " 'CountAbove',\n",
       " 'CountBelow',\n",
       " 'TotalOutOfRange',\n",
       " 'target']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24f22c15-6058-4a23-a1e2-4666098f7b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed('target', 'DiffPrevious')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d35e050e-e815-4223-a0a1-3c15266415b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('target', when(df.DiffPrevious > 0, 1)\n",
    "                                 .when(df.DiffPrevious < 0,-1)\n",
    "                                 .otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce40fa5e-5c6c-4600-9e0b-da0f98ef899b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|DiffPrevious|target|\n",
      "+------------+------+\n",
      "|          77|     1|\n",
      "|         -28|    -1|\n",
      "|         -29|    -1|\n",
      "|          45|     1|\n",
      "|         -42|    -1|\n",
      "|          47|     1|\n",
      "|         -22|    -1|\n",
      "|           6|     1|\n",
      "|           2|     1|\n",
      "|          20|     1|\n",
      "|           1|     1|\n",
      "|           0|     0|\n",
      "|          -9|    -1|\n",
      "|           8|     1|\n",
      "|         -30|    -1|\n",
      "|          23|     1|\n",
      "|         -41|    -1|\n",
      "|          49|     1|\n",
      "|           0|     0|\n",
      "|         -26|    -1|\n",
      "+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('DiffPrevious'), col('target')).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5066b781-5b06-47d6-93b2-753226d46e99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.repartition('NumId').write.parquet('/cephfs/summary_stats/test_cat/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a8c54cb-39e9-40a5-8e73-7d0c12ddca3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NumId', 'int'),\n",
       " ('Chunk', 'int'),\n",
       " ('Mean', 'double'),\n",
       " ('StdDev', 'double'),\n",
       " ('Median', 'float'),\n",
       " ('Min', 'float'),\n",
       " ('Max', 'float'),\n",
       " ('AvgFirstDiff', 'double'),\n",
       " ('AvgSecDiff', 'double'),\n",
       " ('StdFirstDiff', 'double'),\n",
       " ('StdSecDiff', 'double'),\n",
       " ('CountAbove', 'bigint'),\n",
       " ('CountBelow', 'bigint'),\n",
       " ('TotalOutOfRange', 'bigint'),\n",
       " ('DiffPrevious', 'bigint'),\n",
       " ('target', 'int')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a923fa3-143b-4cb3-be57-0ea3dbc963b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
