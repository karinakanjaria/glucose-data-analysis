{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4e2c18-cdd4-4603-b3ad-0b2f4dee2815",
   "metadata": {},
   "source": [
    "### Fill In Missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8927dcb9-c531-4aad-826d-17b3c7c09655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pathlib\n",
    "import time\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from pyspark.pandas.indexes.datetimes import DatetimeIndex\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col,isnan,when,count, when, col, rank, lit, monotonically_increasing_id, date_trunc, udf, min, max, explode\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, FloatType, DateType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587678ae-3b0d-4dc7-b9ca-eb763de9e8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 20:26:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAll([\\\n",
    "    ('spark.app.name', 'ReduceData')])\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e655f3f4-200c-4411-af71-86d789750291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glucose_data_schema=StructType([StructField('NumId', IntegerType(), True),\n",
    "                                        StructField('PatientId', StringType(), True),\n",
    "                                        StructField('Value', FloatType(), True),\n",
    "                                        StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                        StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                        StructField('GlucoseDisplayDate', DateType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d37681c9-56da-440a-9a18-ee67003528fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "   GlucoseDisplayTime NumId  Value  IsFilledIn\n",
      "0 2022-02-01 00:01:00    76  153.0         0.0\n",
      "1 2022-02-01 00:06:00    76  149.0         0.0\n",
      "2 2022-02-01 00:11:00    76  146.0         0.0\n",
      "3 2022-02-01 00:16:00    76  174.0         1.0\n",
      "4 2022-02-01 00:21:00    76  140.0         0.0\n",
      "test\n",
      "   GlucoseDisplayTime NumId  Value  IsFilledIn\n",
      "0 2022-02-01 00:14:00    69  114.0         0.0\n",
      "1 2022-02-01 00:19:00    69  143.0         1.0\n",
      "2 2022-02-01 00:24:00    69  143.0         1.0\n",
      "3 2022-02-01 00:29:00    69  113.0         0.0\n",
      "4 2022-02-01 00:34:00    69  143.0         1.0\n",
      "val\n",
      "   GlucoseDisplayTime NumId  Value  IsFilledIn\n",
      "0 2022-02-16 17:49:00    97  203.0         0.0\n",
      "1 2022-02-16 17:54:00    97  201.0         1.0\n",
      "2 2022-02-16 17:59:00    97  201.0         1.0\n",
      "3 2022-02-16 18:04:00    97  201.0         0.0\n",
      "4 2022-02-16 18:09:00    97  201.0         1.0\n"
     ]
    }
   ],
   "source": [
    "data_types = ['train', 'test', 'val']\n",
    "\n",
    "for data_type in data_types: \n",
    "    data_location = \"/cephfs/train_test_val/\" + data_type\n",
    "    allPaths = [str(x) for x in list(pathlib.Path(data_location).glob('*.parquet')) if 'part-00' in str(x)]\n",
    "    path_counter = 0\n",
    "    \n",
    "    for path in allPaths:\n",
    "        gluc = pd.read_parquet(path, columns=['NumId','GlucoseDisplayTime', 'Value'])\n",
    "        gluc['GlucoseDisplayTime'] = gluc['GlucoseDisplayTime'].dt.floor('Min')\n",
    "        gluc = gluc.sort_values(by=['NumId', 'GlucoseDisplayTime'])\n",
    "\n",
    "        min_max = gluc.groupby('NumId')\\\n",
    "                        .agg({'GlucoseDisplayTime' : ['min','max']})\n",
    "\n",
    "        merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "        starttime = time.time()\n",
    "        last_idx = len(min_max)-1\n",
    "\n",
    "        index_counter = 0\n",
    "        for idx, row in min_max.iterrows():\n",
    "            #grab all potential dates in range\n",
    "\n",
    "            min_val = row['GlucoseDisplayTime']['min']\n",
    "            max_val = row['GlucoseDisplayTime']['max']\n",
    "\n",
    "            date_df = pd.DataFrame(pd.date_range(min_val, max_val, freq='5min'),\\\n",
    "                                   columns=['GlucoseDisplayTime'])  \n",
    "\n",
    "            # merge dates with big pypsark df\n",
    "            id_df = gluc[gluc['NumId'] == idx]\n",
    "\n",
    "            mean = id_df.Value.mean().round()\n",
    "\n",
    "            id_df.set_index('GlucoseDisplayTime', inplace=True)    \n",
    "\n",
    "            date_df.set_index('GlucoseDisplayTime', inplace=True)\n",
    "\n",
    "            merged = id_df.join(date_df, how='outer',\\\n",
    "                                on='GlucoseDisplayTime', sort=True)\n",
    "\n",
    "            merged['IsFilledIn'] = 0\n",
    "            merged.loc[merged.Value.isna(), 'IsFilledIn'] = 1        \n",
    "            merged.loc[merged.Value.isna(), 'Value'] = mean\n",
    "\n",
    "            merged['NumId'] = idx\n",
    "\n",
    "            merged.reset_index(inplace=True)\n",
    "\n",
    "            merged = merged.drop(columns=['index'])\n",
    "\n",
    "            merged['TimeLag'] = np.concatenate((merged['GlucoseDisplayTime'].iloc[0],\\\n",
    "                                                np.array(merged['GlucoseDisplayTime'].iloc[:-1].values)), axis=None)\\\n",
    "                                .astype('datetime64[ns]')\n",
    "\n",
    "            merged['Diff'] = (merged['TimeLag'] - merged['GlucoseDisplayTime']).dt.seconds\n",
    "\n",
    "            len_merged = len(merged)\n",
    "\n",
    "            # get all index of rows with diff less than 5 mins, add 1 to remove next row, \n",
    "            # dont include last row to delete\n",
    "            indexes_to_remove = [x for x in merged[merged['Diff'] < 300].index + 1 if x < len_merged & x != 0]\n",
    "\n",
    "            if len(indexes_to_remove) > 0:\n",
    "                merged = merged.drop(indexes_to_remove)\n",
    "\n",
    "            # its ready freddy for some interpoletty\n",
    "            # merged DF is the dataframe ready to go into interpolation function\n",
    "\n",
    "            # fill with mean\n",
    "\n",
    "            merged = merged.drop(columns=['TimeLag', 'Diff'])\n",
    "\n",
    "            if ((index_counter % 25 != 0) and index_counter != last_idx) or (index_counter == 0):\n",
    "                merge_df = pd.concat([merge_df, merged])\n",
    "            elif (index_counter % 25 == 0) or (index_counter == last_idx):\n",
    "                merge_df = merge_df.astype({'GlucoseDisplayTime': 'datetime64[ns]'})\n",
    "                \n",
    "                merge_df.to_parquet('/cephfs/interpolation/' + data_type + '/parquet_' + str(path_counter) + '_' + str(index_counter) + '.parquet')\n",
    "                merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "               \n",
    "            \n",
    "            index_counter += 1\n",
    "\n",
    "        path_counter += 1\n",
    "    \n",
    "    print(data_type + ' done')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786cea3b-adff-4af2-a2d3-dd146db19e4e",
   "metadata": {},
   "source": [
    "testing if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4416190e-51fd-4df2-a98b-5956938c54c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sample = spark.read.format('parquet').load('/cephfs/interpolation/train/parquet_0_25.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "558a1cb3-8cdf-410f-a07a-36756ba89bc2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------+------------------+----------+-----------------+\n",
      "| GlucoseDisplayTime|NumId|    Value|GlucoseDisplayDate|IsFilledIn|__index_level_0__|\n",
      "+-------------------+-----+---------+------------------+----------+-----------------+\n",
      "|2022-02-01 00:01:00|   76|    153.0|        2022-02-01|       0.0|                0|\n",
      "|2022-02-01 00:06:00|   76|    149.0|        2022-02-01|       0.0|                1|\n",
      "|2022-02-01 00:11:00|   76|    146.0|        2022-02-01|       0.0|                2|\n",
      "|2022-02-01 00:16:00|   76|174.41835|        2022-02-01|       1.0|                3|\n",
      "|2022-02-01 00:21:00|   76|    140.0|        2022-02-01|       0.0|                4|\n",
      "|2022-02-01 00:26:00|   76|    137.0|        2022-02-01|       0.0|                5|\n",
      "|2022-02-01 00:31:00|   76|    135.0|        2022-02-01|       0.0|                6|\n",
      "|2022-02-01 00:36:00|   76|    133.0|        2022-02-01|       0.0|                7|\n",
      "|2022-02-01 00:41:00|   76|174.41835|        2022-02-01|       1.0|                8|\n",
      "|2022-02-01 00:46:00|   76|174.41835|        2022-02-01|       1.0|                9|\n",
      "|2022-02-01 00:51:00|   76|    126.0|        2022-02-01|       0.0|               10|\n",
      "|2022-02-01 00:56:00|   76|174.41835|        2022-02-01|       1.0|               11|\n",
      "|2022-02-01 01:01:00|   76|    123.0|        2022-02-01|       0.0|               12|\n",
      "|2022-02-01 01:06:00|   76|    122.0|        2022-02-01|       0.0|               13|\n",
      "|2022-02-01 01:11:00|   76|    122.0|        2022-02-01|       0.0|               14|\n",
      "|2022-02-01 01:16:00|   76|    120.0|        2022-02-01|       0.0|               15|\n",
      "|2022-02-01 01:21:00|   76|    114.0|        2022-02-01|       0.0|               16|\n",
      "|2022-02-01 01:26:00|   76|174.41835|        2022-02-01|       1.0|               17|\n",
      "|2022-02-01 01:31:00|   76|174.41835|        2022-02-01|       1.0|               18|\n",
      "|2022-02-01 01:36:00|   76|    109.0|        2022-02-01|       0.0|               19|\n",
      "+-------------------+-----+---------+------------------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_sample.sort('NumId', 'GlucoseDisplayTime').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecca149-13e3-4b08-9977-8d800be4a1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sample = spark.read.format('parquet').load('/cephfs/interpolation/test/parquet_0_25.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "273f8ad0-240e-4995-81d6-b5a68ac6c869",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+---------+------------------+----------+-----------------+\n",
      "| GlucoseDisplayTime|NumId|    Value|GlucoseDisplayDate|IsFilledIn|__index_level_0__|\n",
      "+-------------------+-----+---------+------------------+----------+-----------------+\n",
      "|2022-02-01 00:14:00|   69|    114.0|        2022-02-01|       0.0|                0|\n",
      "|2022-02-01 00:19:00|   69|142.54594|        2022-02-01|       1.0|                1|\n",
      "|2022-02-01 00:24:00|   69|142.54594|        2022-02-01|       1.0|                2|\n",
      "|2022-02-01 00:29:00|   69|    113.0|        2022-02-01|       0.0|                3|\n",
      "|2022-02-01 00:34:00|   69|142.54594|        2022-02-01|       1.0|                4|\n",
      "|2022-02-01 00:39:00|   69|142.54594|        2022-02-01|       1.0|                5|\n",
      "|2022-02-01 00:44:00|   69|142.54594|        2022-02-01|       1.0|                6|\n",
      "|2022-02-01 00:49:00|   69|142.54594|        2022-02-01|       1.0|                7|\n",
      "|2022-02-01 00:54:00|   69|142.54594|        2022-02-01|       1.0|                8|\n",
      "|2022-02-01 00:59:00|   69|142.54594|        2022-02-01|       1.0|                9|\n",
      "|2022-02-01 01:04:00|   69|142.54594|        2022-02-01|       1.0|               10|\n",
      "|2022-02-01 01:09:00|   69|142.54594|        2022-02-01|       1.0|               11|\n",
      "|2022-02-01 01:14:00|   69|142.54594|        2022-02-01|       1.0|               12|\n",
      "|2022-02-01 01:19:00|   69|    157.0|        2022-02-01|       0.0|               13|\n",
      "|2022-02-01 01:24:00|   69|142.54594|        2022-02-01|       1.0|               14|\n",
      "|2022-02-01 01:29:00|   69|    162.0|        2022-02-01|       0.0|               15|\n",
      "|2022-02-01 01:34:00|   69|    163.0|        2022-02-01|       0.0|               16|\n",
      "|2022-02-01 01:39:00|   69|142.54594|        2022-02-01|       1.0|               17|\n",
      "|2022-02-01 01:44:00|   69|142.54594|        2022-02-01|       1.0|               18|\n",
      "|2022-02-01 01:49:00|   69|142.54594|        2022-02-01|       1.0|               19|\n",
      "+-------------------+-----+---------+------------------+----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "test_sample.sort('NumId', 'GlucoseDisplayTime').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40b61cc1-dd08-41e2-ad24-cdb460f365e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sample = spark.read.format('parquet').load('/cephfs/train_test_val/val/parquet_0_25.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a4d8719-2fd8-430f-be4c-169293a27a41",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|NumId|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|  134|H24YXOcfrUrg312CB...|122.0|2022-02-01 07:42:00| 2022-02-01T07:42:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|186.0|2022-02-01 10:27:00| 2022-02-01T10:27:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|138.0|2022-02-01 08:47:00| 2022-02-01T08:47:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|135.0|2022-02-01 19:02:00| 2022-02-01T19:02:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|108.0|2022-02-01 13:22:00| 2022-02-01T13:22:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|159.0|2022-02-01 01:12:00| 2022-02-01T01:12:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|100.0|2022-02-01 17:12:00| 2022-02-01T17:12:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|169.0|2022-02-01 09:32:00| 2022-02-01T09:32:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|137.0|2022-02-01 05:47:00| 2022-02-01T05:47:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|180.0|2022-02-01 10:57:00| 2022-02-01T10:57:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|121.0|2022-02-01 21:57:00| 2022-02-01T21:57:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|152.0|2022-02-01 04:42:00| 2022-02-01T04:42:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|154.0|2022-02-01 20:22:00| 2022-02-01T20:22:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|141.0|2022-02-01 04:57:00| 2022-02-01T04:57:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|125.0|2022-02-01 14:17:00| 2022-02-01T14:17:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|141.0|2022-02-01 03:37:00| 2022-02-01T03:37:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|143.0|2022-02-01 03:02:00| 2022-02-01T03:02:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|163.0|2022-02-01 01:52:00| 2022-02-01T01:52:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|119.0|2022-02-01 06:52:00| 2022-02-01T06:52:...|        2022-02-01|\n",
      "|  134|H24YXOcfrUrg312CB...|193.0|2022-02-01 09:57:00| 2022-02-01T09:57:...|        2022-02-01|\n",
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_sample.sort('NumId', 'GlucoseDisplayTime').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1e7cab-7cc3-4276-8dd7-79372516dfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
