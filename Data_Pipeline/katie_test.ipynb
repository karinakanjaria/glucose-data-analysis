{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4e2c18-cdd4-4603-b3ad-0b2f4dee2815",
   "metadata": {},
   "source": [
    "### testing fill missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8927dcb9-c531-4aad-826d-17b3c7c09655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pathlib\n",
    "import time\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from pyspark.pandas.indexes.datetimes import DatetimeIndex\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import when, col, rank, lit, monotonically_increasing_id, date_trunc, udf, min, max, explode\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, FloatType, DateType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587678ae-3b0d-4dc7-b9ca-eb763de9e8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/10 02:34:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAll([\\\n",
    "    ('spark.app.name', 'ReduceData')])\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e655f3f4-200c-4411-af71-86d789750291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glucose_data_schema=StructType([StructField('NumId', IntegerType(), True),\n",
    "                                        StructField('PatientId', StringType(), True),\n",
    "                                        StructField('Value', FloatType(), True),\n",
    "                                        StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                        StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                        StructField('GlucoseDisplayDate', DateType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ff65b08-eab2-47e5-8509-f249e1b098be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.508166551589966\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "data_location = \"/cephfs/train_test_val/train_set\"\n",
    "allPaths = [str(x) for x in list(pathlib.Path(data_location).glob('*.parquet')) if 'part-00' in str(x)][0]\n",
    "#allPaths.sort()\n",
    "# print(allPaths)\n",
    "\n",
    "pyspark_glucose_data = spark.read \\\n",
    "                       .schema(glucose_data_schema) \\\n",
    "                       .format('parquet') \\\n",
    "                       .load(allPaths)\n",
    "pyspark_glucose_data = pyspark_glucose_data.withColumn(\"GlucoseDisplayTime\",\n",
    "                                                       date_trunc(\"minute\",\n",
    "                                                       col(\"GlucoseDisplayTime\")))\n",
    "\n",
    "pyspark_glucose_data=pyspark_glucose_data.orderBy(\"PatientId\",\n",
    "                                                  \"GlucoseDisplayTime\",\n",
    "                                                  ascending=True)\n",
    "\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f685b21d-eada-40ba-890c-46b4d7b5897b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "test_df = pyspark_glucose_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7975387a-fe4a-4bb3-ba70-2cdfd3825911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ea928-a31c-4cdd-be04-f55f6e00f104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max = test_df.groupBy('NumId')\\\n",
    "    .agg(min(\"GlucoseDisplayTime\").alias('MinDate'),\\\n",
    "        max(\"GlucoseDisplayTime\").alias('MaxDate'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85f66c-c9f2-4d1e-964a-c6f0c2345aee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max = min_max.withColumn(\"Index\", monotonically_increasing_id())\\\n",
    ".select('Index','NumId','MinDate','MaxDate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585027e-9ac9-42c9-9689-74f50bf41992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a0dda-510b-48b9-842b-6882687057d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = min_max.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd175cb4-173d-4d8a-b98c-085a37c9f2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_df = test_df.drop(columns=['GlucoseDisplayDate', 'PatientId', 'GlucoseDisplayTimeRaw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26da00c0-82f9-485a-bfcb-12783442817b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_schema = StructType([StructField('NumId', IntegerType(), True),\n",
    "                            StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                           StructField('Value', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ab2a9c8-3541-471a-8c3e-dd96de8720e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105045\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "min_max = test_df.groupby('NumId')\\\n",
    "                .agg({'GlucoseDisplayTime' : ['min', 'max']})\n",
    "\n",
    "merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "for idx, row in min_max.iterrows():\n",
    "    #grab all poteitnal dates in range\n",
    "    \n",
    "    date_df = pd.DataFrame(pd.date_range(row[0], row[1], freq='5min'), columns=['GlucoseDisplayTime'])                              \n",
    "    date_df['NumId']= idx\n",
    "\n",
    "    # merge dates with big pypsark df\n",
    "    merged = test_df[test_df['NumId'] == idx]\\\n",
    "            .merge(date_df, how='right', on=['GlucoseDisplayTime', 'NumId'])\\\n",
    "            .sort_values(by=['GlucoseDisplayTime', 'Value'], na_position='last')\n",
    "\n",
    "    merged['TimeLag'] = np.concatenate((merged['GlucoseDisplayTime'].iloc[1:].values,\\\n",
    "                                        np.array(merged['GlucoseDisplayTime'].iloc[-1])), axis=None)\\\n",
    "                        .astype('datetime64[ns]')\n",
    "\n",
    "    merged['Diff'] = (merged['TimeLag'] - merged['GlucoseDisplayTime']).dt.seconds\n",
    "\n",
    "    len_merged = len(merged)\n",
    "\n",
    "    # get all index of rows with diff less than 5 mins, add 1 to remove next row, \n",
    "    # dont include last row to delete\n",
    "    indexes_to_remove = [x for x in merged[merged['Diff'] < 300].index + 1 if x < len_merged]\n",
    "\n",
    "    if len(indexes_to_remove) > 0:\n",
    "        merged = merged.drop(indexes_to_remove)\n",
    "\n",
    "    # merged is the patient's combined and deduplicated rows \n",
    "    # ready freddy for interpoletty\n",
    "\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e925d86-968e-4c7f-b691-999178167814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(merged_schema, PandasUDFType.GROUPED_MAP)\n",
    "def interpolation(self, df):    \n",
    "    min_max = test_df.groupby('NumId')\\\n",
    "                .agg({'GlucoseDisplayTime' : ['min', 'max']})\n",
    "\n",
    "    merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "    for idx, row in min_max.iterrows():\n",
    "        #grab all poteitnal dates in range\n",
    "\n",
    "        date_df = pd.DataFrame(pd.date_range(row[0], row[1], freq='5min'), columns=['GlucoseDisplayTime'])                              \n",
    "        date_df['NumId']= idx\n",
    "\n",
    "        # merge dates with big pypsark df\n",
    "        merged = test_df[test_df['NumId'] == idx]\\\n",
    "                .merge(date_df, how='right', on=['GlucoseDisplayTime', 'NumId'])\\\n",
    "                .sort_values(by=['GlucoseDisplayTime', 'Value'], na_position='last')\n",
    "\n",
    "        merged['TimeLag'] = np.concatenate((merged['GlucoseDisplayTime'].iloc[1:].values,\\\n",
    "                                            np.array(merged['GlucoseDisplayTime'].iloc[-1])), axis=None)\\\n",
    "                            .astype('datetime64[ns]')\n",
    "\n",
    "        merged['Diff'] = (merged['TimeLag'] - merged['GlucoseDisplayTime']).dt.seconds\n",
    "\n",
    "        len_merged = len(merged)\n",
    "\n",
    "        # get all index of rows with diff less than 5 mins, add 1 to remove next row, \n",
    "        # dont include last row to delete\n",
    "        indexes_to_remove = [x for x in merged[merged['Diff'] < 300].index + 1 if x < len_merged]\n",
    "\n",
    "        if len(indexes_to_remove) > 0:\n",
    "            merged = merged.drop(indexes_to_remove)\n",
    "\n",
    "        # its ready freddy for some interpoletty\n",
    "        # grab all potential dates in range\n",
    "\n",
    "        date_df = pd.DataFrame(pd.date_range(row[0], row[1], freq='5min'), columns=['GlucoseDisplayTime'])                              \n",
    "        date_df['NumId']= idx\n",
    "\n",
    "        # merge dates with big pypsark df\n",
    "        merged = test_df[test_df['NumId'] == idx]\\\n",
    "                .merge(date_df, how='right', on=['GlucoseDisplayTime', 'NumId'])\\\n",
    "                .sort_values(by=['GlucoseDisplayTime', 'Value'], na_position='last')\n",
    "\n",
    "        merged['TimeLag'] = np.concatenate((merged['GlucoseDisplayTime'].iloc[1:].values,\\\n",
    "                                            np.array(merged['GlucoseDisplayTime'].iloc[-1])), axis=None)\\\n",
    "                            .astype('datetime64[ns]')\n",
    "\n",
    "        merged['Diff'] = (merged['TimeLag'] - merged['GlucoseDisplayTime']).dt.seconds\n",
    "\n",
    "        len_merged = len(merged)\n",
    "\n",
    "        # get all index of rows with diff less than 5 mins, add 1 to remove next row, \n",
    "        # dont include last row to delete\n",
    "        indexes_to_remove = [x for x in merged[merged['Diff'] < 300].index + 1 if x < len_merged]\n",
    "\n",
    "        if len(indexes_to_remove) > 0:\n",
    "            merged = merged.drop(indexes_to_remove)\n",
    "\n",
    "        # its ready freddy for some interpoletty\n",
    "        # merged DF is the dataframe ready to go into interpolation function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08615a82-2954-4e53-b12e-e3ef3d78a841",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column:          NumId                                     PatientId  Value  \\\n0         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  164.0   \n1         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  164.0   \n2         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  161.0   \n3         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  158.0   \n4         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  160.0   \n...        ...                                           ...    ...   \n1997486   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  125.0   \n1997487   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  124.0   \n1997488   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  119.0   \n1997489   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  119.0   \n1997490   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  117.0   \n\n         GlucoseDisplayTime          GlucoseDisplayTimeRaw GlucoseDisplayDate  \n0       2022-02-11 12:54:00  2022-02-11T12:54:00.000-05:00         2022-02-11  \n1       2022-02-11 13:04:00  2022-02-11T13:04:00.000-05:00         2022-02-11  \n2       2022-02-11 13:09:00  2022-02-11T13:09:01.000-05:00         2022-02-11  \n3       2022-02-11 13:14:00  2022-02-11T13:14:00.000-05:00         2022-02-11  \n4       2022-02-11 13:19:00  2022-02-11T13:19:00.000-05:00         2022-02-11  \n...                     ...                            ...                ...  \n1997486 2023-02-01 03:43:00  2023-02-01T03:43:02.000+05:30         2023-02-01  \n1997487 2023-02-01 03:48:00  2023-02-01T03:48:02.000+05:30         2023-02-01  \n1997488 2023-02-01 04:03:00  2023-02-01T04:03:02.000+05:30         2023-02-01  \n1997489 2023-02-01 04:08:00  2023-02-01T04:08:02.000+05:30         2023-02-01  \n1997490 2023-02-01 04:28:00  2023-02-01T04:28:04.000+05:30         2023-02-01  \n\n[1997491 rows x 6 columns] of type <class 'pandas.core.frame.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minterpolation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/udf.py:276\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Column:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/udf.py:251\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf\n\u001b[0;32m--> 251\u001b[0m jPythonUDF \u001b[38;5;241m=\u001b[39m judf\u001b[38;5;241m.\u001b[39mapply(\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_java_column\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profiler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m jPythonUDF\u001b[38;5;241m.\u001b[39mexpr()\u001b[38;5;241m.\u001b[39mresultId()\u001b[38;5;241m.\u001b[39mid()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/column.py:86\u001b[0m, in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Column (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [converter(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/column.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03mConvert a list of Column (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m converter:\n\u001b[0;32m---> 86\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [\u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cols]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m     jcol \u001b[38;5;241m=\u001b[39m _create_column_from_name(col)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid argument, not a string or column: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor column literals, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstruct\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreate_map\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(col, \u001b[38;5;28mtype\u001b[39m(col))\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jcol\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column:          NumId                                     PatientId  Value  \\\n0         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  164.0   \n1         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  164.0   \n2         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  161.0   \n3         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  158.0   \n4         5868  0m18JauIIZlt6psiqeBS1SfN+URQ4axXv6xfTPIP2+M=  160.0   \n...        ...                                           ...    ...   \n1997486   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  125.0   \n1997487   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  124.0   \n1997488   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  119.0   \n1997489   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  119.0   \n1997490   4326  zN6N3TFBSOTHIVFbATwVRX+ecqxvHz6ZWZ67CPIpQmo=  117.0   \n\n         GlucoseDisplayTime          GlucoseDisplayTimeRaw GlucoseDisplayDate  \n0       2022-02-11 12:54:00  2022-02-11T12:54:00.000-05:00         2022-02-11  \n1       2022-02-11 13:04:00  2022-02-11T13:04:00.000-05:00         2022-02-11  \n2       2022-02-11 13:09:00  2022-02-11T13:09:01.000-05:00         2022-02-11  \n3       2022-02-11 13:14:00  2022-02-11T13:14:00.000-05:00         2022-02-11  \n4       2022-02-11 13:19:00  2022-02-11T13:19:00.000-05:00         2022-02-11  \n...                     ...                            ...                ...  \n1997486 2023-02-01 03:43:00  2023-02-01T03:43:02.000+05:30         2023-02-01  \n1997487 2023-02-01 03:48:00  2023-02-01T03:48:02.000+05:30         2023-02-01  \n1997488 2023-02-01 04:03:00  2023-02-01T04:03:02.000+05:30         2023-02-01  \n1997489 2023-02-01 04:08:00  2023-02-01T04:08:02.000+05:30         2023-02-01  \n1997490 2023-02-01 04:28:00  2023-02-01T04:28:04.000+05:30         2023-02-01  \n\n[1997491 rows x 6 columns] of type <class 'pandas.core.frame.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "interpolation(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c9ee8b-087e-4fd4-9360-3154fcb213fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tolerance = .01\n",
    "fancy = merge_df.groupby(merge_df['GlucoseDisplayTime'].mul(tolerance).round()).max().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267dbde-fdaf-45c0-b19c-6b237167ccaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(merged_schema, PandasUDFType.GROUPED_MAP)\n",
    "def test(df):\n",
    "    # min_date = df.GlucoseDisplayTime.min()\n",
    "    # max_date = df.GlucoseDisplayTime.max()\n",
    "    merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "    for idx in range(df):\n",
    "        row = min_max[min_max.Index == idx].iloc[0]\n",
    "        date_df = pd.DataFrame(pd.date_range(row.MinDate, row.MaxDate, freq='5min'), columns=['GlucoseDisplayTime'])                              \n",
    "        date_df['NumId']= row.NumId\n",
    "        print('asdfasdf')\n",
    "        merge_df = pd.concat([merge_df, date_df], axis=0)\n",
    "        \n",
    "    return merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f72009-d89d-4ee9-b939-e6e9f6b0da26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped = min_max.groupby('NumId').apply(test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f34372ed-5c92-4f88-a4c7-aa757d5206cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "@pandas_udf(merged_schema, PandasUDFType.GROUPED_MAP)\n",
    "def test(df):\n",
    "    min_date = df.GlucoseDisplayTime.min()\n",
    "    max_date = df.GlucoseDisplayTime.max()\n",
    "    df['dateRange'] = pd.Series(pd.date_range(min_date, max_date, freq='5min'))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39707271-87a1-4712-ad76-9a2fe3a74476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785858f-65e6-4745-9b9b-435d2c6c0329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cf4a4-1676-40e6-889a-6c44d8d37170",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def date_range_def(minDate, maxDate):    \n",
    "    return ps.date_range(minDate, maxDate, freq='5min')\n",
    "\n",
    "date_range_udf = udf(lambda x,y: date_range_def(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb6603-5dda-4f7c-a0fb-bc64f165fcac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max = min_max.withColumn('Range', date_range_udf(col('MinDate'), col('MaxDate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104e769-3eba-4a06-a3f9-0b92e95c5cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_max.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba149e8d-b1e1-4e73-8758-eefd381ef723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates = spark.sql(\"SELECT sequence(CAST('2022-04-08 07:49:00' AS DATE), CAST('2022-10-01 07:47:00' AS DATE), interval 5 minutes) as date\").withColumn(\"date\", explode(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d5208-a681-4098-af66-245b0d3e552c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14d02d7-e552-4d6e-b7b4-48d638587c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pandas_udf(PandasUDFType.GROUPED_MAP)\n",
    "def replace_missing(subset):\n",
    "    \"\"\" INPUT\n",
    "        subset:     spark DataFrame with 1 patient\n",
    "        OUTPUT\n",
    "        filled_df:  spark DataFrame with 1 patient (-1 columns) and all missing rows filled in; not sorted\n",
    "    \"\"\"\n",
    "\n",
    "    '''get first and last date (takes about 10 seconds per ten days of one patient)'''\n",
    "    minimum = subset.agg({'GlucoseDisplayTime': 'min'}).collect()[0][0]\n",
    "    maximum = subset.agg({'GlucoseDisplayTime': 'max'}).collect()[0][0]\n",
    "\n",
    "    '''make a range that fills all those in'''\n",
    "    def date_range_list(start_date, end_date):\n",
    "        if start_date > end_date:\n",
    "            raise ValueError(\"start_date must come before end_date\")\n",
    "\n",
    "        datetime_list = []\n",
    "        curr_date = start_date\n",
    "        while curr_date <= end_date:\n",
    "            datetime_list.append([curr_date])\n",
    "            curr_date += timedelta(minutes=5)\n",
    "        return datetime_list\n",
    "\n",
    "    datetime_list = date_range_list(minimum, maximum)\n",
    "\n",
    "    '''make a dataframe of those dates'''\n",
    "    deptSchema = StructType([       \n",
    "        StructField('GlucoseDisplayTime', TimestampType(), True)\n",
    "    ])\n",
    "    dt_df = self.spark.createDataFrame(data=datetime_list, schema=deptSchema)\n",
    "\n",
    "    '''merge og dataframe back into the new one'''\n",
    "    merged = subset.unionByName(dt_df, allowMissingColumns=True)\n",
    "\n",
    "    '''get rid of the timestamps we already have (using the exact same method as from \"drop duplicate datetimes for each patient\" above)'''\n",
    "    window = Window.partitionBy('GlucoseDisplayTime').orderBy('tiebreak')\n",
    "    merged = (merged\n",
    "     .withColumn('tiebreak', monotonically_increasing_id())\n",
    "     .withColumn('rank', rank().over(window))\n",
    "     .filter(col('rank') == 1).drop('rank','tiebreak')\n",
    "    )\n",
    "\n",
    "    '''filling out the columns as needed:\n",
    "        -PatientId should be all the same string\n",
    "        -GlucoseDisplayTimeRaw should be used for checking the dates here, but implementation will have to come later'''\n",
    "    merged = merged.fillna(patient_str, subset='PatientId')\n",
    "    merged = merged.drop('GlucoseDisplayTimeRaw') #someday i'll have time to use this as the double-checker\n",
    "    merged = merged.withColumn('GlucoseDisplayDate',\n",
    "                               to_date(col('GlucoseDisplayTime')))\n",
    "\n",
    "    \"\"\" ============== FILL IN MISSING VALUES ============== \"\"\"\n",
    "    # filler = subset.agg({'Value': 'median'}).collect()[0][0]\n",
    "    filler = subset.agg({'Value': 'avg'}).collect()[0][0]\n",
    "\n",
    "    filled_df = merged.fillna(filler, subset='Value')\n",
    "\n",
    "    return filled_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13e2ba3-087b-407e-bebb-a70ed4b44b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test_df.groupBy('NumId').apply(lambda x: replace_missing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4d55ca-85c1-451f-9669-b5c11d7d2e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
