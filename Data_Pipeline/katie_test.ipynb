{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d4e2c18-cdd4-4603-b3ad-0b2f4dee2815",
   "metadata": {},
   "source": [
    "### testing fill missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8927dcb9-c531-4aad-826d-17b3c7c09655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pathlib\n",
    "import time\n",
    "import pyspark.pandas as ps\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datetime import date, datetime, timedelta\n",
    "from pyspark.pandas.indexes.datetimes import DatetimeIndex\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col,isnan,when,count, when, col, rank, lit, monotonically_increasing_id, date_trunc, udf, min, max, explode\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType, FloatType, DateType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "587678ae-3b0d-4dc7-b9ca-eb763de9e8b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 03:33:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAll([\\\n",
    "    ('spark.app.name', 'ReduceData')])\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e655f3f4-200c-4411-af71-86d789750291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glucose_data_schema=StructType([StructField('NumId', IntegerType(), True),\n",
    "                                        StructField('PatientId', StringType(), True),\n",
    "                                        StructField('Value', FloatType(), True),\n",
    "                                        StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                        StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                        StructField('GlucoseDisplayDate', DateType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ff65b08-eab2-47e5-8509-f249e1b098be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39244866371154785\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "data_location = \"/cephfs/train_test_val/val\"\n",
    "allPaths = [str(x) for x in list(pathlib.Path(data_location).glob('*.parquet')) if 'part-00' in str(x)]\n",
    "#allPaths.sort()\n",
    "# print(allPaths)\n",
    "\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b86a8390-bfef-43d3-8554-1bc6fc71a12b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d37681c9-56da-440a-9a18-ee67003528fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.098076820373535\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "path_counter = 0\n",
    "for path in allPaths:\n",
    "    gluc = pd.read_parquet(path, columns=['NumId','GlucoseDisplayTime', 'Value', 'GlucoseDisplayDate'])\n",
    "    gluc['GlucoseDisplayTime'] = gluc['GlucoseDisplayTime'].dt.floor('Min')\n",
    "    gluc = gluc.sort_values(by=['NumId', 'GlucoseDisplayTime'])\n",
    "\n",
    "    min_max = gluc.groupby('NumId')\\\n",
    "                    .agg({'GlucoseDisplayTime' : ['min','max']})\n",
    "    \n",
    "    merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "    starttime = time.time()\n",
    "    last_idx = len(min_max)-1\n",
    "\n",
    "    index_counter = 0\n",
    "    for idx, row in min_max.iterrows():\n",
    "        #grab all potential dates in range\n",
    "        \n",
    "        min_val = row['GlucoseDisplayTime']['min']\n",
    "        max_val = row['GlucoseDisplayTime']['max']\n",
    "        \n",
    "        date_df = pd.DataFrame(pd.date_range(min_val, max_val, freq='5min'),\\\n",
    "                               columns=['GlucoseDisplayTime'])  \n",
    "\n",
    "        # merge dates with big pypsark df\n",
    "        id_df = gluc[gluc['NumId'] == idx]\n",
    "        \n",
    "        mean = id_df.Value.mean()\n",
    "        \n",
    "        id_df.set_index('GlucoseDisplayTime', inplace=True)    \n",
    "\n",
    "        date_df.set_index('GlucoseDisplayTime', inplace=True)\n",
    "\n",
    "        merged = id_df.join(date_df, how='outer',\\\n",
    "                            on='GlucoseDisplayTime', sort=True)\n",
    "        \n",
    "        merged['IsFilledIn'] = 0\n",
    "        merged.loc[merged.Value.isna(), 'IsFilledIn'] = 1        \n",
    "        merged.loc[merged.Value.isna(), 'Value'] = mean\n",
    "        merged.loc[merged.GlucoseDisplayDate.isna(), 'GlucoseDisplayDate'] = merged.loc[merged.GlucoseDisplayDate.isna()]['GlucoseDisplayTime'].dt.date\n",
    "        \n",
    "        merged['NumId'] = idx\n",
    "        \n",
    "        merged.reset_index(inplace=True)\n",
    "\n",
    "        merged = merged.drop(columns=['index'])\n",
    "        \n",
    "        merged['TimeLag'] = np.concatenate((merged['GlucoseDisplayTime'].iloc[0],\\\n",
    "                                            np.array(merged['GlucoseDisplayTime'].iloc[:-1].values)), axis=None)\\\n",
    "                            .astype('datetime64[ns]')\n",
    "\n",
    "        merged['Diff'] = (merged['TimeLag'] - merged['GlucoseDisplayTime']).dt.seconds\n",
    "\n",
    "        len_merged = len(merged)\n",
    "\n",
    "        # get all index of rows with diff less than 5 mins, add 1 to remove next row, \n",
    "        # dont include last row to delete\n",
    "        indexes_to_remove = [x for x in merged[merged['Diff'] < 300].index + 1 if x < len_merged & x != 0]\n",
    "\n",
    "        if len(indexes_to_remove) > 0:\n",
    "            merged = merged.drop(indexes_to_remove)\n",
    "\n",
    "        # its ready freddy for some interpoletty\n",
    "        # merged DF is the dataframe ready to go into interpolation function\n",
    "\n",
    "        # fill with mean\n",
    "\n",
    "        merged = merged.drop(columns=['TimeLag', 'Diff'])\n",
    "        \n",
    "        if ((index_counter % 25 != 0) and index_counter != last_idx) or (index_counter == 0):\n",
    "            merge_df = pd.concat([merge_df, merged])\n",
    "        elif (index_counter % 25 == 0) or (index_counter == last_idx):\n",
    "            merge_df = merge_df.astype({'GlucoseDisplayTime': 'datetime64[ns]'})\n",
    "            \n",
    "            merge_df.to_parquet('/cephfs/interpolation/val/parquet_' + str(path_counter) + '_' + str(index_counter) + '.parquet')\n",
    "            merge_df = pd.DataFrame(columns=['GlucoseDisplayTime', 'NumId'])\n",
    "            \n",
    "        index_counter += 1\n",
    "        \n",
    "    path_counter += 1\n",
    "    \n",
    "print(time.time()-starttime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
