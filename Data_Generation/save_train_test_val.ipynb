{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47549336-fd80-4096-8632-5720c21074cd",
   "metadata": {},
   "source": [
    "# Save raw data to 60-20-20 split for train-test-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0e015a-3772-4b76-9e8c-e040be0d956a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pathlib\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import when, col, to_date, date_trunc, rank, monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType, FloatType\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78913a50-46fb-4a16-9335-ba050c6f7426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 23:48:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkContext().setCheckpointDir(dirName=\"/cephfs/train_test_val/_checkpoint\")\n",
    "\n",
    "conf = pyspark.SparkConf().setAll([\\\n",
    "    ('spark.app.name', 'ReduceData')])\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaed09d-fa5e-4784-a365-718748523cf9",
   "metadata": {},
   "source": [
    "### Structs we might need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762323e2-8854-4f02-bf2c-719d6c17c742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glucose_data_schema=StructType([StructField('PatientId', StringType(), True),\n",
    "                                StructField('Value', FloatType(), True),\n",
    "                                StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                StructField('GlucoseDisplayDate', DateType(), True),\n",
    "                                StructField('NumId', IntegerType(), True)\n",
    "                                ])\n",
    "\n",
    "raw_schema=StructType([StructField('_c0', IntegerType(),True),\n",
    "                                StructField('PostDate', TimestampType(),True),\n",
    "                                StructField('IngestionDate', TimestampType(),True),\n",
    "                                StructField('PostId', StringType(),True),\n",
    "                                StructField('PostTime', TimestampType(), True),\n",
    "                                StructField('PatientId', StringType(), True),\n",
    "                                StructField('Stream', StringType(), True),\n",
    "                                StructField('SequenceNumber', StringType(), True),\n",
    "                                StructField('TransmitterNumber', StringType(), True),\n",
    "                                StructField('ReceiverNumber', StringType(), True),\n",
    "                                StructField('RecordedSystemTime', TimestampType(), True),\n",
    "                                StructField('RecordedDisplayTime', TimestampType(), True),\n",
    "                                StructField('RecordedDisplayTimeRaw', TimestampType(), True),\n",
    "                                StructField('TransmitterId', StringType(), True),\n",
    "                                StructField('TransmitterTime', StringType(), True),\n",
    "                                StructField('GlucoseSystemTime', TimestampType(), True),\n",
    "                                StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                StructField('Value', FloatType(), True),\n",
    "                                StructField('Status', StringType(), True),\n",
    "                                StructField('TrendArrow', StringType(), True),\n",
    "                                StructField('TrendRate', FloatType(), True),\n",
    "                                StructField('IsBackFilled', StringType(), True),\n",
    "                                StructField('InternalStatus', StringType(), True),\n",
    "                                StructField('SessionStartTime', StringType(), True)])\n",
    "\n",
    "cohortSchema = StructType([StructField('', IntegerType(), True),\n",
    "                        StructField('UserId', StringType(), True),\n",
    "                        StructField('Gender', StringType(), True),\n",
    "                        StructField('DOB', TimestampType(), True),\n",
    "                        StructField('Age', IntegerType(), True),\n",
    "                        StructField('DiabetesType', StringType(), True),\n",
    "                        StructField('Treatment', StringType(), True)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e940d3-d3cc-4fe7-b7bc-b794b8090882",
   "metadata": {},
   "source": [
    "### Get all paths in to read from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4449441-67a3-44c2-a7a4-316ae54aade4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all parquets of the patient data'''\n",
    "# allPaths = [str(x) for x in list(pathlib.Path('/cephfs/data_by_patient').glob('*.parquet')) if 'part-00' in str(x)]\n",
    "allPaths = [str(x) for x in list(pathlib.Path('/cephfs/data').glob('*.csv')) if 'glucose_records' in str(x)]\n",
    "\n",
    "allPaths.sort()\n",
    "\n",
    "# print(\"train length:\", len(trainPaths), \"\\nvalidation length:\", len(valPaths),\"\\ntest length:\", len(testPaths))\n",
    "\n",
    "# print(allPaths[-1])\n",
    "# allPaths = allPaths[:-1]\n",
    "len(allPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715afa5-65cf-4af6-b591-ecb88b63733b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read in the Cohort dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c0e057-fdfc-4998-b8e8-521f982e7457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NumId', 'int'), ('UserId', 'string'), ('Gender', 'string'), ('DOB', 'timestamp'), ('Age', 'int'), ('DiabetesType', 'string'), ('Treatment', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+-------------------+---+------------+---------+\n",
      "|NumId|              UserId|Gender|                DOB|Age|DiabetesType|Treatment|\n",
      "+-----+--------------------+------+-------------------+---+------------+---------+\n",
      "|    0|5lZPrCk6qk8L6Jw+S...|Female|1931-01-01 00:00:00| 92|    type-two|       no|\n",
      "|    1|9qY9mZ+GV5Kd/O/NB...|  Male|1937-01-01 00:00:00| 86|    type-two|       no|\n",
      "|    2|uhsyLhr4Zl6NfGbNB...|Female|1938-01-01 00:00:00| 85|    type-two|       no|\n",
      "|    3|9uAVHBOgoCJ9hfcrL...|  Male|1938-01-01 00:00:00| 85|    type-two|       no|\n",
      "|    4|Fyb156jU1edGykL7N...|Female|1939-01-01 00:00:00| 84|    type-two|       no|\n",
      "|    5|86XfZ0fNI0VWOzWrl...|Female|1939-01-01 00:00:00| 84|    type-two|       no|\n",
      "|    6|JfJMH1qCpiYNuPOp/...|Female|1940-01-01 00:00:00| 83|    type-two|       no|\n",
      "|    7|EkW0PD80req7mL/5S...|  Male|1940-01-01 00:00:00| 83|    type-two|       no|\n",
      "|    8|OyqSKorAj1OPZaevj...|Female|1941-01-01 00:00:00| 82|    type-two|       no|\n",
      "|    9|4n03YE5Q5c2Ge1O4+...|Female|1941-01-01 00:00:00| 82|    type-two|       no|\n",
      "+-----+--------------------+------+-------------------+---+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "20.66317057609558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in cohort dataframe, with Number ID properly labeled\n",
    "startTime = time.time()\n",
    "\n",
    "cohortDf = spark.read.options(delimiter=',')\\\n",
    "        .csv('/cephfs/data/cohort.csv', header=True, schema=cohortSchema)\\\n",
    "        .withColumnRenamed('', 'NumId')\n",
    "\n",
    "print(cohortDf.dtypes)\n",
    "cohortDf.show(10)\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcd870b0-daaf-4b9f-bbec-be16948f0da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make mini dataframe of the string IDs and number IDs\n",
    "patientIds = cohortDf.select(col('UserId'), col('NumId')).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22241605-090f-4864-a9a5-99a784a051c1",
   "metadata": {},
   "source": [
    "### Load in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fc21e3c-1115-4297-a27b-acaa552709a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:======================================>               (262 + 17) / 365]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92893648147583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# df = spark.read \\\n",
    "#            .schema(glucose_data_schema) \\\n",
    "#            .format('parquet') \\\n",
    "#            .load(allPaths)\n",
    "df = spark.read\\\n",
    "    .format('csv')\\\n",
    "    .option('delimiter', ',')\\\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .schema(raw_schema)\\\n",
    "    .load(allPaths)\\\n",
    "    .select(col(\"PatientId\"), col(\"Value\"), \\\n",
    "            col(\"GlucoseDisplayTime\"))\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe06596-8561-45b1-bd0a-aebfe4298187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PatientId: string (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- GlucoseDisplayTime: timestamp (nullable = true)\n",
      "\n",
      "0.001371622085571289\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22ee54-5468-44c3-bdc7-7e3e85283923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5977 total patients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count:  3323456\n",
      "+--------------------+-----+--------------------+\n",
      "|           PatientId|Value|  GlucoseDisplayTime|\n",
      "+--------------------+-----+--------------------+\n",
      "|1Jxgxke6R3Uh2c9aR...|  0.0|2022-02-01 14:45:...|\n",
      "|toBStbTTYI2GU28Yd...|  0.0|2022-02-01 17:46:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 14:58:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 22:53:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 22:38:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 05:03:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 09:33:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 16:38:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 20:58:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 23:29:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 03:48:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 11:28:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 20:08:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 10:43:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 06:43:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 02:13:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 23:08:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 19:48:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 04:28:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 07:53:...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "3.796643018722534\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "patIds = [i.PatientId for i in df.select('PatientId').distinct().collect()]\n",
    "print(len(patIds), \"total patients\")\n",
    "print(\"row count: \", df.count())\n",
    "df.show()\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5134c-dae7-4aac-8c80-6370c7936583",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f72a60-8a8a-4ac5-92c1-501027484aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2772250175476074\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"the following cell is leslie's cleanup code, yoinked from fill-missing to read-data to here\"\"\"\n",
    "startTime = time.time()\n",
    "\n",
    "'''get rid of any dates from before the actual start-date of Feb 1, 2022'''\n",
    "df = df.where(\"GlucoseDisplayTime > '2022-01-31 23:59:59'\")\n",
    "\n",
    "'''replace 0s with NaN and dropna'''\n",
    "df = df.withColumn(\"Value\", when(col(\"Value\")==\"0\", None) \\\n",
    "                                                       .otherwise(col(\"Value\")))\n",
    "df = df.na.drop(subset=['PatientId','Value','GlucoseDisplayTime'])\n",
    "# df = df.where(df.Value>0)\n",
    "\n",
    "df = df.withColumn(\"GlucoseDisplayTime\",\n",
    "                   date_trunc(\"minute\",\n",
    "                   col(\"GlucoseDisplayTime\")))\n",
    "\n",
    "'''drop duplicate datetimes for each patient'''\n",
    "window = Window.partitionBy('GlucoseDisplayTime','PatientId').orderBy('tiebreak')\n",
    "df = (df\n",
    " .withColumn('tiebreak', monotonically_increasing_id())\n",
    " .withColumn('rank', rank().over(window))\n",
    " .filter(col('rank') == 1).drop('rank','tiebreak')\n",
    ")\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d27182-392b-4940-b6af-1922a85a2f16",
   "metadata": {},
   "source": [
    "### **CHECKPOINT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2ccf1-be2e-4118-a8e0-e504d6fa4689",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===>                                                 (183 + 16) / 2814]\r"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "df = df.checkpoint()\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528cf3e8-7811-4a30-8f77-42877caf8d1f",
   "metadata": {},
   "source": [
    "### Add in (join) the NumId column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ec259f4-3b80-41fa-9a3a-39af036fc8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08810687065124512\n"
     ]
    }
   ],
   "source": [
    "'''add numId to df'''\n",
    "startTime = time.time()\n",
    "\n",
    "df = df.join(patientIds, df.PatientId == patientIds.UserId)\\\n",
    "            .select(df.PatientId, patientIds.NumId, df.Value, df.GlucoseDisplayTime)\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c9756-5e17-46c6-98c0-b83ec17c4641",
   "metadata": {},
   "source": [
    "## Split into Train-Test-Val groups of 60-20-20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc68e8b-1aa8-431a-bbf0-95be6209b98c",
   "metadata": {},
   "source": [
    "plan:\n",
    "* get total count of values per patient\n",
    "* make 2 cols:\n",
    "    * int that's the total number of rows that should make up 60% of the patient's data\n",
    "    * int that's the total number of rows that should make up 80% of the patient's data\n",
    "* merge that dataframe back onto the main df dataframe\n",
    "* (is it possible to filter based on another column?)\n",
    "* add ranks based on patient ID\n",
    "* filter once that splits at the 60% mark, asking for 'rank'< or > to the 60% mark\n",
    "    * save the <60% as training\n",
    "* filter the >60% again on the 80% mark\n",
    "    * save the <80% as validation\n",
    "    * save the >80% as test\n",
    "\n",
    "somewhere in there, get rid of patients with less than 80% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc19d043-f14b-41d3-bf63-0a1d2424d159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum number of glucose recordings we should be allowing are 84096.0\n"
     ]
    }
   ],
   "source": [
    "'''get total counts of values per patient'''\n",
    "counter = df.groupBy('NumId').count()\n",
    "# print(\"row count: \", counter.count())\n",
    "\n",
    "'''(?) filter out patients with too little usable data'''\n",
    "minUsable = 0.80 * (60/5 * 24 * 365)\n",
    "# temp = counter.filter(col('count') < minUsable)\n",
    "print(\"the minimum number of glucose recordings we should be allowing are\", minUsable)\n",
    "# print(\"but there are only\", temp.count(), \"patients that have at least that much.\")\n",
    "# counter.write.parquet('temp_data/')\n",
    "\n",
    "'''get the numbers that indicate where the splits between train-val and val-test will be'''\n",
    "counter = counter.withColumn(\"split60\",(col(\"count\")* 0.6).cast(\"Integer\"))\n",
    "counter = counter.withColumn(\"split80\",(col(\"count\")* 0.8).cast(\"Integer\"))\n",
    "counter = counter.drop('count')\n",
    "\n",
    "# '''if i want to merge both small dataframes first before merging on the big main df'''\n",
    "# patientIds = patientIds.join(counter, patientIds.UserId == counter.PatientId)\\\n",
    "#             .select(patientIds.NumId, patientIds.UserId, counter.split60, counter.split80)\n",
    "\n",
    "'''rename for future merge (will get an \"ambiguous\" error without this)'''\n",
    "counter = counter.withColumnRenamed(\"NumId\",\"UserId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0ecdffc-f208-46b5-9701-49356a6b5d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1132810115814209\n"
     ]
    }
   ],
   "source": [
    "'''get everything into order for ranking/sorting by 60%-20%-20%'''\n",
    "startTime = time.time()\n",
    "\n",
    "df = df.join(counter, df.NumId == counter.UserId)\\\n",
    "            .select(df.PatientId, df.NumId, df.Value, df.GlucoseDisplayTime, \\\n",
    "                    counter.split60, counter.split80)\n",
    "\n",
    "df = df.orderBy(\"NumId\", \"GlucoseDisplayTime\", ascending=True)\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6020cd82-3a1e-4469-b322-b772f91cd4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.058053016662597656\n"
     ]
    }
   ],
   "source": [
    "'''rank, then next cell is filtering'''\n",
    "startTime = time.time()\n",
    "\n",
    "window = Window.partitionBy('PatientId').orderBy('tiebreak')\n",
    "df = df \\\n",
    " .withColumn('tiebreak', monotonically_increasing_id()) \\\n",
    " .withColumn('rank', rank().over(window))\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eecc0ac4-96b6-47c9-8d2f-1d101e86cf20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''training set'''\n",
    "trainSet = df.filter(col('rank') <= col('split60')) \\\n",
    "                 .drop('rank','tiebreak','split60','split80')\n",
    "\n",
    "'''validation set'''\n",
    "valSet = df.filter((col('rank') > col('split60')) & (col('rank') <= col('split80'))) \\\n",
    "               .drop('rank','tiebreak','split60','split80')\n",
    "\n",
    "'''test set'''\n",
    "testSet = df.filter(col('rank') > col('split80')) \\\n",
    "                .drop('rank','tiebreak','split60','split80')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8274c0-fe09-430b-8ea9-40d037441999",
   "metadata": {},
   "source": [
    "## Save out into parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5adc7a54-1ab1-43b1-809f-10d78d6de56b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============>(2811 + 3) / 2814][Stage 8:>               (0 + 13) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1119.353s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 8.0 (TID 6001): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.353s][warning][gc,alloc] Executor task launch worker for task 2.0 in stage 8.0 (TID 5998): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.562s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 8.0 (TID 5997): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 8.0 in stage 8.0 (TID 6004): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 8.0 (TID 6003): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 11.0 in stage 8.0 (TID 6007): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 8.0 (TID 6000): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 8.0 (TID 6005): Retried waiting for GCLocker too often allocating 131074 words\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 11.0 in stage 8.0 (TID 6007)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 4.0 in stage 8.0 (TID 6000)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 7.0 in stage 8.0 (TID 6003)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 2.0 in stage 8.0 (TID 5998)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4142/0x0000000801fadf68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2895/0x0000000801dd69f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2571/0x0000000801d30dd8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 5.0 in stage 8.0 (TID 6001)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4142/0x0000000801fadf68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2895/0x0000000801dd69f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2571/0x0000000801d30dd8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 9.0 in stage 8.0 (TID 6005)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 1.0 in stage 8.0 (TID 5997)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 8.0 in stage 8.0 (TID 6004)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 8.0 (TID 6003),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 8.0 (TID 6004),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 8.0 (TID 6005),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 8.0 (TID 5997),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 11.0 in stage 8.0 (TID 6007),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 8.0 (TID 6000),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 8.0 (TID 5998),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4142/0x0000000801fadf68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2895/0x0000000801dd69f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2571/0x0000000801d30dd8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 8.0 (TID 6001),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4142/0x0000000801fadf68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2895/0x0000000801dd69f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2571/0x0000000801d30dd8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 8.0 in stage 8.0 (TID 6004) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "23/05/16 23:18:14 ERROR TaskSetManager: Task 8 in stage 8.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============>(2811 + 3) / 2814][Stage 8:>               (0 + 15) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 23:18:14 ERROR FileFormatWriter: Aborting job 414e7339-6f64-4779-88d6-28382fb4a3a3.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 8.0 failed 1 times, most recent failure: Lost task 8.0 in stage 8.0 (TID 6004) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 13.0 in stage 8.0 (TID 6009) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 14.0 in stage 8.0 (TID 6010) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 12.0 in stage 8.0 (TID 6008) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 10.0 in stage 8.0 (TID 6006) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 3.0 in stage 8.0 (TID 5999) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 5996) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 6.0 in stage 8.0 (TID 6002) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o200.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 8.0 failed 1 times, most recent failure: Lost task 8.0 in stage 8.0 (TID 6004) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# df.repartition('PatientId')\\\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     .write.parquet('/cephfs/train_test_val/test/') \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainSet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPatientId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/cephfs/train_test_val/train_set/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m valSet\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatientId\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cephfs/train_test_val/val_set/\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      8\u001b[0m testSet\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatientId\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cephfs/train_test_val/test_set/\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o200.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 8.0 failed 1 times, most recent failure: Lost task 8.0 in stage 8.0 (TID 6004) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "'''godspeed'''\n",
    "startTime = time.time()\n",
    "\n",
    "# df.repartition('PatientId')\\\n",
    "#     .write.parquet('/cephfs/train_test_val/test/') \n",
    "trainSet.repartition('PatientId').write.parquet('/cephfs/train_test_val/train_set/') \n",
    "# valSet.repartition('PatientId').write.parquet('/cephfs/train_test_val/val_set/') \n",
    "# testSet.repartition('PatientId').write.parquet('/cephfs/train_test_val/test_set/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ac3c7-a9b0-4340-a106-f6f8c3ab3ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "endTime-startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff34598e-bbbe-4a9a-9958-b42cf9d4727b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.854722222222223"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# endTime = time.time()\n",
    "# endTime-startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9adadff8-b934-40b4-89fc-256905ece957",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.805555555555555"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17300/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5abec12d-6820-40a1-9177-98ff0002a933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.74638888888889"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45887/60/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
