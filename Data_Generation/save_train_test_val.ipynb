{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47549336-fd80-4096-8632-5720c21074cd",
   "metadata": {},
   "source": [
    "# Save raw data to 60-20-20 split for train-test-validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0e015a-3772-4b76-9e8c-e040be0d956a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pathlib\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import when, col, to_date, date_trunc, rank, monotonically_increasing_id, min, max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DateType, FloatType\n",
    "\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78913a50-46fb-4a16-9335-ba050c6f7426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/17 03:49:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/17 03:49:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAll([\\\n",
    "    ('spark.app.name', 'ReduceData')])\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaed09d-fa5e-4784-a365-718748523cf9",
   "metadata": {},
   "source": [
    "### Structs we might need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762323e2-8854-4f02-bf2c-719d6c17c742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "glucose_data_schema=StructType([StructField('PatientId', StringType(), True),\n",
    "                                StructField('Value', FloatType(), True),\n",
    "                                StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                StructField('GlucoseDisplayDate', DateType(), True),\n",
    "                                StructField('NumId', IntegerType(), True)\n",
    "                                ])\n",
    "\n",
    "raw_schema=StructType([StructField('_c0', IntegerType(),True),\n",
    "                                StructField('PostDate', TimestampType(),True),\n",
    "                                StructField('IngestionDate', TimestampType(),True),\n",
    "                                StructField('PostId', StringType(),True),\n",
    "                                StructField('PostTime', TimestampType(), True),\n",
    "                                StructField('PatientId', StringType(), True),\n",
    "                                StructField('Stream', StringType(), True),\n",
    "                                StructField('SequenceNumber', StringType(), True),\n",
    "                                StructField('TransmitterNumber', StringType(), True),\n",
    "                                StructField('ReceiverNumber', StringType(), True),\n",
    "                                StructField('RecordedSystemTime', TimestampType(), True),\n",
    "                                StructField('RecordedDisplayTime', TimestampType(), True),\n",
    "                                StructField('RecordedDisplayTimeRaw', TimestampType(), True),\n",
    "                                StructField('TransmitterId', StringType(), True),\n",
    "                                StructField('TransmitterTime', StringType(), True),\n",
    "                                StructField('GlucoseSystemTime', TimestampType(), True),\n",
    "                                StructField('GlucoseDisplayTime', TimestampType(), True),\n",
    "                                StructField('GlucoseDisplayTimeRaw', StringType(), True),\n",
    "                                StructField('Value', FloatType(), True),\n",
    "                                StructField('Status', StringType(), True),\n",
    "                                StructField('TrendArrow', StringType(), True),\n",
    "                                StructField('TrendRate', FloatType(), True),\n",
    "                                StructField('IsBackFilled', StringType(), True),\n",
    "                                StructField('InternalStatus', StringType(), True),\n",
    "                                StructField('SessionStartTime', StringType(), True)])\n",
    "\n",
    "cohortSchema = StructType([StructField('', IntegerType(), True),\n",
    "                        StructField('UserId', StringType(), True),\n",
    "                        StructField('Gender', StringType(), True),\n",
    "                        StructField('DOB', TimestampType(), True),\n",
    "                        StructField('Age', IntegerType(), True),\n",
    "                        StructField('DiabetesType', StringType(), True),\n",
    "                        StructField('Treatment', StringType(), True)\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e940d3-d3cc-4fe7-b7bc-b794b8090882",
   "metadata": {},
   "source": [
    "### Get all paths in to read from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4449441-67a3-44c2-a7a4-316ae54aade4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''all CSVs of the raw data'''\n",
    "# allPaths = [str(x) for x in list(pathlib.Path('/cephfs/data_by_patient').glob('*.parquet')) if 'part-00' in str(x)]\n",
    "allPaths = [str(x) for x in list(pathlib.Path('/cephfs/data').glob('*.csv')) if 'glucose_records' in str(x)]\n",
    "\n",
    "allPaths.sort()\n",
    "\n",
    "# print(\"train length:\", len(trainPaths), \"\\nvalidation length:\", len(valPaths),\"\\ntest length:\", len(testPaths))\n",
    "\n",
    "# print(allPaths[-1])\n",
    "allPaths = allPaths[:3]\n",
    "len(allPaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715afa5-65cf-4af6-b591-ecb88b63733b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read in the Cohort dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07c0e057-fdfc-4998-b8e8-521f982e7457",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('UserId', 'string'), ('NumId', 'int')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|              UserId|NumId|\n",
      "+--------------------+-----+\n",
      "|afuXDu4gswOv1nPz8...|  252|\n",
      "|2xvLF6iyzUsM3KlN3...|  299|\n",
      "|/me7Mcqd+uJvuwNzH...|  574|\n",
      "|pRzpPZcuJxjdcDk9R...|  893|\n",
      "|GMkzjcKvy/rP0iyhV...| 1579|\n",
      "|ufcKFPML1EYMZBOmL...| 1929|\n",
      "|2MjuVdaQH+LpaKwGN...| 1931|\n",
      "|dJ8la8IA6j03CO4jS...| 2160|\n",
      "|iQ0rMtZFN6kDkjr4G...| 2460|\n",
      "|S/XdC0rnkBPEB1n6v...| 2656|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "6.270871877670288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in cohort dataframe, with Number ID properly labeled\n",
    "startTime = time.time()\n",
    "\n",
    "patientIds = spark.read.options(delimiter=',') \\\n",
    "                .csv('/cephfs/data/cohort.csv', header=True, schema=cohortSchema) \\\n",
    "                .withColumnRenamed('', 'NumId') \\\n",
    "                .select(col('UserId'), col('NumId')) \\\n",
    "                .distinct()\n",
    "\n",
    "print(patientIds.dtypes)\n",
    "patientIds.show(10)\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22241605-090f-4864-a9a5-99a784a051c1",
   "metadata": {},
   "source": [
    "### Load in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc21e3c-1115-4297-a27b-acaa552709a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4299309253692627\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# df = spark.read \\\n",
    "#            .schema(glucose_data_schema) \\\n",
    "#            .format('parquet') \\\n",
    "#            .load(allPaths)\n",
    "df = spark.read\\\n",
    "    .format('csv')\\\n",
    "    .option('delimiter', ',')\\\n",
    "    .option(\"mode\", \"DROPMALFORMED\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .schema(raw_schema)\\\n",
    "    .load(allPaths)\\\n",
    "    .select(col(\"PatientId\"), col(\"Value\"), \\\n",
    "            col(\"GlucoseDisplayTime\"))\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22ee54-5468-44c3-bdc7-7e3e85283923",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PatientId: string (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- GlucoseDisplayTime: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6036 total patients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count:  4994511\n",
      "+--------------------+-----+--------------------+\n",
      "|           PatientId|Value|  GlucoseDisplayTime|\n",
      "+--------------------+-----+--------------------+\n",
      "|1Jxgxke6R3Uh2c9aR...|  0.0|2022-02-01 14:45:...|\n",
      "|toBStbTTYI2GU28Yd...|  0.0|2022-02-01 17:46:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 14:58:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 22:53:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 22:38:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 05:03:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 09:33:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 16:38:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 20:58:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 23:29:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 03:48:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 11:28:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 20:08:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 10:43:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 06:43:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 02:13:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 23:08:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-01-31 19:48:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 04:28:...|\n",
      "|+XAhHhm+BkhqusxsZ...|  0.0|2022-02-01 07:53:...|\n",
      "+--------------------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "10.42384934425354\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "df.printSchema()\n",
    "patIds = [i.PatientId for i in df.select('PatientId').distinct().collect()]\n",
    "print(len(patIds), \"total patients\")\n",
    "print(\"row count: \", df.count())\n",
    "df.show()\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d5134c-dae7-4aac-8c80-6370c7936583",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04f72a60-8a8a-4ac5-92c1-501027484aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08103632926940918\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"the following cell is leslie's cleanup code, yoinked from fill-missing to read-data to here\"\"\"\n",
    "startTime = time.time()\n",
    "\n",
    "'''get rid of any dates from before the actual start-date of Feb 1, 2022'''\n",
    "df = df.where(\"GlucoseDisplayTime > '2022-01-31 23:59:59'\")\n",
    "\n",
    "'''replace 0s with NaN and dropna'''\n",
    "df = df.withColumn(\"Value\", when(col(\"Value\")==\"0\", None) \\\n",
    "                                                       .otherwise(col(\"Value\")))\n",
    "df = df.na.drop(subset=['PatientId','Value','GlucoseDisplayTime'])\n",
    "# df = df.where(df.Value>0)\n",
    "\n",
    "df = df.withColumn(\"GlucoseDisplayTime\",\n",
    "                   date_trunc(\"minute\",\n",
    "                   col(\"GlucoseDisplayTime\")))\n",
    "\n",
    "'''drop duplicate datetimes for each patient'''\n",
    "window = Window.partitionBy('GlucoseDisplayTime','PatientId').orderBy('tiebreak')\n",
    "df = (df\n",
    " .withColumn('tiebreak', monotonically_increasing_id())\n",
    " .withColumn('rank', rank().over(window))\n",
    " .filter(col('rank') == 1).drop('rank','tiebreak')\n",
    ")\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa0e3c8a-e1f4-45b0-8332-8e6334b93053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PatientId: string (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- GlucoseDisplayTime: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count:  4311678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=====================================================>   (17 + 1) / 18]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+\n",
      "|           PatientId|Value| GlucoseDisplayTime|\n",
      "+--------------------+-----+-------------------+\n",
      "|/rAxYocQpbKaUml2y...|266.0|2022-02-01 00:00:00|\n",
      "|1FAUCirnkLqrYiNWb...|177.0|2022-02-01 00:00:00|\n",
      "|1VZ3RiHSjH9RIy9GB...|121.0|2022-02-01 00:00:00|\n",
      "|1uUAsobV9i087qUq4...|295.0|2022-02-01 00:00:00|\n",
      "|2Oc9FE04nl4AbfE/z...|188.0|2022-02-01 00:00:00|\n",
      "|3A2/x042CHD09Plf7...|149.0|2022-02-01 00:00:00|\n",
      "|9IkmK0geGi4BXGhT8...|168.0|2022-02-01 00:00:00|\n",
      "|9urL8fduVWtONauiT...|116.0|2022-02-01 00:00:00|\n",
      "|AxWf3T1YEzDIOYKik...|141.0|2022-02-01 00:00:00|\n",
      "|CH4+gLYbwWF0+FUw1...| 73.0|2022-02-01 00:00:00|\n",
      "|CapkbhvAry1quSf4I...|124.0|2022-02-01 00:00:00|\n",
      "|DX0Pbdq+EBN4/PQIR...|222.0|2022-02-01 00:00:00|\n",
      "|DY0Ni7z8FswFAdcSl...|186.0|2022-02-01 00:00:00|\n",
      "|E5GUqb4eVAUBIU1EX...|314.0|2022-02-01 00:00:00|\n",
      "|ER+gxKFxHvXyfESG6...|128.0|2022-02-01 00:00:00|\n",
      "|F/JsFxzP/YSrZl3Fj...|125.0|2022-02-01 00:00:00|\n",
      "|FgSnoYnhijtsbDFNb...|209.0|2022-02-01 00:00:00|\n",
      "|GlILdY2V8rf7zvUqi...|235.0|2022-02-01 00:00:00|\n",
      "|If26A1LxlTVpzDcu5...|158.0|2022-02-01 00:00:00|\n",
      "|KHxBbw+3jHEQ61F97...|248.0|2022-02-01 00:00:00|\n",
      "+--------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "14.640056371688843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "df.printSchema()\n",
    "print(\"row count: \", df.count())\n",
    "df.show()\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d27182-392b-4940-b6af-1922a85a2f16",
   "metadata": {},
   "source": [
    "### **CHECKPOINT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c2ccf1-be2e-4118-a8e0-e504d6fa4689",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                       (0 + 16) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[495.625s][warning][gc,alloc] Executor task launch worker for task 13.0 in stage 4.0 (TID 3193): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[495.625s][warning][gc,alloc] Executor task launch worker for task 3.0 in stage 4.0 (TID 3183): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[495.833s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 4.0 (TID 3185): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[495.839s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 4.0 (TID 3181): Retried waiting for GCLocker too often allocating 131074 words\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 5.0 in stage 4.0 (TID 3185)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 3.0 in stage 4.0 (TID 3183)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$3715/0x0000000801df6f50.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2930/0x0000000801de5f20.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2608/0x0000000801d44000.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 13.0 in stage 4.0 (TID 3193)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$3715/0x0000000801df6f50.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2930/0x0000000801de5f20.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2608/0x0000000801d44000.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 1.0 in stage 4.0 (TID 3181)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 8.0 in stage 4.0 (TID 3188)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "[496.287s][warning][gc,alloc] Executor task launch worker for task 10.0 in stage 4.0 (TID 3190): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[496.293s][warning][gc,alloc] Executor task launch worker for task 2.0 in stage 4.0 (TID 3182): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[496.296s][warning][gc,alloc] Executor task launch worker for task 6.0 in stage 4.0 (TID 3186): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[496.296s][warning][gc,alloc] Executor task launch worker for task 0.0 in stage 4.0 (TID 3180): Retried waiting for GCLocker too often allocating 131074 words\n",
      "23/05/16 23:57:08 WARN TaskMemoryManager: Failed to allocate a page (1048576 bytes), try again.\n",
      "[496.306s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 4.0 (TID 3187): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[496.317s][warning][gc,alloc] Executor task launch worker for task 0.0 in stage 4.0 (TID 3180): Retried waiting for GCLocker too often allocating 131074 words\n",
      "23/05/16 23:57:08 WARN TaskMemoryManager: Failed to allocate a page (1048576 bytes), try again.\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 2.0 in stage 4.0 (TID 3182)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 6.0 in stage 4.0 (TID 3186)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 10.0 in stage 4.0 (TID 3190)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 7.0 in stage 4.0 (TID 3187)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 4.0 (TID 3185),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 4.0 (TID 3182),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 4.0 (TID 3181),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 4.0 (TID 3187),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 10.0 in stage 4.0 (TID 3190),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 6.0 in stage 4.0 (TID 3186),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 3.0 in stage 4.0 (TID 3183),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$3715/0x0000000801df6f50.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2930/0x0000000801de5f20.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2608/0x0000000801d44000.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 4.0 (TID 3188),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 13.0 in stage 4.0 (TID 3193),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:105)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:74)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$3715/0x0000000801df6f50.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2930/0x0000000801de5f20.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2608/0x0000000801d44000.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "[496.479s][warning][gc,alloc] Executor task launch worker for task 11.0 in stage 4.0 (TID 3191): Retried waiting for GCLocker too often allocating 131074 words\n",
      "23/05/16 23:57:08 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@320191b7 rejected from java.util.concurrent.ThreadPoolExecutor@7e6c5c6[Shutting down, pool size = 16, active threads = 16, queued tasks = 0, completed tasks = 3180]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:305)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/16 23:57:08 ERROR Executor: Exception in task 11.0 in stage 4.0 (TID 3191)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 11.0 in stage 4.0 (TID 3191),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:57:08 WARN TaskSetManager: Lost task 2.0 in stage 4.0 (TID 3182) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "23/05/16 23:57:08 ERROR TaskSetManager: Task 2 in stage 4.0 failed 1 times; aborting job\n",
      "23/05/16 23:57:09 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:143)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:139)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writePartitionToCheckpointFile$1(ReliableCheckpointRDD.scala:218)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writePartitionToCheckpointFile(ReliableCheckpointRDD.scala:225)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1$adapted(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/16 23:57:09 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:143)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:139)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writePartitionToCheckpointFile$1(ReliableCheckpointRDD.scala:218)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writePartitionToCheckpointFile(ReliableCheckpointRDD.scala:225)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1$adapted(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/16 23:57:09 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:143)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:139)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writePartitionToCheckpointFile$1(ReliableCheckpointRDD.scala:218)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writePartitionToCheckpointFile(ReliableCheckpointRDD.scala:225)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1$adapted(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/16 23:57:09 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:143)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:139)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writePartitionToCheckpointFile$1(ReliableCheckpointRDD.scala:218)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writePartitionToCheckpointFile(ReliableCheckpointRDD.scala:225)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1$adapted(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/16 23:57:09 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:143)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:139)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writePartitionToCheckpointFile$1(ReliableCheckpointRDD.scala:218)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writePartitionToCheckpointFile(ReliableCheckpointRDD.scala:225)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1$adapted(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/16 23:57:09 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:217)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.loadNext(UnsafeSorterSpillReader.java:110)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillMerger$1.loadNext(UnsafeSorterSpillMerger.java:87)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.advanceNext(UnsafeExternalRowSorter.java:187)\n",
      "\tat org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:143)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:139)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writePartitionToCheckpointFile$1(ReliableCheckpointRDD.scala:218)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1538)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writePartitionToCheckpointFile(ReliableCheckpointRDD.scala:225)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.rdd.ReliableCheckpointRDD$.$anonfun$writeRDDToCheckpointDirectory$1$adapted(ReliableCheckpointRDD.scala:167)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o116.checkpoint.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 3182) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writeRDDToCheckpointDirectory(ReliableCheckpointRDD.scala:166)\n\tat org.apache.spark.rdd.ReliableRDDCheckpointData.doCheckpoint(ReliableRDDCheckpointData.scala:60)\n\tat org.apache.spark.rdd.RDDCheckpointData.checkpoint(RDDCheckpointData.scala:75)\n\tat org.apache.spark.rdd.RDD.$anonfun$doCheckpoint$1(RDD.scala:1906)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDD.doCheckpoint(RDD.scala:1896)\n\tat org.apache.spark.sql.Dataset.$anonfun$checkpoint$1(Dataset.scala:687)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:678)\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:641)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m startTime)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:682\u001b[0m, in \u001b[0;36mDataFrame.checkpoint\u001b[0;34m(self, eager)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, eager: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    666\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a checkpointed version of this :class:`DataFrame`. Checkpointing can be used to\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    truncate the logical plan of this :class:`DataFrame`, which is especially useful in\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;124;03m    iterative algorithms where the plan may grow exponentially. It will be saved to files\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m    This API is experimental.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 682\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43meager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o116.checkpoint.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 4.0 failed 1 times, most recent failure: Lost task 2.0 in stage 4.0 (TID 3182) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2281)\n\tat org.apache.spark.rdd.ReliableCheckpointRDD$.writeRDDToCheckpointDirectory(ReliableCheckpointRDD.scala:166)\n\tat org.apache.spark.rdd.ReliableRDDCheckpointData.doCheckpoint(ReliableRDDCheckpointData.scala:60)\n\tat org.apache.spark.rdd.RDDCheckpointData.checkpoint(RDDCheckpointData.scala:75)\n\tat org.apache.spark.rdd.RDD.$anonfun$doCheckpoint$1(RDD.scala:1906)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDD.doCheckpoint(RDD.scala:1896)\n\tat org.apache.spark.sql.Dataset.$anonfun$checkpoint$1(Dataset.scala:687)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:678)\n\tat org.apache.spark.sql.Dataset.checkpoint(Dataset.scala:641)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "df = df.checkpoint()\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528cf3e8-7811-4a30-8f77-42877caf8d1f",
   "metadata": {},
   "source": [
    "### Add in (join) the NumId column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ec259f4-3b80-41fa-9a3a-39af036fc8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02255535125732422\n"
     ]
    }
   ],
   "source": [
    "'''add numId to df'''\n",
    "startTime = time.time()\n",
    "\n",
    "df = df.join(patientIds, df.PatientId == patientIds.UserId)\\\n",
    "            .select(df.PatientId, patientIds.NumId, df.Value, df.GlucoseDisplayTime)\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c9756-5e17-46c6-98c0-b83ec17c4641",
   "metadata": {},
   "source": [
    "## Split into Train-Test-Val groups of 60-20-20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc68e8b-1aa8-431a-bbf0-95be6209b98c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "plan:\n",
    "* get total count of values per patient\n",
    "* make 2 cols:\n",
    "    * int that's the total number of rows that should make up 60% of the patient's data\n",
    "    * int that's the total number of rows that should make up 80% of the patient's data\n",
    "* merge that dataframe back onto the main df dataframe\n",
    "* (is it possible to filter based on another column?)\n",
    "* add ranks based on patient ID\n",
    "* filter once that splits at the 60% mark, asking for 'rank'< or > to the 60% mark\n",
    "    * save the <60% as training\n",
    "* filter the >60% again on the 80% mark\n",
    "    * save the <80% as validation\n",
    "    * save the >80% as test\n",
    "\n",
    "somewhere in there, get rid of patients with less than 80% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc19d043-f14b-41d3-bf63-0a1d2424d159",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum number of glucose recordings we should be allowing are 84096.0\n"
     ]
    }
   ],
   "source": [
    "'''get total counts of values per patient'''\n",
    "counter = df.groupBy('NumId').count()\n",
    "# print(\"row count: \", counter.count())\n",
    "\n",
    "'''(?) filter out patients with too little usable data'''\n",
    "minUsable = 0.80 * (60/5 * 24 * 365)\n",
    "# temp = counter.filter(col('count') < minUsable)\n",
    "print(\"the minimum number of glucose recordings we should be allowing are\", minUsable)\n",
    "# print(\"but there are only\", temp.count(), \"patients that have at least that much.\")\n",
    "# counter.write.parquet('temp_data/')\n",
    "\n",
    "'''get the numbers that indicate where the splits between train-val and val-test will be'''\n",
    "counter = counter.withColumn(\"split60\",(col(\"count\")* 0.6).cast(\"Integer\"))\n",
    "counter = counter.withColumn(\"split80\",(col(\"count\")* 0.8).cast(\"Integer\"))\n",
    "counter = counter.drop('count')\n",
    "\n",
    "# '''if i want to merge both small dataframes first before merging on the big main df'''\n",
    "# patientIds = patientIds.join(counter, patientIds.UserId == counter.PatientId)\\\n",
    "#             .select(patientIds.NumId, patientIds.UserId, counter.split60, counter.split80)\n",
    "\n",
    "'''rename for future merge (will get an \"ambiguous\" error without this)'''\n",
    "counter = counter.withColumnRenamed(\"NumId\",\"UserId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0ecdffc-f208-46b5-9701-49356a6b5d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.050591230392456055\n"
     ]
    }
   ],
   "source": [
    "'''get everything into order for ranking/sorting by 60%-20%-20%'''\n",
    "startTime = time.time()\n",
    "\n",
    "df = df.join(counter, df.NumId == counter.UserId)\\\n",
    "            .select(df.PatientId, df.NumId, df.Value, df.GlucoseDisplayTime, \\\n",
    "                    counter.split60, counter.split80)\n",
    "\n",
    "df = df.orderBy(\"NumId\", \"GlucoseDisplayTime\", ascending=True)\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "021ffaed-9175-4f2b-b5a4-687486d17862",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PatientId: string (nullable = true)\n",
      " |-- NumId: integer (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- GlucoseDisplayTime: timestamp (nullable = true)\n",
      " |-- split60: integer (nullable = true)\n",
      " |-- split80: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count:  4311678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-------------------+-------+-------+\n",
      "|           PatientId|NumId|Value| GlucoseDisplayTime|split60|split80|\n",
      "+--------------------+-----+-----+-------------------+-------+-------+\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|155.0|2022-02-01 00:02:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|155.0|2022-02-01 00:07:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|154.0|2022-02-01 00:12:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|153.0|2022-02-01 00:17:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|153.0|2022-02-01 00:22:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|153.0|2022-02-01 00:27:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|151.0|2022-02-01 00:32:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|151.0|2022-02-01 00:37:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|149.0|2022-02-01 00:42:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|147.0|2022-02-01 00:47:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|145.0|2022-02-01 00:52:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|145.0|2022-02-01 00:57:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|146.0|2022-02-01 01:02:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|146.0|2022-02-01 01:07:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|146.0|2022-02-01 01:12:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|146.0|2022-02-01 01:17:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|146.0|2022-02-01 01:22:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|145.0|2022-02-01 01:27:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|144.0|2022-02-01 01:32:00|    481|    641|\n",
      "|5lZPrCk6qk8L6Jw+S...|    0|144.0|2022-02-01 01:37:00|    481|    641|\n",
      "+--------------------+-----+-----+-------------------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "17.974385738372803\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "df.printSchema()\n",
    "print(\"row count: \", df.count())\n",
    "df.show()\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6020cd82-3a1e-4469-b322-b772f91cd4b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02889847755432129\n"
     ]
    }
   ],
   "source": [
    "'''rank, then next cell is filtering'''\n",
    "startTime = time.time()\n",
    "\n",
    "window = Window.partitionBy('PatientId').orderBy('GlucoseDisplayTime')\n",
    "df = df \\\n",
    " .withColumn('rank', rank().over(window))\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fbe650d-06d6-4cf0-8628-084eadbd5200",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PatientId: string (nullable = true)\n",
      " |-- NumId: integer (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- GlucoseDisplayTime: timestamp (nullable = true)\n",
      " |-- split60: integer (nullable = true)\n",
      " |-- split80: integer (nullable = true)\n",
      " |-- rank: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row count:  4311678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 126:===================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-------------------+-------+-------+----+\n",
      "|           PatientId|NumId|Value| GlucoseDisplayTime|split60|split80|rank|\n",
      "+--------------------+-----+-----+-------------------+-------+-------+----+\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|196.0|2022-02-01 00:02:00|    465|    620|   1|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|193.0|2022-02-01 00:07:00|    465|    620|   2|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|192.0|2022-02-01 00:12:00|    465|    620|   3|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|193.0|2022-02-01 00:17:00|    465|    620|   4|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|191.0|2022-02-01 00:22:00|    465|    620|   5|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|189.0|2022-02-01 00:27:00|    465|    620|   6|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|186.0|2022-02-01 00:32:00|    465|    620|   7|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|183.0|2022-02-01 00:37:00|    465|    620|   8|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|180.0|2022-02-01 00:42:00|    465|    620|   9|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|177.0|2022-02-01 00:47:00|    465|    620|  10|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|175.0|2022-02-01 00:52:00|    465|    620|  11|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|173.0|2022-02-01 00:57:00|    465|    620|  12|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|170.0|2022-02-01 01:02:00|    465|    620|  13|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|166.0|2022-02-01 01:07:00|    465|    620|  14|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|163.0|2022-02-01 01:12:00|    465|    620|  15|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|160.0|2022-02-01 01:17:00|    465|    620|  16|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|158.0|2022-02-01 01:22:00|    465|    620|  17|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|155.0|2022-02-01 01:27:00|    465|    620|  18|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|153.0|2022-02-01 01:32:00|    465|    620|  19|\n",
      "|+Gr/1qOf9OWMa4LOL...| 4660|151.0|2022-02-01 01:37:00|    465|    620|  20|\n",
      "+--------------------+-----+-----+-------------------+-------+-------+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "22.082598447799683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "df.printSchema()\n",
    "print(\"row count: \", df.count())\n",
    "df.show()\n",
    "\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eecc0ac4-96b6-47c9-8d2f-1d101e86cf20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''training set'''\n",
    "trainSet = df.filter(col('rank') <= col('split60')) \\\n",
    "                 .drop('rank','tiebreak','split60','split80')\n",
    "\n",
    "'''validation set'''\n",
    "valSet = df.filter((col('rank') > col('split60')) & (col('rank') <= col('split80'))) \\\n",
    "               .drop('rank','tiebreak','split60','split80')\n",
    "\n",
    "'''test set'''\n",
    "testSet = df.filter(col('rank') > col('split80')) \\\n",
    "                .drop('rank','tiebreak','split60','split80')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2368eae5-5be2-4721-9058-21af177489fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 218:=========>                                             (3 + 14) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+\n",
      "|NumId|                Max|                Min|\n",
      "+-----+-------------------+-------------------+\n",
      "| 4818|2022-02-02 15:19:00|2022-02-01 00:04:00|\n",
      "| 7754|2022-02-02 15:47:00|2022-02-01 00:02:00|\n",
      "| 2235|2022-02-02 19:17:00|2022-02-01 11:07:00|\n",
      "| 4219|2022-02-02 15:28:00|2022-02-01 00:03:00|\n",
      "| 2711|2022-02-02 20:28:00|2022-02-01 00:03:00|\n",
      "+-----+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp = trainSet.groupby('NumId')\\\n",
    "            .agg(max(\"GlucoseDisplayTime\").alias(\"Max\"),\n",
    "                min(\"GlucoseDisplayTime\").alias(\"Min\"))\n",
    "temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "673f339b-15a8-4023-8282-097a7a4f599d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 249:===================================================>   (16 + 1) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+\n",
      "|NumId|                Max|                Min|\n",
      "+-----+-------------------+-------------------+\n",
      "| 4818|2022-02-03 04:24:00|2022-02-02 15:24:00|\n",
      "| 7754|2022-02-03 04:47:00|2022-02-02 15:52:00|\n",
      "| 2235|2022-02-03 04:18:00|2022-02-02 19:22:00|\n",
      "| 4219|2022-02-03 04:38:00|2022-02-02 15:33:00|\n",
      "| 2711|2022-02-03 07:33:00|2022-02-02 20:33:00|\n",
      "+-----+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp = valSet.filter(col('NumId').isin([4818, 7754, 2235, 4219, 2711]))\n",
    "temp = temp.groupby('NumId')\\\n",
    "            .agg(max(\"GlucoseDisplayTime\").alias(\"Max\"),\n",
    "                min(\"GlucoseDisplayTime\").alias(\"Min\"))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a39a71ba-5841-4073-a424-f56a27f4ad80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 187:======>                                                (2 + 15) / 17]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-------------------+\n",
      "|NumId|                Max|                Min|\n",
      "+-----+-------------------+-------------------+\n",
      "| 4818|2022-02-03 17:34:00|2022-02-03 04:29:00|\n",
      "| 7754|2022-02-03 17:47:00|2022-02-03 04:52:00|\n",
      "| 2235|2022-02-03 09:03:00|2022-02-03 04:23:00|\n",
      "| 4219|2022-02-03 17:48:00|2022-02-03 04:43:00|\n",
      "| 2711|2022-02-03 18:03:00|2022-02-03 07:38:00|\n",
      "+-----+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp = testSet.filter(col('NumId').isin([4818, 7754, 2235, 4219, 2711]))\n",
    "temp = temp.groupby('NumId')\\\n",
    "            .agg(max(\"GlucoseDisplayTime\").alias(\"Max\"),\n",
    "                min(\"GlucoseDisplayTime\").alias(\"Min\"))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8274c0-fe09-430b-8ea9-40d037441999",
   "metadata": {},
   "source": [
    "## Save out into parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5adc7a54-1ab1-43b1-809f-10d78d6de56b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============>(2811 + 3) / 2814][Stage 8:>               (0 + 13) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1119.353s][warning][gc,alloc] Executor task launch worker for task 5.0 in stage 8.0 (TID 6001): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.353s][warning][gc,alloc] Executor task launch worker for task 2.0 in stage 8.0 (TID 5998): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.562s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 8.0 (TID 5997): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 8.0 in stage 8.0 (TID 6004): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 7.0 in stage 8.0 (TID 6003): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 11.0 in stage 8.0 (TID 6007): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 8.0 (TID 6000): Retried waiting for GCLocker too often allocating 131074 words\n",
      "[1119.572s][warning][gc,alloc] Executor task launch worker for task 9.0 in stage 8.0 (TID 6005): Retried waiting for GCLocker too often allocating 131074 words\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 11.0 in stage 8.0 (TID 6007)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 4.0 in stage 8.0 (TID 6000)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 7.0 in stage 8.0 (TID 6003)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 2.0 in stage 8.0 (TID 5998)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4142/0x0000000801fadf68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2895/0x0000000801dd69f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2571/0x0000000801d30dd8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 5.0 in stage 8.0 (TID 6001)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4142/0x0000000801fadf68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2895/0x0000000801dd69f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2571/0x0000000801d30dd8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 9.0 in stage 8.0 (TID 6005)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 1.0 in stage 8.0 (TID 5997)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR Executor: Exception in task 8.0 in stage 8.0 (TID 6004)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 7.0 in stage 8.0 (TID 6003),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 8.0 in stage 8.0 (TID 6004),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 9.0 in stage 8.0 (TID 6005),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 1.0 in stage 8.0 (TID 5997),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 11.0 in stage 8.0 (TID 6007),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 8.0 (TID 6000),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 8.0 (TID 5998),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4142/0x0000000801fadf68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2895/0x0000000801dd69f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2571/0x0000000801d30dd8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "23/05/16 23:18:14 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 5.0 in stage 8.0 (TID 6001),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:50)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:553)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:172)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage7.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextRow(WindowExec.scala:113)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.<init>(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec.$anonfun$doExecute$3(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$Lambda$4142/0x0000000801fadf68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$2895/0x0000000801dd69f8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2571/0x0000000801d30dd8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 8.0 in stage 8.0 (TID 6004) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "23/05/16 23:18:14 ERROR TaskSetManager: Task 8 in stage 8.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============>(2811 + 3) / 2814][Stage 8:>               (0 + 15) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/16 23:18:14 ERROR FileFormatWriter: Aborting job 414e7339-6f64-4779-88d6-28382fb4a3a3.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 8.0 failed 1 times, most recent failure: Lost task 8.0 in stage 8.0 (TID 6004) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 13.0 in stage 8.0 (TID 6009) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 14.0 in stage 8.0 (TID 6010) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 12.0 in stage 8.0 (TID 6008) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 10.0 in stage 8.0 (TID 6006) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 3.0 in stage 8.0 (TID 5999) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 5996) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/16 23:18:14 WARN TaskSetManager: Lost task 6.0 in stage 8.0 (TID 6002) (jupyter-ljoe-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o200.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 8.0 failed 1 times, most recent failure: Lost task 8.0 in stage 8.0 (TID 6004) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# df.repartition('PatientId')\\\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     .write.parquet('/cephfs/train_test_val/test/') \u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainSet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPatientId\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/cephfs/train_test_val/train_set/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m valSet\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatientId\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cephfs/train_test_val/val_set/\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m      8\u001b[0m testSet\u001b[38;5;241m.\u001b[39mrepartition(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatientId\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/cephfs/train_test_val/test_set/\u001b[39m\u001b[38;5;124m'\u001b[39m) \n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1140\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o200.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 8.0 failed 1 times, most recent failure: Lost task 8.0 in stage 8.0 (TID 6004) (jupyter-ljoe-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
     ]
    }
   ],
   "source": [
    "'''godspeed'''\n",
    "startTime = time.time()\n",
    "\n",
    "# df.repartition('PatientId')\\\n",
    "#     .write.parquet('/cephfs/train_test_val/test/') \n",
    "trainSet.repartition('PatientId').write.parquet('/cephfs/train_test_val/train_set/')\n",
    "valSet.repartition('PatientId').write.parquet('/cephfs/train_test_val/val_set/')\n",
    "testSet.repartition('PatientId').write.parquet('/cephfs/train_test_val/test_set/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ac3c7-a9b0-4340-a106-f6f8c3ab3ad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endTime = time.time()\n",
    "endTime-startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff34598e-bbbe-4a9a-9958-b42cf9d4727b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.854722222222223"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# endTime = time.time()\n",
    "# endTime-startTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9adadff8-b934-40b4-89fc-256905ece957",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.805555555555555"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "17300/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5abec12d-6820-40a1-9177-98ff0002a933",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.74638888888889"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "45887/60/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
