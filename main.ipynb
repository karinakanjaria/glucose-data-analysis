{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Current Process\n",
    "1. Read in data --> Done\n",
    "\n",
    "2. Custom Imputation --> Done\n",
    "\n",
    "3. Add Binary Class --> Done, Should Add Binary Class Later\n",
    "\n",
    "4. Summary Statistics Features --> Done\n",
    "\n",
    "5. Wrapper Functions --> Done, Need to Test Though\n",
    "\n",
    "6. Sklearn Pipeline Categorical Features --> One Hot Encoding Done\n",
    "\n",
    "7. Sklearn Pipeline Numerical Features --> StandardScaler Done\n",
    "\n",
    "8. Create Lagged Features --> Done\n",
    "\n",
    "9. Modeling --> Currently XgBoost, (Maybe Try: TensorFlow Decision Tree, TensorFlow Probability Model)\n",
    "\n",
    "10. Model Evaluation --> Accuracy, Precision, Recall, F1, Confusion Matrix (Need to add Variable Importance Based on Variance)\n",
    "\n",
    "11. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Need to Run These in Notebook Version For Pandas UDF\n",
    "! pip install pyarrow\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install pyspark\n",
    "! pip install xgboost\n",
    "! pip install kaleido\n",
    "! pip install EntropyHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Input_Variables.read_vars import train_data_storage, validation_data_storage, test_data_storage, \\\n",
    "                                      one_hot_encoding_data, \\\n",
    "                                      analysis_group, \\\n",
    "                                      daily_stats_features_lower, daily_stats_features_upper, \\\n",
    "                                      model_storage_location, random_seed, \\\n",
    "                                      time_series_lag_values_created, \\\n",
    "                                      evaluation_metrics_output_storage, \\\n",
    "                                      feature_importance_storage_location, \\\n",
    "                                      overall_feature_importance_plot_location\n",
    "\n",
    "from Data_Schema.schema import Pandas_UDF_Data_Schema\n",
    "from Read_In_Data.read_data import Reading_Data\n",
    "from Data_Pipeline.imputation_pipeline import Date_And_Value_Imputation\n",
    "\n",
    "\n",
    "from Feature_Generation.create_binary_labels import Create_Binary_Labels\n",
    "from Feature_Generation.summary_stats import Summary_Stats_Features\n",
    "from Feature_Generation.lag_features import Create_Lagged_Features\n",
    "from Feature_Generation.time_series_feature_creation import TS_Features\n",
    "from Feature_Generation.difference_features import Difference_Features\n",
    "\n",
    "from Data_Pipeline.encoding_scaling_pipeline import Feature_Transformations\n",
    "\n",
    "from Model_Creation.pyspark_xgboost import Create_PySpark_XGBoost\n",
    "\n",
    "from Model_Predictions.pyspark_model_preds import Model_Predictions\n",
    "\n",
    "from Model_Evaluation.pyspark_model_eval import Evaluate_Model\n",
    "\n",
    "from Feature_Importance.model_feature_importance import Feature_Importance\n",
    "\n",
    "from Model_Plots.xgboost_classification_plots import XGBoost_Classification_Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/08 19:20:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/08 19:20:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# PySpark UDF Schema Activation\n",
    "pandas_udf_data_schema=Pandas_UDF_Data_Schema()\n",
    "\n",
    "# Data Location\n",
    "reading_data=Reading_Data()\n",
    "\n",
    "# Create Binary y Variables\n",
    "create_binary_labels=Create_Binary_Labels()\n",
    "\n",
    "# Imputation\n",
    "date_and_value_imputation=Date_And_Value_Imputation(reading_data.spark)\n",
    "\n",
    "# Features Daily Stats Module\n",
    "summary_stats_features=Summary_Stats_Features()\n",
    "\n",
    "# Features Complex\n",
    "ts_features=TS_Features()\n",
    "\n",
    "# Features Lagged Value\n",
    "create_lag_features=Create_Lagged_Features()\n",
    "\n",
    "# Features Differences\n",
    "difference_features=Difference_Features()\n",
    "\n",
    "# PySpark XGBoost Model Module\n",
    "create_pyspark_xgboost=Create_PySpark_XGBoost()\n",
    "\n",
    "# Classification Evaluation\n",
    "evaluate_model=Evaluate_Model()\n",
    "\n",
    "# Model Plots Feature Importance\n",
    "xgboost_classification_plot=XGBoost_Classification_Plot()\n",
    "\n",
    "# Feature Transformations\n",
    "feature_transformations=Feature_Transformations()\n",
    "\n",
    "\n",
    "pyspark_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "\n",
    "\n",
    "model_predictions=Model_Predictions()\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance=Feature_Importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. PySpark: Reading In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:====================================================>    (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|NumId|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "| 2159|+/WgTKHs7vvb24NIL...|121.0|2022-02-04 14:50:00| 2022-02-04T14:50:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...|115.0|2022-02-04 14:55:00| 2022-02-04T14:55:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...|111.0|2022-02-04 15:00:00| 2022-02-04T15:00:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 98.0|2022-02-04 15:05:00| 2022-02-04T15:05:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 99.0|2022-02-04 15:10:00| 2022-02-04T15:10:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...|110.0|2022-02-04 15:15:00| 2022-02-04T15:15:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...|111.0|2022-02-04 15:20:00| 2022-02-04T15:20:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...|109.0|2022-02-04 15:25:00| 2022-02-04T15:25:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...|109.0|2022-02-04 15:30:00| 2022-02-04T15:30:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...|101.0|2022-02-04 15:35:00| 2022-02-04T15:35:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 95.0|2022-02-04 15:40:00| 2022-02-04T15:40:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 89.0|2022-02-04 15:45:00| 2022-02-04T15:45:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 83.0|2022-02-04 15:50:00| 2022-02-04T15:50:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 79.0|2022-02-04 15:55:00| 2022-02-04T15:55:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 73.0|2022-02-04 16:00:00| 2022-02-04T16:00:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 70.0|2022-02-04 16:05:00| 2022-02-04T16:05:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 69.0|2022-02-04 16:10:00| 2022-02-04T16:10:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 63.0|2022-02-04 16:15:00| 2022-02-04T16:15:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 56.0|2022-02-04 16:20:00| 2022-02-04T16:20:...|        2022-02-04|\n",
      "| 2159|+/WgTKHs7vvb24NIL...| 49.0|2022-02-04 16:25:00| 2022-02-04T16:25:...|        2022-02-04|\n",
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df=reading_data.read_in_pyspark_data(data_location=train_data_storage)\n",
    "training_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df=reading_data.read_in_pyspark_data(data_location=validation_data_storage)\n",
    "validation_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_df=reading_data.read_in_pyspark_data(data_location=test_data_storage)\n",
    "testing_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. PySpark: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+\n",
      "| GlucoseDisplayTime|           PatientId|Value|\n",
      "+-------------------+--------------------+-----+\n",
      "|2022-04-11 13:15:00|+BPY2YsPzI4b+DwiN...|102.0|\n",
      "|2022-04-11 13:20:00|+BPY2YsPzI4b+DwiN...| 93.0|\n",
      "|2022-04-11 13:25:00|+BPY2YsPzI4b+DwiN...| 89.0|\n",
      "|2022-04-11 13:30:00|+BPY2YsPzI4b+DwiN...| 88.0|\n",
      "|2022-04-11 13:35:00|+BPY2YsPzI4b+DwiN...| 94.0|\n",
      "|2022-04-11 13:40:00|+BPY2YsPzI4b+DwiN...| 99.0|\n",
      "|2022-04-11 13:45:00|+BPY2YsPzI4b+DwiN...| 98.0|\n",
      "|2022-04-11 13:50:00|+BPY2YsPzI4b+DwiN...| 98.0|\n",
      "|2022-04-11 13:55:00|+BPY2YsPzI4b+DwiN...| 95.0|\n",
      "|2022-04-11 14:00:00|+BPY2YsPzI4b+DwiN...| 87.0|\n",
      "|2022-04-11 14:05:00|+BPY2YsPzI4b+DwiN...| 84.0|\n",
      "|2022-04-11 14:10:00|+BPY2YsPzI4b+DwiN...| 84.0|\n",
      "|2022-04-11 14:15:00|+BPY2YsPzI4b+DwiN...| 91.0|\n",
      "|2022-04-11 14:20:00|+BPY2YsPzI4b+DwiN...| 99.0|\n",
      "|2022-04-11 14:25:00|+BPY2YsPzI4b+DwiN...|106.0|\n",
      "|2022-04-11 14:30:00|+BPY2YsPzI4b+DwiN...|103.0|\n",
      "|2022-04-11 14:35:00|+BPY2YsPzI4b+DwiN...|113.0|\n",
      "|2022-04-11 14:40:00|+BPY2YsPzI4b+DwiN...|130.0|\n",
      "|2022-04-11 14:45:00|+BPY2YsPzI4b+DwiN...|138.0|\n",
      "|2022-04-11 14:50:00|+BPY2YsPzI4b+DwiN...|141.0|\n",
      "+-------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "training_custom_imputation_pipeline=date_and_value_imputation.\\\n",
    "                                        pyspark_custom_imputation_pipeline(df=training_df, \n",
    "                                                                           output_schema=pyspark_custom_imputation_schema,\n",
    "                                                                           analysis_group=analysis_group)\n",
    "\n",
    "training_custom_imputation_pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "testing_custom_imputation_pipeline=date_and_value_imputation.\\\n",
    "                                        pyspark_custom_imputation_pipeline(df=testing_df, \n",
    "                                                                           output_schema=pyspark_custom_imputation_schema,\n",
    "                                                                           analysis_group=analysis_group)\n",
    "\n",
    "testing_custom_imputation_pipeline.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. PySpark: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                        (0 + 4) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/08 19:24:47 WARN ArrowPythonRunner: Detected deadlock while completing task 2.0 in stage 15 (TID 310): Attempting to kill Python Worker\n",
      "23/05/08 19:24:47 ERROR Executor: Exception in task 2.0 in stage 15.0 (TID 310)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat scala.reflect.ClassTag$GenericClassTag.newArray(ClassTag.scala:171)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:341)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\n",
      "\tat scala.collection.generic.TraversableForwarder.toArray(TraversableForwarder.scala:67)\n",
      "\tat scala.collection.generic.TraversableForwarder.toArray$(TraversableForwarder.scala:67)\n",
      "\tat scala.collection.mutable.ListBuffer.toArray(ListBuffer.scala:47)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries$.org$apache$spark$sql$catalyst$util$QuantileSummaries$$compressImmut(QuantileSummaries.scala:388)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.compress(QuantileSummaries.scala:140)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.insert(QuantileSummaries.scala:69)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$.apply$1(StatFunctions.scala:91)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$.$anonfun$multipleApproxQuantiles$6(StatFunctions.scala:103)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$$$Lambda$3923/0x0000000801cf74b0.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3925/0x0000000801cf7a68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3926/0x0000000801cf85b8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1637/0x00000008014ebc98.apply(Unknown Source)\n",
      "23/05/08 19:24:47 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 15.0 (TID 310),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat scala.reflect.ClassTag$GenericClassTag.newArray(ClassTag.scala:171)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:341)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\n",
      "\tat scala.collection.generic.TraversableForwarder.toArray(TraversableForwarder.scala:67)\n",
      "\tat scala.collection.generic.TraversableForwarder.toArray$(TraversableForwarder.scala:67)\n",
      "\tat scala.collection.mutable.ListBuffer.toArray(ListBuffer.scala:47)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries$.org$apache$spark$sql$catalyst$util$QuantileSummaries$$compressImmut(QuantileSummaries.scala:388)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.compress(QuantileSummaries.scala:140)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.insert(QuantileSummaries.scala:69)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$.apply$1(StatFunctions.scala:91)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$.$anonfun$multipleApproxQuantiles$6(StatFunctions.scala:103)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$$$Lambda$3923/0x0000000801cf74b0.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3925/0x0000000801cf7a68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3926/0x0000000801cf85b8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1637/0x00000008014ebc98.apply(Unknown Source)\n",
      "23/05/08 19:24:47 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6b2f56bc rejected from java.util.concurrent.ThreadPoolExecutor@2dc50dc9[Shutting down, pool size = 4, active threads = 4, queued tasks = 0, completed tasks = 308]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2065)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:305)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/08 19:24:47 WARN TaskSetManager: Lost task 2.0 in stage 15.0 (TID 310) (jupyter-cmonsivais-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n",
      "\tat scala.reflect.ClassTag$GenericClassTag.newArray(ClassTag.scala:171)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:341)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\n",
      "\tat scala.collection.generic.TraversableForwarder.toArray(TraversableForwarder.scala:67)\n",
      "\tat scala.collection.generic.TraversableForwarder.toArray$(TraversableForwarder.scala:67)\n",
      "\tat scala.collection.mutable.ListBuffer.toArray(ListBuffer.scala:47)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries$.org$apache$spark$sql$catalyst$util$QuantileSummaries$$compressImmut(QuantileSummaries.scala:388)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.compress(QuantileSummaries.scala:140)\n",
      "\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.insert(QuantileSummaries.scala:69)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$.apply$1(StatFunctions.scala:91)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$.$anonfun$multipleApproxQuantiles$6(StatFunctions.scala:103)\n",
      "\tat org.apache.spark.sql.execution.stat.StatFunctions$$$Lambda$3923/0x0000000801cf74b0.apply(Unknown Source)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
      "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
      "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
      "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n",
      "\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n",
      "\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3925/0x0000000801cf7a68.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3926/0x0000000801cf85b8.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$1637/0x00000008014ebc98.apply(Unknown Source)\n",
      "\n",
      "23/05/08 19:24:47 ERROR TaskSetManager: Task 2 in stage 15.0 failed 1 times; aborting job\n",
      "23/05/08 19:24:48 ERROR TaskContextImpl: Error in TaskCompletionListener\n",
      "org.apache.spark.SparkException: Block broadcast_12 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:358)\n",
      "\tat org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:1291)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1(TorrentBroadcast.scala:289)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1$adapted(TorrentBroadcast.scala:289)\n",
      "\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:131)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:143)\n",
      "\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:143)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:198)\n",
      "\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:143)\n",
      "\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:136)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:146)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/08 19:24:48 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 3.0 in stage 15.0 (TID 311)\n",
      "java.lang.NullPointerException: Cannot invoke \"org.apache.spark.SparkEnv.blockManager()\" because the return value of \"org.apache.spark.SparkEnv$.get()\" is null\n",
      "\tat org.apache.spark.scheduler.Task.$anonfun$run$3(Task.scala:157)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:155)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o76.approxQuantile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 15.0 failed 1 times, most recent failure: Lost task 2.0 in stage 15.0 (TID 310) (jupyter-cmonsivais-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n\tat scala.reflect.ClassTag$GenericClassTag.newArray(ClassTag.scala:171)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:341)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\n\tat scala.collection.generic.TraversableForwarder.toArray(TraversableForwarder.scala:67)\n\tat scala.collection.generic.TraversableForwarder.toArray$(TraversableForwarder.scala:67)\n\tat scala.collection.mutable.ListBuffer.toArray(ListBuffer.scala:47)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries$.org$apache$spark$sql$catalyst$util$QuantileSummaries$$compressImmut(QuantileSummaries.scala:388)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.compress(QuantileSummaries.scala:140)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.insert(QuantileSummaries.scala:69)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.apply$1(StatFunctions.scala:91)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.$anonfun$multipleApproxQuantiles$6(StatFunctions.scala:103)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$$Lambda$3923/0x0000000801cf74b0.apply(Unknown Source)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD$$Lambda$3925/0x0000000801cf7a68.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD$$Lambda$3926/0x0000000801cf85b8.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD$$Lambda$1637/0x00000008014ebc98.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:103)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:104)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n\tat scala.reflect.ClassTag$GenericClassTag.newArray(ClassTag.scala:171)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:341)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\n\tat scala.collection.generic.TraversableForwarder.toArray(TraversableForwarder.scala:67)\n\tat scala.collection.generic.TraversableForwarder.toArray$(TraversableForwarder.scala:67)\n\tat scala.collection.mutable.ListBuffer.toArray(ListBuffer.scala:47)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries$.org$apache$spark$sql$catalyst$util$QuantileSummaries$$compressImmut(QuantileSummaries.scala:388)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.compress(QuantileSummaries.scala:140)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.insert(QuantileSummaries.scala:69)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.apply$1(StatFunctions.scala:91)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.$anonfun$multipleApproxQuantiles$6(StatFunctions.scala:103)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$$Lambda$3923/0x0000000801cf74b0.apply(Unknown Source)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD$$Lambda$3925/0x0000000801cf7a68.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD$$Lambda$3926/0x0000000801cf85b8.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD$$Lambda$1637/0x00000008014ebc98.apply(Unknown Source)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_df_added_binary_labels\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_binary_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyspark_binary_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_custom_imputation_pipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m training_df_added_binary_labels\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/glucose-data-analysis/Feature_Generation/create_binary_labels.py:12\u001b[0m, in \u001b[0;36mCreate_Binary_Labels.pyspark_binary_labels\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpyspark_binary_labels\u001b[39m(\u001b[38;5;28mself\u001b[39m, df):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#get 10th and 90th percentiles of patient\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     lower_10, upper_90  \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproxQuantile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mValue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#if 10th percentile of patient is lower than default, use percentile\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     lower \u001b[38;5;241m=\u001b[39m lower_10 \u001b[38;5;28;01mif\u001b[39;00m lower_10 \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlower\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2851\u001b[0m, in \u001b[0;36mDataFrame.approxQuantile\u001b[0;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[1;32m   2848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelativeError should be >= 0.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2849\u001b[0m relativeError \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(relativeError)\n\u001b[0;32m-> 2851\u001b[0m jaq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproxQuantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelativeError\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2852\u001b[0m jaq_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(j) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m jaq]\n\u001b[1;32m   2853\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jaq_list[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m isStr \u001b[38;5;28;01melse\u001b[39;00m jaq_list\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o76.approxQuantile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 15.0 failed 1 times, most recent failure: Lost task 2.0 in stage 15.0 (TID 310) (jupyter-cmonsivais-40ucsd-2eedu executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n\tat scala.reflect.ClassTag$GenericClassTag.newArray(ClassTag.scala:171)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:341)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\n\tat scala.collection.generic.TraversableForwarder.toArray(TraversableForwarder.scala:67)\n\tat scala.collection.generic.TraversableForwarder.toArray$(TraversableForwarder.scala:67)\n\tat scala.collection.mutable.ListBuffer.toArray(ListBuffer.scala:47)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries$.org$apache$spark$sql$catalyst$util$QuantileSummaries$$compressImmut(QuantileSummaries.scala:388)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.compress(QuantileSummaries.scala:140)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.insert(QuantileSummaries.scala:69)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.apply$1(StatFunctions.scala:91)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.$anonfun$multipleApproxQuantiles$6(StatFunctions.scala:103)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$$Lambda$3923/0x0000000801cf74b0.apply(Unknown Source)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD$$Lambda$3925/0x0000000801cf7a68.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD$$Lambda$3926/0x0000000801cf85b8.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD$$Lambda$1637/0x00000008014ebc98.apply(Unknown Source)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2323)\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1174)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1267)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1228)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1214)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1214)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:103)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:104)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:115)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.lang.reflect.Array.newInstance(Array.java:78)\n\tat scala.reflect.ClassTag$GenericClassTag.newArray(ClassTag.scala:171)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:341)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat scala.collection.AbstractTraversable.toArray(Traversable.scala:108)\n\tat scala.collection.generic.TraversableForwarder.toArray(TraversableForwarder.scala:67)\n\tat scala.collection.generic.TraversableForwarder.toArray$(TraversableForwarder.scala:67)\n\tat scala.collection.mutable.ListBuffer.toArray(ListBuffer.scala:47)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries$.org$apache$spark$sql$catalyst$util$QuantileSummaries$$compressImmut(QuantileSummaries.scala:388)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.compress(QuantileSummaries.scala:140)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.insert(QuantileSummaries.scala:69)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.apply$1(StatFunctions.scala:91)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.$anonfun$multipleApproxQuantiles$6(StatFunctions.scala:103)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$$Lambda$3923/0x0000000801cf74b0.apply(Unknown Source)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.aggregate(TraversableOnce.scala:260)\n\tat scala.collection.TraversableOnce.aggregate$(TraversableOnce.scala:260)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1431)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$4(RDD.scala:1236)\n\tat org.apache.spark.rdd.RDD$$Lambda$3925/0x0000000801cf7a68.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$6(RDD.scala:1237)\n\tat org.apache.spark.rdd.RDD$$Lambda$3926/0x0000000801cf85b8.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n\tat org.apache.spark.rdd.RDD$$Lambda$1637/0x00000008014ebc98.apply(Unknown Source)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 44230)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "training_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=training_custom_imputation_pipeline)\n",
    "\n",
    "training_df_added_binary_labels.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=testing_custom_imputation_pipeline)\n",
    "\n",
    "testing_df_added_binary_labels.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PySpark: Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_df_differences = difference_features.add_difference_features(training_df_added_binary_labels)\n",
    "training_df_differences.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_df_chunks = summary_stats_features.create_chunk_col(training_df_differences, chunk_val = 288)\n",
    "training_df_chunks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_df_poincare = training_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.poincare)\n",
    "# training_df_poincare.show(5)\n",
    "\n",
    "# training_df_entropy = training_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.entropy)\n",
    "# training_df_entropy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_df_complex_features = training_df_poincare.join(training_df_entropy,['PatientId', 'Chunk'])\n",
    "# training_df_complex_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_df_sleep = ts_features.process_for_sleep(df=training_df_added_binary_labels)\n",
    "# training_df_sleep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=training_df_chunks)\n",
    "# merge complex features and summary stats and demographics and sleep features\n",
    "# merge in one hot encoded cohort file info demographics\n",
    "    # '/cephfs/data/cohort_encoded.parquet' (gender, treatment, age category)\n",
    "    # groupby patientId and chunk\n",
    "\n",
    "training_features_summary_stats.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #add target variable\n",
    "# training_features_final_summary = summary_stats_features\\\n",
    "#                                     .add_lag_out_of_range(df=training_features_summary_stats, chunk_lag=1)\n",
    "\n",
    "# training_features_final_summary.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge these together\n",
    "# training_features_summary_stats\n",
    "# training_df_complex_features\n",
    "# one-hot-encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_df_differences = difference_features.add_difference_features(testing_df_added_binary_labels)\n",
    "testing_df_differences.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_df_chunks = summary_stats_features.create_chunk_col(testing_df_differences, chunk_val = 288)\n",
    "testing_df_chunks.show(5)\n",
    "\n",
    "# testing_df_poincare = testing_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.poincare)\n",
    "# testing_df_poincare.show(5)\n",
    "\n",
    "# testing_df_entropy = testing_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.entropy)\n",
    "# testing_df_entropy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing_df_complex_features = testing_df_poincare.join(testing_df_entropy,['PatientId', 'Chunk'])\n",
    "# testing_df_complex_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_df_sleep = ts_features.process_for_sleep(df=testing_df_added_binary_labels)\n",
    "# training_df_sleep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=testing_df_chunks)\n",
    "\n",
    "# merge complex features and summary stats and demographics and sleep features\n",
    "# merge in one hot encoded cohort file info demographics\n",
    "    # '/cephfs/data/cohort_encoded.parquet' (gender, treatment, age category)\n",
    "    # groupby patientId and chunk\n",
    "\n",
    "testing_features_summary_stats.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge these together\n",
    "# testing_features_summary_stats\n",
    "# training_df_complex_features\n",
    "# one-hot-encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PySpark: Sklearn Regression Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_df=reading_data.read_in_one_hot_encoded_data(one_hot_encoding_location=one_hot_encoding_data)\n",
    "one_hot_encoded_df=one_hot_encoded_df.select('UserId', \n",
    "                                             'Sex_Encoded', \n",
    "                                             'Treatment_Encoded', \n",
    "                                             'AgeGroup_Encoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_encoded=training_features_summary_stats.join(one_hot_encoded_df,\n",
    "                                                       training_features_summary_stats.PatientId==one_hot_encoded_df.UserId,\n",
    "                                                       \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_encoded.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge training_features_summary with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_encoded=testing_features_summary_stats.join(one_hot_encoded_df,\n",
    "                                                       testing_features_summary_stats.PatientId==one_hot_encoded_df.UserId,\n",
    "                                                       \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_encoded.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. PySpark: Sklearn Numerical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.sql.types import DoubleType, FloatType, LongType\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_encoded.select('PatientId').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_1=training_encoded.filter(training_encoded.PatientId=='8W/rpnb48OMm47W2x4FSkc7+9u2mol061DQuJoMdiK0=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "double_cols=[f.name for f in patient_1.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "float_cols=[f.name for f in patient_1.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "long_cols=[f.name for f in patient_1.schema.fields if isinstance(f.dataType, LongType)]\n",
    "\n",
    "# all_numerical=list(set(double_cols+float_cols))\n",
    "# all_numerical_lags=[x for x in all_numerical if \"lag\" in x]\n",
    "all_numerical=list(set(double_cols+float_cols+long_cols))\n",
    "all_numerical.remove('target')\n",
    "\n",
    "# featureArr = [('scaled_' + f) for f in all_numerical_lags]\n",
    "featureArr = [('scaled_' + f) for f in all_numerical]+['Sex_Encoded', 'Treatment_Encoded', 'AgeGroup_Encoded']\n",
    "# featureArr = [('scaled_' + f) for f in all_numerical]\n",
    "\n",
    "va1 = [VectorAssembler(inputCols=[f], outputCol=('vec_' + f)) for f in all_numerical]\n",
    "ss = [StandardScaler(inputCol='vec_' + f, outputCol='scaled_' + f, withMean=True, withStd=True) for f in all_numerical]\n",
    "\n",
    "va2 = VectorAssembler(inputCols=featureArr, outputCol=\"features\")\n",
    "\n",
    "stages = va1 + ss + [va2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "model=pipeline.fit(training_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.transform(training_encoded).select('features').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.transform(training_encoded).select('features').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "model=pipeline.fit(patient_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.transform(patient_1).select('features').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_numerical_stages=feature_transformations.numerical_scaling(df=training_encoded)\n",
    "training_numerical_stages=feature_transformations.numerical_scaling(df=training_features_summary_stats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. PySpark: XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgboost_model=create_pyspark_xgboost.xgboost_classifier(ml_df=training_encoded,\n",
    "#                                                         stages=training_numerical_stages,\n",
    "#                                                         model_storage_location=model_storage_location,\n",
    "#                                                         random_seed=random_seed)\n",
    "\n",
    "xgboost_model=create_pyspark_xgboost.xgboost_classifier(ml_df=training_features_summary_stats,\n",
    "                                                        stages=training_numerical_stages,\n",
    "                                                        model_storage_location=model_storage_location,\n",
    "                                                        random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. PySpark: Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. PySpark: Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing_predictions=model_predictions.create_predictions_with_model(test_df=testing_encoded, \n",
    "#                                                                     model=xgboost_model)\n",
    "# testing_predictions.show(10)\n",
    "\n",
    "testing_predictions=model_predictions.create_predictions_with_model(test_df=training_features_summary_stats, \n",
    "                                                                    model=xgboost_model)\n",
    "testing_predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. PySpark: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluation=evaluate_model.regression_evaluation(testing_predictions=testing_predictions, \n",
    "                                                          eval_csv_location=evaluation_metrics_output_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance_df=feature_importance.\\\n",
    "                        feature_importance_accuracy_gain(xgboost_model=xgboost_model, \n",
    "                                                         feature_importance_storage_location=feature_importance_storage_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. PySpark: Feature Importance Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_feature_plot=xgboost_classification_plot.feature_overall_importance_plot(feature_importance_df=feature_importance_df,\n",
    "                                                                                 overall_importance_plot_location=overall_feature_importance_plot_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_feature_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.PySpark: Local Level Feature Importance --> Shap Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add to reqs if this works\n",
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_model.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgboost_model.stages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e282f9bbb605e17ec33c138684786f2c6a0e45a19b318784bb5671e7a7934052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
