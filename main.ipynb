{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Process\n",
    "1. Read in data --> Done\n",
    "\n",
    "2. Custom Imputation --> Done\n",
    "\n",
    "3. Aggregate Data --> Not Done Yet\n",
    "\n",
    "4. Add Binary Class --> Done, Should Add Binary Class Later\n",
    "\n",
    "5. Summary Statistics Features --> Done\n",
    "\n",
    "6. Wrapper Functions --> Done, Need to Test Though\n",
    "\n",
    "7. Sklearn Pipeline Categorical Features --> One Hot Encoding Done\n",
    "\n",
    "8. Sklearn Pipeline Numerical Features --> StandardScaler Done\n",
    "\n",
    "9. Create Lagged Features --> Done\n",
    "\n",
    "10. Modeling --> Currently XgBoost, (Maybe Try: TensorFlow Decision Tree, TensorFlow Probability Model)\n",
    "\n",
    "11. Model Evaluation --> Accuracy, Precision, Recall, F1, Confusion Matrix (Need to add Variable Importance Based on Variance)\n",
    "\n",
    "12. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Input_Variables.read_vars import raw_data_storage, \\\n",
    "                                      analysis_group, \\\n",
    "                                      daily_stats_features_lower, daily_stats_features_upper, \\\n",
    "                                      ml_models_train_split, ml_models_test_split, model_storage_location, \\\n",
    "                                      time_series_lag_values_created\n",
    "\n",
    "from Data_Schema.schema import Pandas_UDF_Data_Schema\n",
    "from Read_In_Data.read_data import Reading_Data\n",
    "from Data_Pipeline.sklearn_pipeline import Sklearn_Pipeline\n",
    "from Feature_Generation.create_binary_labels import Create_Binary_Labels\n",
    "from Feature_Generation.summary_stats import Summary_Stats_Features\n",
    "from Feature_Generation.lag_features import Create_Lagged_Features\n",
    "from Model_Creation.xgboost_model import XGBoost_Classification\n",
    "from Model_Evaluation.classification_evaluation import Classification_Evalaution_Metrics\n",
    "from Model_Plots.xgboost_classification_plots import XGBoost_Classification_Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PySpark UDF Schema Activation\n",
    "pandas_udf_data_schema=Pandas_UDF_Data_Schema()\n",
    "\n",
    "# Data Location\n",
    "reading_data=Reading_Data(data_location=raw_data_storage)\n",
    "\n",
    "# Create Binary y Variables\n",
    "create_binary_labels=Create_Binary_Labels()\n",
    "\n",
    "# Sklearn Pipeline\n",
    "pandas_sklearn_pipeline=Sklearn_Pipeline()\n",
    "\n",
    "# Features Daily Stats Module\n",
    "summary_stats_features=Summary_Stats_Features()\n",
    "\n",
    "# Features Lagged Value\n",
    "create_lag_features=Create_Lagged_Features()\n",
    "\n",
    "# XGBoost Model Module\n",
    "xgboost_classification=XGBoost_Classification()\n",
    "\n",
    "# Classification Evaluation\n",
    "classification_evalaution_metrics=Classification_Evalaution_Metrics()\n",
    "\n",
    "# Model Plots Feature Importance\n",
    "xgboost_classification_plot=XGBoost_Classification_Plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PySpark: Reading In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 18:24:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####### PySpark\n",
    "pyspark_df=reading_data.read_in_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|b+SG81ztOyK5NQEu3...|  0.0|2022-09-11 11:34:07| 2022-09-11T11:34:...|        2022-09-11|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pyspark_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PySpark: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/18 18:25:31 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Cannot reserve 536870912 bytes of direct buffer memory (allocated: 798665860, limit: 1037959168)\n",
      "\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n",
      "\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:121)\n",
      "\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:332)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n",
      "\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n",
      "\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n",
      "\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:92)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:103)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Cannot reserve 536870912 bytes of direct buffer memory (allocated: 798665860, limit: 1037959168)\n",
      "\tat java.base/java.nio.Bits.reserveMemory(Bits.java:178)\n",
      "\tat java.base/java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:121)\n",
      "\tat java.base/java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:332)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.allocateDirect(UnpooledDirectByteBuf.java:104)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.allocateDirect(UnpooledByteBufAllocator.java:215)\n",
      "\tat io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:64)\n",
      "\tat io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:41)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator$InstrumentedUnpooledUnsafeDirectByteBuf.<init>(UnpooledByteBufAllocator.java:210)\n",
      "\tat io.netty.buffer.UnpooledByteBufAllocator.newDirectBuffer(UnpooledByteBufAllocator.java:91)\n",
      "\tat io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:188)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.newDirectBufferL(PooledByteBufAllocatorL.java:171)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL$InnerAllocator.directBuffer(PooledByteBufAllocatorL.java:214)\n",
      "\tat io.netty.buffer.PooledByteBufAllocatorL.allocate(PooledByteBufAllocatorL.java:58)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:77)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager.<init>(NettyAllocationManager.java:84)\n",
      "\tat org.apache.arrow.memory.NettyAllocationManager$1.create(NettyAllocationManager.java:34)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:315)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.newAllocationManager(BaseAllocator.java:310)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.bufferWithoutReservation(BaseAllocator.java:298)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:276)\n",
      "\tat org.apache.arrow.memory.BaseAllocator.buffer(BaseAllocator.java:240)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.reallocDataBuffer(BaseVariableWidthVector.java:522)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.handleSafe(BaseVariableWidthVector.java:1255)\n",
      "\tat org.apache.arrow.vector.BaseVariableWidthVector.setSafe(BaseVariableWidthVector.java:1091)\n",
      "\tat org.apache.spark.sql.execution.arrow.StringWriter.setValue(ArrowWriter.scala:251)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowFieldWriter.write(ArrowWriter.scala:130)\n",
      "\tat org.apache.spark.sql.execution.arrow.ArrowWriter.write(ArrowWriter.scala:95)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.$anonfun$writeIteratorToStream$1(ArrowPythonRunner.scala:92)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.writeIteratorToStream(ArrowPythonRunner.scala:103)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:438)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:272)\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m pyspark_custom_imputation_schema\u001b[38;5;241m=\u001b[39mpandas_udf_data_schema\u001b[38;5;241m.\u001b[39mcustom_imputation_pyspark_schema()\n\u001b[1;32m      3\u001b[0m pyspark_custom_imputation_pipeline\u001b[38;5;241m=\u001b[39mpandas_sklearn_pipeline\u001b[38;5;241m.\u001b[39mpyspark_custom_imputation_pipeline(df\u001b[38;5;241m=\u001b[39mpyspark_df, \n\u001b[1;32m      4\u001b[0m                                                                                               output_schema\u001b[38;5;241m=\u001b[39mpyspark_custom_imputation_schema,\n\u001b[1;32m      5\u001b[0m                                                                                               analysis_group\u001b[38;5;241m=\u001b[39manalysis_group)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mpyspark_custom_imputation_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####### PySpark\n",
    "pyspark_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "pyspark_custom_imputation_pipeline=pandas_sklearn_pipeline.pyspark_custom_imputation_pipeline(df=pyspark_df, \n",
    "                                                                                              output_schema=pyspark_custom_imputation_schema,\n",
    "                                                                                              analysis_group=analysis_group)\n",
    "pyspark_custom_imputation_pipeline.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PySpark Aggregate Data at Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PySpark: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=pyspark_custom_imputation_pipeline, \n",
    "                                                                          lower=daily_stats_features_lower, \n",
    "                                                                          upper=daily_stats_features_upper)\n",
    "\n",
    "pyspark_df_added_binary_labels.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PySpark: Features: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_features_summary_stats_schema=pandas_udf_data_schema.summary_stats_schema()\n",
    "pyspark_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=pyspark_df_added_binary_labels, \n",
    "                                                                                 output_schema=pyspark_features_summary_stats_schema, \n",
    "                                                                                 lower=daily_stats_features_lower, \n",
    "                                                                                 upper=daily_stats_features_upper,\n",
    "                                                                                 analysis_group=analysis_group)\n",
    "\n",
    "pyspark_features_summary_stats.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. PySpark: Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PySpark: Sklearn Categorical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_categorical_schema=pandas_udf_data_schema.sklearn_pyspark_categorical_schema()\n",
    "pyspark_transformations_categorical=pandas_sklearn_pipeline.pyspark_sklearn_pipeline_categorical(df=pyspark_features_summary_stats, \n",
    "                                                                                                 output_schema=pyspark_categorical_schema,\n",
    "                                                                                                 analysis_group=analysis_group)\n",
    "pyspark_transformations_categorical.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. PySpark: Sklearn Numerical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_numerical_schema=pandas_udf_data_schema.sklearn_pyspark_numerical_schema()\n",
    "pyspark_transformations_numerical=pandas_sklearn_pipeline.pyspark_sklearn_pipeline_numerical(df=pyspark_transformations_categorical, \n",
    "                                                                                             output_schema=pyspark_numerical_schema)\n",
    "pyspark_transformations_numerical.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. PySpark: Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_lag_features_creation=create_lag_features.pyspark_lag_features(df=pyspark_transformations_numerical,\n",
    "                                                                       time_series_lag_values_created=time_series_lag_values_created)\n",
    "pyspark_lag_features_creation.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. PySpark: XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_xgboost_classsification_schema=pandas_udf_data_schema.xgboost_classification_schema()\n",
    "\n",
    "classification_model_outputs=xgboost_classification.pyspark_xgboost(df=pyspark_lag_features_creation, \n",
    "                                                                    output_schema=pyspark_xgboost_classsification_schema, \n",
    "                                                                    train_split=ml_models_train_split, \n",
    "                                                                    test_split=ml_models_test_split)\n",
    "\n",
    "classification_model_outputs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. PySpark: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_classification_metric_schema=pandas_udf_data_schema.classification_metric_schema()\n",
    "\n",
    "classification_metric_df=classification_evalaution_metrics.pyspark_classification_model_evaluation_metrics(df=classification_model_outputs, \n",
    "                                                                                                           output_schema=pyspark_classification_metric_schema)\n",
    "classification_metric_df.show()                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_classification_plot.read_model_plot_variance(model_storage_location=model_storage_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_df=reading_data.read_in_pandas()\n",
    "pandas_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pat=pandas_df[pandas_df['PatientId']=='tHu8WPnIffml5CL+AbOBkXcbFApQnP06KdrHbjinta4=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_custom_imputation_pipeline=pandas_sklearn_pipeline.pandas_custom_imputation_pipeline(df=pandas_df)\n",
    "pandas_custom_imputation_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_imputation_pipeline=pandas_sklearn_pipeline.pandas_custom_imputation_pipeline(df=pandas_df)\n",
    "pandas_custom_imputation_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Aggregate Data at Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_added_binary_labels=create_binary_labels.pandas_binary_labels(df=pandas_custom_imputation_pipeline, \n",
    "                                                                        lower=daily_stats_features_lower, \n",
    "                                                                        upper=daily_stats_features_upper)\n",
    "pandas_df_added_binary_labels.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Features: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_features_summary_stats=summary_stats_features.pandas_compressDailyValues(data=pandas_df_added_binary_labels, \n",
    "                                                                                lower=daily_stats_features_lower, \n",
    "                                                                                upper=daily_stats_features_upper)\n",
    "pandas_features_summary_stats.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Sklearn Categorical Pipeline in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df=pandas_features_summary_stats[['PatientId', 'Value', 'GlucoseDisplayTime', 'GlucoseDisplayDate', 'inserted', \n",
    "        'missing', 'y_Binary', 'Median', 'Mean', 'Std Dev', 'Max', 'Min', 'AreaBelow', 'AreaAbove']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in df['PatientId'].unique():\n",
    "    # Categorical Features\n",
    "    categorical_features=['inserted', 'missing']\n",
    "    categorical_transformer=Pipeline([('imputer_cat', SimpleImputer(strategy='constant', fill_value=np.nan)),\n",
    "                                        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor_2=ColumnTransformer([('categorical', categorical_transformer, categorical_features)],\n",
    "                                    remainder = 'passthrough')\n",
    "\n",
    "    cat_pipe_pipeline=Pipeline([('preprocessing_2', preprocessor_2)])\n",
    "\n",
    "    transformed_data1=cat_pipe_pipeline.fit_transform(df)\n",
    "\n",
    "    transformed_data_df=pd.DataFrame(transformed_data1)\n",
    "\n",
    "    transformed_data_df['combine_inserted']=transformed_data_df[[0,1]].values.tolist()\n",
    "    transformed_data_df['combine_missing']=transformed_data_df[[2,3]].values.tolist()\n",
    "    transformed_data_df=transformed_data_df.drop(transformed_data_df.iloc[:, 0:4],axis = 1)\n",
    "\n",
    "    transformed_data_df.columns=['PatientId', 'Value', 'GlucoseDisplayTime', 'GlucoseDisplayDate', \n",
    "                                    'y_Binary', 'Median', 'Mean', 'Std Dev', 'Max', 'Min', 'AreaBelow', \n",
    "                                    'AreaAbove', 'inserted', 'missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_categorical_pipeline=pandas_sklearn_pipeline.pandas_transform_categorical_features(df=pandas_features_summary_stats)\n",
    "pandas_custom_categorical_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Sklearn Numerical Pipeline in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_numerical_pipeline=pandas_sklearn_pipeline.pandas_transform_numerical_features(df=pandas_custom_categorical_pipeline)\n",
    "pandas_custom_numerical_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e282f9bbb605e17ec33c138684786f2c6a0e45a19b318784bb5671e7a7934052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
