{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Process\n",
    "1. Read in data --> Done\n",
    "\n",
    "2. Custom Imputation --> Done\n",
    "\n",
    "3. Add Binary Class --> Done, Should Add Binary Class Later\n",
    "\n",
    "4. Summary Statistics Features --> Done\n",
    "\n",
    "5. Wrapper Functions --> Done, Need to Test Though\n",
    "\n",
    "6. Sklearn Pipeline Categorical Features --> One Hot Encoding Done\n",
    "\n",
    "7. Sklearn Pipeline Numerical Features --> StandardScaler Done\n",
    "\n",
    "8. Create Lagged Features --> Done\n",
    "\n",
    "9. Modeling --> Currently XgBoost, (Maybe Try: TensorFlow Decision Tree, TensorFlow Probability Model)\n",
    "\n",
    "10. Model Evaluation --> Accuracy, Precision, Recall, F1, Confusion Matrix (Need to add Variable Importance Based on Variance)\n",
    "\n",
    "11. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Input_Variables.read_vars import raw_data_storage, \\\n",
    "                                      analysis_group, \\\n",
    "                                      daily_stats_features_lower, daily_stats_features_upper, \\\n",
    "                                      ml_models_train_split, ml_models_test_split, model_storage_location, \\\n",
    "                                      time_series_lag_values_created\n",
    "\n",
    "from Data_Schema.schema import Pandas_UDF_Data_Schema\n",
    "from Read_In_Data.read_data import Reading_Data\n",
    "from Data_Pipeline.sklearn_pipeline import Sklearn_Pipeline\n",
    "from Feature_Generation.create_binary_labels import Create_Binary_Labels\n",
    "from Feature_Generation.summary_stats import Summary_Stats_Features\n",
    "from Feature_Generation.lag_features import Create_Lagged_Features\n",
    "from Model_Creation.xgboost_model import XGBoost_Classification\n",
    "from Model_Evaluation.classification_evaluation import Classification_Evalaution_Metrics\n",
    "from Model_Plots.xgboost_classification_plots import XGBoost_Classification_Plot\n",
    "\n",
    "from Data_Pipeline.encoding_scaling_pipeline import Feature_Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.24.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "# Need to Run These in Notebook Version For Pandas UDF\n",
    "! pip install pyarrow\n",
    "! pip install pandas\n",
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PySpark UDF Schema Activation\n",
    "pandas_udf_data_schema=Pandas_UDF_Data_Schema()\n",
    "\n",
    "# Data Location\n",
    "reading_data=Reading_Data(data_location=raw_data_storage)\n",
    "\n",
    "# Create Binary y Variables\n",
    "create_binary_labels=Create_Binary_Labels()\n",
    "\n",
    "# Sklearn Pipeline\n",
    "pandas_sklearn_pipeline=Sklearn_Pipeline()\n",
    "\n",
    "# Features Daily Stats Module\n",
    "summary_stats_features=Summary_Stats_Features()\n",
    "\n",
    "# Features Lagged Value\n",
    "create_lag_features=Create_Lagged_Features()\n",
    "\n",
    "# XGBoost Model Module\n",
    "xgboost_classification=XGBoost_Classification()\n",
    "\n",
    "# Classification Evaluation\n",
    "classification_evalaution_metrics=Classification_Evalaution_Metrics()\n",
    "\n",
    "# Model Plots Feature Importance\n",
    "xgboost_classification_plot=XGBoost_Classification_Plot()\n",
    "\n",
    "# Feature Transformations\n",
    "feature_transformations=Feature_Transformations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PySpark: Reading In Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 05:09:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/20 05:09:53 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####### PySpark\n",
    "pyspark_df=reading_data.read_in_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5734, 5)\n"
     ]
    }
   ],
   "source": [
    "print((pyspark_df.count(), len(pyspark_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_trunc, col\n",
    "pyspark_df=pyspark_df.withColumn(\"GlucoseDisplayTime\", date_trunc(\"minute\", col(\"GlucoseDisplayTime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyspark_df=pyspark_df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5734, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print((pyspark_df.count(), len(pyspark_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|vH4j/sVPDk4luo9wf...|157.0|2022-12-28 02:52:00| 2022-12-28T02:52:...|        2022-12-28|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pyspark_df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PySpark: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyspark_df=pyspark_df.orderBy(\"PatientId\", \n",
    "                              \"GlucoseDisplayTime\",\n",
    "                              ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "####### PySpark\n",
    "pyspark_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "pyspark_custom_imputation_pipeline=pandas_sklearn_pipeline.pyspark_custom_imputation_pipeline(df=pyspark_df, \n",
    "                                                                                              output_schema=pyspark_custom_imputation_schema,\n",
    "                                                                                              analysis_group=analysis_group)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+\n",
      "| GlucoseDisplayTime|           PatientId|Value|\n",
      "+-------------------+--------------------+-----+\n",
      "|2022-02-18 17:05:00|Zw997clFRcTAHrWiO...|260.0|\n",
      "+-------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pyspark_custom_imputation_pipeline.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PySpark: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------+\n",
      "| GlucoseDisplayTime|           PatientId|Value|y_Binary|\n",
      "+-------------------+--------------------+-----+--------+\n",
      "|2022-02-18 17:05:00|Zw997clFRcTAHrWiO...|260.0|       1|\n",
      "+-------------------+--------------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pyspark_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=pyspark_custom_imputation_pipeline, \n",
    "                                                                          lower=daily_stats_features_lower, \n",
    "                                                                          upper=daily_stats_features_upper)\n",
    "\n",
    "pyspark_df_added_binary_labels.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PySpark: Features: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=pyspark_df_added_binary_labels,\n",
    "                                                                                 daily_stats_features_lower=daily_stats_features_lower,\n",
    "                                                                                 daily_stats_features_upper=daily_stats_features_upper)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+\n",
      "|           PatientId|Chunk| GlucoseDisplayTime|Value|y_Binary|index|y_summary_binary|              Mean|          Std Dev|Median|  Min|  Max|CountBelow|CountAbove|PercentageBelow|   PercentageAbove|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+\n",
      "|Zw997clFRcTAHrWiO...|    0|2022-02-18 17:55:00|271.0|       1|   11|               1|273.45454545454544|8.017027333914188| 272.0|260.0|284.0|         0|        11|            0.0|0.9166666666666666|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pyspark_features_summary_stats.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PySpark: Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. PySpark: Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/20 05:10:16 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "|           PatientId|Chunk| GlucoseDisplayTime|Value|y_Binary|index|y_summary_binary|              Mean|          Std Dev|Median|  Min|  Max|CountBelow|CountAbove|PercentageBelow|   PercentageAbove|value_lag_1|        mean_lag_1|    std_dev_lag_1|med_lag_1|min_lag_1|max_lag_1|cnt_bel_lag_1|cnt_abv_lag_1|perc_belw_lag_1|    perc_abv_lag_1|value_lag_2|        mean_lag_2|    std_dev_lag_2|med_lag_2|min_lag_2|max_lag_2|cnt_bel_lag_2|cnt_abv_lag_2|perc_belw_lag_2|    perc_abv_lag_2|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "|Zw997clFRcTAHrWiO...|    0|2022-02-18 17:15:00|275.0|       1|    3|               1|273.45454545454544|8.017027333914188| 272.0|260.0|284.0|         0|        11|            0.0|0.9166666666666666|      266.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|      260.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|\n",
      "|Zw997clFRcTAHrWiO...|    0|2022-02-18 17:20:00|283.0|       1|    4|               1|273.45454545454544|8.017027333914188| 272.0|260.0|284.0|         0|        11|            0.0|0.9166666666666666|      275.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|      266.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|\n",
      "|Zw997clFRcTAHrWiO...|    0|2022-02-18 17:25:00|284.0|       1|    5|               1|273.45454545454544|8.017027333914188| 272.0|260.0|284.0|         0|        11|            0.0|0.9166666666666666|      283.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|      275.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|\n",
      "|Zw997clFRcTAHrWiO...|    0|2022-02-18 17:30:00|283.0|       1|    6|               1|273.45454545454544|8.017027333914188| 272.0|260.0|284.0|         0|        11|            0.0|0.9166666666666666|      284.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|      283.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|\n",
      "|Zw997clFRcTAHrWiO...|    0|2022-02-18 17:35:00|279.0|       1|    7|               1|273.45454545454544|8.017027333914188| 272.0|260.0|284.0|         0|        11|            0.0|0.9166666666666666|      283.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|      284.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####### PySpark\n",
    "pyspark_lag_features_creation=create_lag_features.pyspark_lag_features(df=pyspark_features_summary_stats,\n",
    "                                                                       time_series_lag_values_created=time_series_lag_values_created)\n",
    "pyspark_lag_features_creation.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. PySpark: Sklearn Categorical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_categorical_schema=pandas_udf_data_schema.sklearn_pyspark_categorical_schema()\n",
    "pyspark_transformations_categorical=pandas_sklearn_pipeline.pyspark_sklearn_pipeline_categorical(df=pyspark_features_summary_stats, \n",
    "                                                                                                 output_schema=pyspark_categorical_schema,\n",
    "                                                                                                 analysis_group=analysis_group)\n",
    "pyspark_transformations_categorical.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. PySpark: Sklearn Numerical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 550:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-------------------+-------------+--------------------+--------------------+-------------+--------------------+-------------+-------------+-------------------+-------------------+---------------+-------------+-------------+-------------------+---------------+--------------------+----------------------+--------------------+---------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+-------------------+--------------------+--------------------+---------------------+\n",
      "|           PatientId|Chunk| GlucoseDisplayTime|Value|y_Binary|index|y_summary_binary|              Mean|          Std Dev|Median|  Min|  Max|CountBelow|CountAbove|PercentageBelow|   PercentageAbove|value_lag_1|        mean_lag_1|    std_dev_lag_1|med_lag_1|min_lag_1|max_lag_1|cnt_bel_lag_1|cnt_abv_lag_1|perc_belw_lag_1|    perc_abv_lag_1|value_lag_2|        mean_lag_2|    std_dev_lag_2|med_lag_2|min_lag_2|max_lag_2|cnt_bel_lag_2|cnt_abv_lag_2|perc_belw_lag_2|    perc_abv_lag_2|perc_belw_lag_1_vec|min_lag_2_vec|  perc_abv_lag_2_vec|      mean_lag_2_vec|max_lag_2_vec|      mean_lag_1_vec|med_lag_1_vec|min_lag_1_vec|  std_dev_lag_2_vec|perc_belw_lag_2_vec|value_lag_2_vec|med_lag_2_vec|max_lag_1_vec|  std_dev_lag_1_vec|value_lag_1_vec|  perc_abv_lag_1_vec|perc_belw_lag_1_scaled|    min_lag_2_scaled|perc_abv_lag_2_scaled|   mean_lag_2_scaled|   max_lag_2_scaled|   mean_lag_1_scaled|    med_lag_1_scaled|    min_lag_1_scaled|std_dev_lag_2_scaled|perc_belw_lag_2_scaled|  value_lag_2_scaled|    med_lag_2_scaled|   max_lag_1_scaled|std_dev_lag_1_scaled|  value_lag_1_scaled|perc_abv_lag_1_scaled|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-------------------+-------------+--------------------+--------------------+-------------+--------------------+-------------+-------------+-------------------+-------------------+---------------+-------------+-------------+-------------------+---------------+--------------------+----------------------+--------------------+---------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+-------------------+--------------------+--------------------+---------------------+\n",
      "|Zw997clFRcTAHrWiO...|    0|2022-02-18 17:15:00|275.0|       1|    3|               1|273.45454545454544|8.017027333914188| 272.0|260.0|284.0|         0|        11|            0.0|0.9166666666666666|      266.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|      260.0|273.45454545454544|8.017027333914188|    272.0|    260.0|    284.0|            0|           11|            0.0|0.9166666666666666|              [0.0]|      [260.0]|[0.9166666666666666]|[273.45454545454544]|      [284.0]|[273.45454545454544]|      [272.0]|      [260.0]|[8.017027333914188]|              [0.0]|        [260.0]|      [272.0]|      [284.0]|[8.017027333914188]|        [266.0]|[0.9166666666666666]|                 [0.0]|[14.388852410212348]| [1.8380882867131774]|[15.077810817609206]|[15.26408812564875]|[15.078201331644328]|[14.973598230779592]|[14.389351676962718]| [3.790818102955339]|                 [0.0]|[14.245368752828021]|[14.973374678319434]|[15.26447970660592]|  [3.79076378735449]|[14.574470099143936]|  [1.838095053149909]|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+-----------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-------------------+-------------+--------------------+--------------------+-------------+--------------------+-------------+-------------+-------------------+-------------------+---------------+-------------+-------------+-------------------+---------------+--------------------+----------------------+--------------------+---------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+--------------------+-------------------+--------------------+--------------------+---------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "####### PySpark\n",
    "pyspark_numerical_features=feature_transformations.numerical_scaling(df=pyspark_lag_features_creation)\n",
    "pyspark_numerical_features.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. PySpark: XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Model_Creation.pyspark_xgboost import Create_PySpark_XGBoost\n",
    "create_pyspark_xgboost=Create_PySpark_XGBoost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "features_cols param requires enabling use_gpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_model\u001b[38;5;241m=\u001b[39m\u001b[43mcreate_pyspark_xgboost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxgboost_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyspark_numerical_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/Model_Creation/pyspark_xgboost.py:19\u001b[0m, in \u001b[0;36mCreate_PySpark_XGBoost.xgboost_classifier\u001b[0;34m(self, ml_df)\u001b[0m\n\u001b[1;32m     13\u001b[0m regressor \u001b[38;5;241m=\u001b[39m SparkXGBRegressor(features_col\u001b[38;5;241m=\u001b[39mscaled_features,\n\u001b[1;32m     14\u001b[0m                               label_col\u001b[38;5;241m=\u001b[39mlabel_name,\n\u001b[1;32m     15\u001b[0m                               num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     16\u001b[0m                               use_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# train and return the model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mregressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# predict on test data\u001b[39;00m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/xgboost/spark/core.py:657\u001b[0m, in \u001b[0;36m_SparkXGBEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# pylint: disable=too-many-statements, too-many-locals\u001b[39;00m\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m     label_col \u001b[38;5;241m=\u001b[39m col(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetOrDefault(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabelCol))\u001b[38;5;241m.\u001b[39malias(alias\u001b[38;5;241m.\u001b[39mlabel)\n\u001b[1;32m    660\u001b[0m     select_cols \u001b[38;5;241m=\u001b[39m [label_col]\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/xgboost/spark/estimator.py:111\u001b[0m, in \u001b[0;36mSparkXGBRegressor._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misDefined(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqid_col):\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark Xgboost regressor estimator does not support `qid_col` param.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         )\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/xgboost/spark/core.py:296\u001b[0m, in \u001b[0;36m_SparkXGBParams._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetOrDefault(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_cols):\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetOrDefault(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_gpu):\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures_cols param requires enabling use_gpu.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m     get_logger(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf features_cols param set, then features_col param is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetOrDefault(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: features_cols param requires enabling use_gpu."
     ]
    }
   ],
   "source": [
    "test_model=create_pyspark_xgboost.xgboost_classifier(ml_df=pyspark_numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_xgboost_classsification_schema=pandas_udf_data_schema.xgboost_classification_schema()\n",
    "\n",
    "classification_model_outputs=xgboost_classification.pyspark_xgboost(df=pyspark_lag_features_creation, \n",
    "                                                                    output_schema=pyspark_xgboost_classsification_schema, \n",
    "                                                                    train_split=ml_models_train_split, \n",
    "                                                                    test_split=ml_models_test_split)\n",
    "\n",
    "classification_model_outputs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. PySpark: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PySpark\n",
    "pyspark_classification_metric_schema=pandas_udf_data_schema.classification_metric_schema()\n",
    "\n",
    "classification_metric_df=classification_evalaution_metrics.pyspark_classification_model_evaluation_metrics(df=classification_model_outputs, \n",
    "                                                                                                           output_schema=pyspark_classification_metric_schema)\n",
    "classification_metric_df.show()                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_classification_plot.read_model_plot_variance(model_storage_location=model_storage_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_df=reading_data.read_in_pandas()\n",
    "pandas_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pat=pandas_df[pandas_df['PatientId']=='tHu8WPnIffml5CL+AbOBkXcbFApQnP06KdrHbjinta4=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_custom_imputation_pipeline=pandas_sklearn_pipeline.pandas_custom_imputation_pipeline(df=pandas_df)\n",
    "pandas_custom_imputation_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_imputation_pipeline=pandas_sklearn_pipeline.pandas_custom_imputation_pipeline(df=pandas_df)\n",
    "pandas_custom_imputation_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Aggregate Data at Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_added_binary_labels=create_binary_labels.pandas_binary_labels(df=pandas_custom_imputation_pipeline, \n",
    "                                                                        lower=daily_stats_features_lower, \n",
    "                                                                        upper=daily_stats_features_upper)\n",
    "pandas_df_added_binary_labels.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Features: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_features_summary_stats=summary_stats_features.pandas_compressDailyValues(data=pandas_df_added_binary_labels, \n",
    "                                                                                lower=daily_stats_features_lower, \n",
    "                                                                                upper=daily_stats_features_upper)\n",
    "pandas_features_summary_stats.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Sklearn Categorical Pipeline in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df=pandas_features_summary_stats[['PatientId', 'Value', 'GlucoseDisplayTime', 'GlucoseDisplayDate', 'inserted', \n",
    "        'missing', 'y_Binary', 'Median', 'Mean', 'Std Dev', 'Max', 'Min', 'AreaBelow', 'AreaAbove']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in df['PatientId'].unique():\n",
    "    # Categorical Features\n",
    "    categorical_features=['inserted', 'missing']\n",
    "    categorical_transformer=Pipeline([('imputer_cat', SimpleImputer(strategy='constant', fill_value=np.nan)),\n",
    "                                        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor_2=ColumnTransformer([('categorical', categorical_transformer, categorical_features)],\n",
    "                                    remainder = 'passthrough')\n",
    "\n",
    "    cat_pipe_pipeline=Pipeline([('preprocessing_2', preprocessor_2)])\n",
    "\n",
    "    transformed_data1=cat_pipe_pipeline.fit_transform(df)\n",
    "\n",
    "    transformed_data_df=pd.DataFrame(transformed_data1)\n",
    "\n",
    "    transformed_data_df['combine_inserted']=transformed_data_df[[0,1]].values.tolist()\n",
    "    transformed_data_df['combine_missing']=transformed_data_df[[2,3]].values.tolist()\n",
    "    transformed_data_df=transformed_data_df.drop(transformed_data_df.iloc[:, 0:4],axis = 1)\n",
    "\n",
    "    transformed_data_df.columns=['PatientId', 'Value', 'GlucoseDisplayTime', 'GlucoseDisplayDate', \n",
    "                                    'y_Binary', 'Median', 'Mean', 'Std Dev', 'Max', 'Min', 'AreaBelow', \n",
    "                                    'AreaAbove', 'inserted', 'missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_categorical_pipeline=pandas_sklearn_pipeline.pandas_transform_categorical_features(df=pandas_features_summary_stats)\n",
    "pandas_custom_categorical_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Sklearn Numerical Pipeline in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_numerical_pipeline=pandas_sklearn_pipeline.pandas_transform_numerical_features(df=pandas_custom_categorical_pipeline)\n",
    "pandas_custom_numerical_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e282f9bbb605e17ec33c138684786f2c6a0e45a19b318784bb5671e7a7934052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
