{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Process\n",
    "1. Read in data --> Done\n",
    "\n",
    "2. Custom Imputation --> Done\n",
    "\n",
    "3. Add Binary Class --> Done, Should Add Binary Class Later\n",
    "\n",
    "4. Summary Statistics Features --> Done\n",
    "\n",
    "5. Wrapper Functions --> Done, Need to Test Though\n",
    "\n",
    "6. Sklearn Pipeline Categorical Features --> One Hot Encoding Done\n",
    "\n",
    "7. Sklearn Pipeline Numerical Features --> StandardScaler Done\n",
    "\n",
    "8. Create Lagged Features --> Done\n",
    "\n",
    "9. Modeling --> Currently XgBoost, (Maybe Try: TensorFlow Decision Tree, TensorFlow Probability Model)\n",
    "\n",
    "10. Model Evaluation --> Accuracy, Precision, Recall, F1, Confusion Matrix (Need to add Variable Importance Based on Variance)\n",
    "\n",
    "11. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (12.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.24.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Collecting pyspark\n",
      "  Using cached pyspark-3.4.0-py2.py3-none-any.whl\n",
      "\u001b[31mERROR: Wheel 'pyspark' located at /home/jovyan/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327/pyspark-3.4.0-py2.py3-none-any.whl is invalid.\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: xgboost in /opt/conda/lib/python3.10/site-packages (1.7.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from xgboost) (1.10.1)\n",
      "Requirement already satisfied: kaleido in /opt/conda/lib/python3.10/site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "# # Need to Run These in Notebook Version For Pandas UDF\n",
    "! pip install pyarrow\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install pyspark\n",
    "! pip install xgboost\n",
    "! pip install kaleido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Input_Variables.read_vars import train_data_storage, validation_data_storage, test_data_storage, \\\n",
    "                                      analysis_group, \\\n",
    "                                      daily_stats_features_lower, daily_stats_features_upper, \\\n",
    "                                      model_storage_location, random_seed, \\\n",
    "                                      time_series_lag_values_created, \\\n",
    "                                      evaluation_metrics_output_storage, \\\n",
    "                                      feature_importance_storage_location, \\\n",
    "                                      overall_feature_importance_plot_location\n",
    "\n",
    "from Data_Schema.schema import Pandas_UDF_Data_Schema\n",
    "from Read_In_Data.read_data import Reading_Data\n",
    "from Data_Pipeline.imputation_pipeline import Date_And_Value_Imputation\n",
    "\n",
    "from Feature_Generation.create_binary_labels import Create_Binary_Labels\n",
    "from Feature_Generation.summary_stats import Summary_Stats_Features\n",
    "from Feature_Generation.lag_features import Create_Lagged_Features\n",
    "from Feature_Generation.time_series_feature_creation import TS_Features\n",
    "from Feature_Generation.difference_features import Difference_Features\n",
    "\n",
    "from Data_Pipeline.encoding_scaling_pipeline import Feature_Transformations\n",
    "\n",
    "from Model_Creation.pyspark_xgboost import Create_PySpark_XGBoost\n",
    "\n",
    "from Model_Predictions.pyspark_model_preds import Model_Predictions\n",
    "\n",
    "from Model_Evaluation.pyspark_model_eval import Evaluate_Model\n",
    "\n",
    "from Feature_Importance.model_feature_importance import Feature_Importance\n",
    "\n",
    "from Model_Plots.xgboost_classification_plots import XGBoost_Classification_Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PySpark UDF Schema Activation\n",
    "pandas_udf_data_schema=Pandas_UDF_Data_Schema()\n",
    "\n",
    "# Data Location\n",
    "reading_data=Reading_Data()\n",
    "\n",
    "# Create Binary y Variables\n",
    "create_binary_labels=Create_Binary_Labels()\n",
    "\n",
    "# Imputation\n",
    "date_and_value_imputation=Date_And_Value_Imputation()\n",
    "\n",
    "# Features Daily Stats Module\n",
    "summary_stats_features=Summary_Stats_Features()\n",
    "\n",
    "# Features Complex\n",
    "ts_features=TS_Features()\n",
    "\n",
    "# Features Lagged Value\n",
    "create_lag_features=Create_Lagged_Features()\n",
    "\n",
    "# Features Differences\n",
    "difference_features=Difference_Features()\n",
    "\n",
    "# PySpark XGBoost Model Module\n",
    "create_pyspark_xgboost=Create_PySpark_XGBoost()\n",
    "\n",
    "# Classification Evaluation\n",
    "evaluate_model=Evaluate_Model()\n",
    "\n",
    "# Model Plots Feature Importance\n",
    "xgboost_classification_plot=XGBoost_Classification_Plot()\n",
    "\n",
    "# Feature Transformations\n",
    "feature_transformations=Feature_Transformations()\n",
    "\n",
    "\n",
    "pyspark_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "\n",
    "\n",
    "model_predictions=Model_Predictions()\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance=Feature_Importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. PySpark: Reading In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|8W/rpnb48OMm47W2x...|328.0|2022-01-31 17:38:00| 2022-01-31T17:38:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|331.0|2022-01-31 17:43:00| 2022-01-31T17:43:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|329.0|2022-01-31 17:48:00| 2022-01-31T17:48:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|321.0|2022-01-31 17:53:00| 2022-01-31T17:53:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|315.0|2022-01-31 17:58:00| 2022-01-31T17:58:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|313.0|2022-01-31 18:03:00| 2022-01-31T18:03:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|304.0|2022-01-31 18:08:00| 2022-01-31T18:08:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|298.0|2022-01-31 18:13:00| 2022-01-31T18:13:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|294.0|2022-01-31 18:18:00| 2022-01-31T18:18:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|291.0|2022-01-31 18:23:00| 2022-01-31T18:23:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|289.0|2022-01-31 18:28:00| 2022-01-31T18:28:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|288.0|2022-01-31 18:33:00| 2022-01-31T18:33:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|286.0|2022-01-31 18:38:00| 2022-01-31T18:38:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|283.0|2022-01-31 18:43:00| 2022-01-31T18:43:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|282.0|2022-01-31 18:48:00| 2022-01-31T18:48:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|280.0|2022-01-31 18:53:00| 2022-01-31T18:53:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|278.0|2022-01-31 18:58:00| 2022-01-31T18:58:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|276.0|2022-01-31 19:03:00| 2022-01-31T19:03:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|274.0|2022-01-31 19:08:00| 2022-01-31T19:08:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|271.0|2022-01-31 19:13:00| 2022-01-31T19:13:...|        2022-01-31|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df=reading_data.read_in_pyspark_training(training_data_location=train_data_storage)\n",
    "training_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 16:59:00| 2022-02-08T16:59:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:04:00| 2022-02-08T17:04:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:09:00| 2022-02-08T17:09:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:14:00| 2022-02-08T17:14:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:19:00| 2022-02-08T17:19:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:24:00| 2022-02-08T17:24:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|277.0|2022-02-08 17:29:00| 2022-02-08T17:29:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|270.0|2022-02-08 17:34:00| 2022-02-08T17:34:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|268.0|2022-02-08 17:39:00| 2022-02-08T17:39:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|259.0|2022-02-08 17:44:00| 2022-02-08T17:44:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|246.0|2022-02-08 17:49:00| 2022-02-08T17:49:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|232.0|2022-02-08 17:54:00| 2022-02-08T17:54:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|225.0|2022-02-08 17:59:00| 2022-02-08T17:59:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|228.0|2022-02-08 18:04:00| 2022-02-08T18:04:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|249.0|2022-02-08 18:09:00| 2022-02-08T18:09:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|273.0|2022-02-08 18:14:00| 2022-02-08T18:14:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|294.0|2022-02-08 18:19:00| 2022-02-08T18:19:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|308.0|2022-02-08 18:24:00| 2022-02-08T18:24:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|311.0|2022-02-08 18:29:00| 2022-02-08T18:29:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|307.0|2022-02-08 18:34:00| 2022-02-08T18:34:...|        2022-02-08|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df=reading_data.read_in_pyspark_testing(testing_data_location=test_data_storage)\n",
    "testing_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. PySpark: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 437:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+\n",
      "| GlucoseDisplayTime|           PatientId|Value|\n",
      "+-------------------+--------------------+-----+\n",
      "|2022-01-31 17:35:00|8W/rpnb48OMm47W2x...|328.0|\n",
      "+-------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "training_custom_imputation_pipeline=date_and_value_imputation.\\\n",
    "                                        pyspark_custom_imputation_pipeline(df=training_df, \n",
    "                                                                           output_schema=pyspark_custom_imputation_schema,\n",
    "                                                                           analysis_group=analysis_group)\n",
    "\n",
    "training_custom_imputation_pipeline.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 449:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+\n",
      "| GlucoseDisplayTime|           PatientId|    Value|\n",
      "+-------------------+--------------------+---------+\n",
      "|2022-02-08 16:55:00|8W/rpnb48OMm47W2x...|164.20488|\n",
      "+-------------------+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "testing_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "testing_custom_imputation_pipeline=date_and_value_imputation.\\\n",
    "                                        pyspark_custom_imputation_pipeline(df=testing_df, \n",
    "                                                                           output_schema=pyspark_custom_imputation_schema,\n",
    "                                                                           analysis_group=analysis_group)\n",
    "\n",
    "testing_custom_imputation_pipeline.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. PySpark: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1272:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------+\n",
      "| GlucoseDisplayTime|           PatientId|Value|y_Binary|\n",
      "+-------------------+--------------------+-----+--------+\n",
      "|2022-01-31 17:35:00|8W/rpnb48OMm47W2x...|328.0|       1|\n",
      "+-------------------+--------------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=training_custom_imputation_pipeline, \n",
    "                                                                          lower=daily_stats_features_lower, \n",
    "                                                                          upper=daily_stats_features_upper)\n",
    "\n",
    "training_df_added_binary_labels.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1284:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------------------------------+---------+--------+\n",
      "|GlucoseDisplayTime |PatientId                                   |Value    |y_Binary|\n",
      "+-------------------+--------------------------------------------+---------+--------+\n",
      "|2022-02-08 16:55:00|8W/rpnb48OMm47W2x4FSkc7+9u2mol061DQuJoMdiK0=|164.20488|0       |\n",
      "+-------------------+--------------------------------------------+---------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "testing_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=testing_custom_imputation_pipeline, \n",
    "                                                                          lower=daily_stats_features_lower, \n",
    "                                                                          upper=daily_stats_features_upper)\n",
    "\n",
    "testing_df_added_binary_labels.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PySpark: Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1296:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------+---------+-------+\n",
      "| GlucoseDisplayTime|           PatientId|Value|y_Binary|FirstDiff|SecDiff|\n",
      "+-------------------+--------------------+-----+--------+---------+-------+\n",
      "|2022-01-31 17:35:00|8W/rpnb48OMm47W2x...|328.0|       1|      0.0|    0.0|\n",
      "|2022-01-31 17:40:00|8W/rpnb48OMm47W2x...|331.0|       1|      3.0|    3.0|\n",
      "|2022-01-31 17:45:00|8W/rpnb48OMm47W2x...|329.0|       1|     -2.0|   -5.0|\n",
      "|2022-01-31 17:50:00|8W/rpnb48OMm47W2x...|321.0|       1|     -8.0|   -6.0|\n",
      "|2022-01-31 17:55:00|8W/rpnb48OMm47W2x...|315.0|       1|     -6.0|    2.0|\n",
      "+-------------------+--------------------+-----+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df_differences = difference_features.add_difference_features(training_df_added_binary_labels)\n",
    "training_df_differences.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------+---------+-------+-----+-----+\n",
      "| GlucoseDisplayTime|           PatientId|Value|y_Binary|FirstDiff|SecDiff|index|Chunk|\n",
      "+-------------------+--------------------+-----+--------+---------+-------+-----+-----+\n",
      "|2022-01-31 17:35:00|8W/rpnb48OMm47W2x...|328.0|       1|      0.0|    0.0|    1|    0|\n",
      "|2022-01-31 17:40:00|8W/rpnb48OMm47W2x...|331.0|       1|      3.0|    3.0|    2|    0|\n",
      "|2022-01-31 17:45:00|8W/rpnb48OMm47W2x...|329.0|       1|     -2.0|   -5.0|    3|    0|\n",
      "|2022-01-31 17:50:00|8W/rpnb48OMm47W2x...|321.0|       1|     -8.0|   -6.0|    4|    0|\n",
      "|2022-01-31 17:55:00|8W/rpnb48OMm47W2x...|315.0|       1|     -6.0|    2.0|    5|    0|\n",
      "+-------------------+--------------------+-----+--------+---------+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:46: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = round(short_term_variation / long_term_variation, 3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+----------------+-------------+\n",
      "|           PatientId|Chunk|ShortTermVariance|LongTermVariance|VarianceRatio|\n",
      "+--------------------+-----+-----------------+----------------+-------------+\n",
      "|8W/rpnb48OMm47W2x...|    0|            3.619|           4.698|         0.77|\n",
      "|8W/rpnb48OMm47W2x...|    1|            3.856|           7.476|        0.516|\n",
      "|8W/rpnb48OMm47W2x...|    2|            2.693|           5.503|        0.489|\n",
      "|8W/rpnb48OMm47W2x...|    3|            4.064|             6.8|        0.598|\n",
      "|8W/rpnb48OMm47W2x...|    4|            4.215|           6.338|        0.665|\n",
      "+--------------------+-----+-----------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1352:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|           PatientId|Chunk|   Entropy|\n",
      "+--------------------+-----+----------+\n",
      "|8W/rpnb48OMm47W2x...|    0|0.06102413|\n",
      "|8W/rpnb48OMm47W2x...|    1|0.16895121|\n",
      "|8W/rpnb48OMm47W2x...|    2|0.21363738|\n",
      "|8W/rpnb48OMm47W2x...|    3|0.21762376|\n",
      "|8W/rpnb48OMm47W2x...|    4|0.11467516|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df_chunks = summary_stats_features.create_chunk_col(training_df_differences, chunk_val = 288)\n",
    "training_df_chunks.show(5)\n",
    "\n",
    "training_df_poincare = training_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.poincare)\n",
    "training_df_poincare.show(5)\n",
    "\n",
    "training_df_entropy = training_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.entropy)\n",
    "training_df_entropy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1369:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 02:08:02 ERROR Executor: Exception in task 0.0 in stage 1369.0 (TID 498)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n",
      "    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n",
      "    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\n",
      "AssertionError: Sig:   must be a numpy vector\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:85)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/03 02:08:02 WARN TaskSetManager: Lost task 0.0 in stage 1369.0 (TID 498) (jupyter-kkanjaria-40ucsd-2eedu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n",
      "    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n",
      "    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\n",
      "AssertionError: Sig:   must be a numpy vector\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:85)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "23/05/03 02:08:02 ERROR TaskSetManager: Task 0 in stage 1369.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\nAssertionError: Sig:   must be a numpy vector\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m training_df_complex_features \u001b[38;5;241m=\u001b[39m training_df_poincare\u001b[38;5;241m.\u001b[39mjoin(training_df_entropy,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatientId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChunk\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtraining_df_complex_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\nAssertionError: Sig:   must be a numpy vector\n"
     ]
    }
   ],
   "source": [
    "training_df_complex_features = training_df_poincare.join(training_df_entropy,['PatientId', 'Chunk'])\n",
    "training_df_complex_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=training_df_chunks,\n",
    "                                                                                 daily_stats_features_lower=daily_stats_features_lower,\n",
    "                                                                                 daily_stats_features_upper=daily_stats_features_upper)\n",
    "# merge complex features and summary stats and demographics\n",
    "# merge in one hot encoded cohort file info demographics\n",
    "    # '/cephfs/data/cohort_encoded.parquet' (gender, treatment, age category)\n",
    "    # groupby patientId and chunk\n",
    "\n",
    "training_features_summary_stats.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1381:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+--------+---------+-------+\n",
      "| GlucoseDisplayTime|           PatientId|    Value|y_Binary|FirstDiff|SecDiff|\n",
      "+-------------------+--------------------+---------+--------+---------+-------+\n",
      "|2022-02-08 16:55:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|\n",
      "|2022-02-08 17:00:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|\n",
      "|2022-02-08 17:05:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|\n",
      "|2022-02-08 17:10:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|\n",
      "|2022-02-08 17:15:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|\n",
      "+-------------------+--------------------+---------+--------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "testing_df_differences = difference_features.add_difference_features(testing_df_added_binary_labels)\n",
    "testing_df_differences.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+--------+---------+-------+-----+-----+\n",
      "| GlucoseDisplayTime|           PatientId|    Value|y_Binary|FirstDiff|SecDiff|index|Chunk|\n",
      "+-------------------+--------------------+---------+--------+---------+-------+-----+-----+\n",
      "|2022-02-08 16:55:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|    1|    0|\n",
      "|2022-02-08 17:00:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|    2|    0|\n",
      "|2022-02-08 17:05:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|    3|    0|\n",
      "|2022-02-08 17:10:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|    4|    0|\n",
      "|2022-02-08 17:15:00|8W/rpnb48OMm47W2x...|164.20488|       0|      0.0|    0.0|    5|    0|\n",
      "+-------------------+--------------------+---------+--------+---------+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----------------+----------------+-------------+\n",
      "|           PatientId|Chunk|ShortTermVariance|LongTermVariance|VarianceRatio|\n",
      "+--------------------+-----+-----------------+----------------+-------------+\n",
      "|8W/rpnb48OMm47W2x...|    0|           13.264|          14.547|        0.912|\n",
      "|8W/rpnb48OMm47W2x...|    1|            8.753|           8.528|        1.026|\n",
      "|8W/rpnb48OMm47W2x...|    2|            1.732|           4.848|        0.357|\n",
      "|CzndP9OQqEYW/LY7h...|    0|            4.418|           8.068|        0.548|\n",
      "|CzndP9OQqEYW/LY7h...|    1|            4.176|           7.156|        0.584|\n",
      "+--------------------+-----+-----------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1437:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 02:13:44 ERROR Executor: Exception in task 0.0 in stage 1437.0 (TID 522)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n",
      "    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n",
      "    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\n",
      "AssertionError: Sig:   must be a numpy vector\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:85)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/03 02:13:44 WARN TaskSetManager: Lost task 0.0 in stage 1437.0 (TID 522) (jupyter-kkanjaria-40ucsd-2eedu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n",
      "    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n",
      "    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\n",
      "AssertionError: Sig:   must be a numpy vector\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:85)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage10.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "23/05/03 02:13:44 ERROR TaskSetManager: Task 0 in stage 1437.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\nAssertionError: Sig:   must be a numpy vector\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m testing_df_poincare\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m testing_df_entropy \u001b[38;5;241m=\u001b[39m testing_df_chunks\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatientId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChunk\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mapply(ts_features\u001b[38;5;241m.\u001b[39mentropy)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtesting_df_entropy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\nAssertionError: Sig:   must be a numpy vector\n"
     ]
    }
   ],
   "source": [
    "testing_df_chunks = summary_stats_features.create_chunk_col(testing_df_differences, chunk_val = 288)\n",
    "testing_df_chunks.show(5)\n",
    "\n",
    "testing_df_poincare = testing_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.poincare)\n",
    "testing_df_poincare.show(5)\n",
    "\n",
    "testing_df_entropy = testing_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.entropy)\n",
    "testing_df_entropy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1369:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/03 02:08:02 ERROR Executor: Exception in task 0.0 in stage 1369.0 (TID 498)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n",
      "    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n",
      "    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\n",
      "AssertionError: Sig:   must be a numpy vector\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:85)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/03 02:08:02 WARN TaskSetManager: Lost task 0.0 in stage 1369.0 (TID 498) (jupyter-kkanjaria-40ucsd-2eedu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n",
      "    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n",
      "    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\n",
      "AssertionError: Sig:   must be a numpy vector\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:85)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage15.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "23/05/03 02:08:02 ERROR TaskSetManager: Task 0 in stage 1369.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\nAssertionError: Sig:   must be a numpy vector\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m training_df_complex_features \u001b[38;5;241m=\u001b[39m training_df_poincare\u001b[38;5;241m.\u001b[39mjoin(training_df_entropy,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPatientId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChunk\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtraining_df_complex_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py\", line 18, in entropy\n    entropy = eH.SampEn(df.Value.values, m=4)[0][-1]\n  File \"/opt/conda/lib/python3.10/site-packages/EntropyHub/_SampEn.py\", line 43, in SampEn\n    assert N>10 and Sig.ndim == 1,  \"Sig:   must be a numpy vector\"\nAssertionError: Sig:   must be a numpy vector\n"
     ]
    }
   ],
   "source": [
    "testing_df_complex_features = testing_df_poincare.join(testing_df_entropy,['PatientId', 'Chunk'])\n",
    "testing_df_complex_features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=testing_df_added_binary_labels,\n",
    "                                                                                 daily_stats_features_lower=daily_stats_features_lower,\n",
    "                                                                                 daily_stats_features_upper=daily_stats_features_upper)\n",
    "\n",
    "# merge complex features and summary stats and demographics\n",
    "# merge in one hot encoded cohort file info demographics\n",
    "    # '/cephfs/data/cohort_encoded.parquet' (gender, treatment, age category)\n",
    "    # groupby patientId and chunk\n",
    "\n",
    "testing_features_summary_stats.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PySpark: Lag Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_lag_features_creation=create_lag_features.pyspark_lag_features(df=training_features_summary_stats,\n",
    "                                                                       time_series_lag_values_created=time_series_lag_values_created)\n",
    "training_lag_features_creation.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_lag_features_creation=create_lag_features.pyspark_lag_features(df=testing_features_summary_stats,\n",
    "                                                                       time_series_lag_values_created=time_series_lag_values_created)\n",
    "testing_lag_features_creation.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PySpark: Sklearn Categorical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. PySpark: Sklearn Numerical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_numerical_stages=feature_transformations.numerical_scaling(df=training_lag_features_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. PySpark: XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_model=create_pyspark_xgboost.xgboost_classifier(ml_df=training_lag_features_creation,\n",
    "                                                        stages=training_numerical_stages,\n",
    "                                                        model_storage_location=model_storage_location,\n",
    "                                                        random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. PySpark: Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. PySpark: Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_predictions=model_predictions.create_predictions_with_model(test_df=testing_lag_features_creation, \n",
    "                                                                    model=xgboost_model)\n",
    "testing_predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. PySpark: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluation=evaluate_model.classification_evaluation(testing_predictions=testing_predictions, \n",
    "                                                          eval_csv_location=evaluation_metrics_output_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance_df=feature_importance.\\\n",
    "                        feature_importance_accuracy_gain(xgboost_model=xgboost_model, \n",
    "                                                         feature_importance_storage_location=feature_importance_storage_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. PySpark: Feature Importance Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_feature_plot=xgboost_classification_plot.feature_overall_importance_plot(feature_importance_df=feature_importance_df,\n",
    "                                                                                 overall_importance_plot_location=overall_feature_importance_plot_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_feature_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.PySpark: Local Level Feature Importance --> Shap Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add to reqs if this works\n",
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_model.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgboost_model.stages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e282f9bbb605e17ec33c138684786f2c6a0e45a19b318784bb5671e7a7934052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
