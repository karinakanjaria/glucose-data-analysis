{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Current Process\n",
    "1. Read in data --> Done\n",
    "\n",
    "2. Custom Imputation --> Done\n",
    "\n",
    "3. Add Binary Class --> Done, Should Add Binary Class Later\n",
    "\n",
    "4. Summary Statistics Features --> Done\n",
    "\n",
    "5. Wrapper Functions --> Done, Need to Test Though\n",
    "\n",
    "6. Sklearn Pipeline Categorical Features --> One Hot Encoding Done\n",
    "\n",
    "7. Sklearn Pipeline Numerical Features --> StandardScaler Done\n",
    "\n",
    "8. Create Lagged Features --> Done\n",
    "\n",
    "9. Modeling --> Currently XgBoost, (Maybe Try: TensorFlow Decision Tree, TensorFlow Probability Model)\n",
    "\n",
    "10. Model Evaluation --> Accuracy, Precision, Recall, F1, Confusion Matrix (Need to add Variable Importance Based on Variance)\n",
    "\n",
    "11. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Need to Run These in Notebook Version For Pandas UDF\n",
    "! pip install pyarrow\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install pyspark\n",
    "! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Input_Variables.read_vars import train_data_storage, validation_data_storage, test_data_storage, \\\n",
    "                                      analysis_group, \\\n",
    "                                      daily_stats_features_lower, daily_stats_features_upper, \\\n",
    "                                      ml_models_train_split, ml_models_test_split, model_storage_location, \\\n",
    "                                      time_series_lag_values_created\n",
    "\n",
    "from Data_Schema.schema import Pandas_UDF_Data_Schema\n",
    "from Read_In_Data.read_data import Reading_Data\n",
    "from Data_Pipeline.sklearn_pipeline import Sklearn_Pipeline\n",
    "from Feature_Generation.create_binary_labels import Create_Binary_Labels\n",
    "from Feature_Generation.summary_stats import Summary_Stats_Features\n",
    "from Feature_Generation.lag_features import Create_Lagged_Features\n",
    "from Model_Evaluation.classification_evaluation import Classification_Evalaution_Metrics\n",
    "from Model_Plots.xgboost_classification_plots import XGBoost_Classification_Plot\n",
    "\n",
    "from Data_Pipeline.encoding_scaling_pipeline import Feature_Transformations\n",
    "\n",
    "from Model_Creation.pyspark_xgboost import Create_PySpark_XGBoost\n",
    "\n",
    "from Model_Predictions.pyspark_model_preds import Model_Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/21 06:19:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# PySpark UDF Schema Activation\n",
    "pandas_udf_data_schema=Pandas_UDF_Data_Schema()\n",
    "\n",
    "# Data Location\n",
    "reading_data=Reading_Data()\n",
    "\n",
    "# Create Binary y Variables\n",
    "create_binary_labels=Create_Binary_Labels()\n",
    "\n",
    "# Sklearn Pipeline\n",
    "pandas_sklearn_pipeline=Sklearn_Pipeline()\n",
    "\n",
    "# Features Daily Stats Module\n",
    "summary_stats_features=Summary_Stats_Features()\n",
    "\n",
    "# Features Lagged Value\n",
    "create_lag_features=Create_Lagged_Features()\n",
    "\n",
    "# PySpark XGBoost Model Module\n",
    "create_pyspark_xgboost=Create_PySpark_XGBoost()\n",
    "\n",
    "# Classification Evaluation\n",
    "classification_evalaution_metrics=Classification_Evalaution_Metrics()\n",
    "\n",
    "# Model Plots Feature Importance\n",
    "xgboost_classification_plot=XGBoost_Classification_Plot()\n",
    "\n",
    "# Feature Transformations\n",
    "feature_transformations=Feature_Transformations()\n",
    "\n",
    "\n",
    "pyspark_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "\n",
    "\n",
    "model_predictions=Model_Predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. PySpark: Reading In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|8W/rpnb48OMm47W2x...|328.0|2022-01-31 17:38:00| 2022-01-31T17:38:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|331.0|2022-01-31 17:43:00| 2022-01-31T17:43:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|329.0|2022-01-31 17:48:00| 2022-01-31T17:48:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|321.0|2022-01-31 17:53:00| 2022-01-31T17:53:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|315.0|2022-01-31 17:58:00| 2022-01-31T17:58:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|313.0|2022-01-31 18:03:00| 2022-01-31T18:03:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|304.0|2022-01-31 18:08:00| 2022-01-31T18:08:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|298.0|2022-01-31 18:13:00| 2022-01-31T18:13:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|294.0|2022-01-31 18:18:00| 2022-01-31T18:18:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|291.0|2022-01-31 18:23:00| 2022-01-31T18:23:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|289.0|2022-01-31 18:28:00| 2022-01-31T18:28:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|288.0|2022-01-31 18:33:00| 2022-01-31T18:33:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|286.0|2022-01-31 18:38:00| 2022-01-31T18:38:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|283.0|2022-01-31 18:43:00| 2022-01-31T18:43:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|282.0|2022-01-31 18:48:00| 2022-01-31T18:48:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|280.0|2022-01-31 18:53:00| 2022-01-31T18:53:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|278.0|2022-01-31 18:58:00| 2022-01-31T18:58:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|276.0|2022-01-31 19:03:00| 2022-01-31T19:03:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|274.0|2022-01-31 19:08:00| 2022-01-31T19:08:...|        2022-01-31|\n",
      "|8W/rpnb48OMm47W2x...|271.0|2022-01-31 19:13:00| 2022-01-31T19:13:...|        2022-01-31|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df=reading_data.read_in_pyspark_training(training_data_location=train_data_storage)\n",
    "training_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 16:59:00| 2022-02-08T16:59:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:04:00| 2022-02-08T17:04:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:09:00| 2022-02-08T17:09:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:14:00| 2022-02-08T17:14:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:19:00| 2022-02-08T17:19:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|  0.0|2022-02-08 17:24:00| 2022-02-08T17:24:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|277.0|2022-02-08 17:29:00| 2022-02-08T17:29:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|270.0|2022-02-08 17:34:00| 2022-02-08T17:34:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|268.0|2022-02-08 17:39:00| 2022-02-08T17:39:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|259.0|2022-02-08 17:44:00| 2022-02-08T17:44:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|246.0|2022-02-08 17:49:00| 2022-02-08T17:49:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|232.0|2022-02-08 17:54:00| 2022-02-08T17:54:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|225.0|2022-02-08 17:59:00| 2022-02-08T17:59:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|228.0|2022-02-08 18:04:00| 2022-02-08T18:04:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|249.0|2022-02-08 18:09:00| 2022-02-08T18:09:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|273.0|2022-02-08 18:14:00| 2022-02-08T18:14:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|294.0|2022-02-08 18:19:00| 2022-02-08T18:19:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|308.0|2022-02-08 18:24:00| 2022-02-08T18:24:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|311.0|2022-02-08 18:29:00| 2022-02-08T18:29:...|        2022-02-08|\n",
      "|8W/rpnb48OMm47W2x...|307.0|2022-02-08 18:34:00| 2022-02-08T18:34:...|        2022-02-08|\n",
      "+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df=reading_data.read_in_pyspark_testing(testing_data_location=test_data_storage)\n",
    "testing_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PySpark: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+\n",
      "| GlucoseDisplayTime|           PatientId|Value|\n",
      "+-------------------+--------------------+-----+\n",
      "|2022-01-31 17:35:00|8W/rpnb48OMm47W2x...|328.0|\n",
      "+-------------------+--------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "training_custom_imputation_pipeline=pandas_sklearn_pipeline.pyspark_custom_imputation_pipeline(df=training_df, \n",
    "                                                                                               output_schema=pyspark_custom_imputation_schema,\n",
    "                                                                                               analysis_group=analysis_group)\n",
    "\n",
    "training_custom_imputation_pipeline.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+\n",
      "| GlucoseDisplayTime|           PatientId|    Value|\n",
      "+-------------------+--------------------+---------+\n",
      "|2022-02-08 16:55:00|8W/rpnb48OMm47W2x...|164.20488|\n",
      "+-------------------+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "testing_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "testing_custom_imputation_pipeline=pandas_sklearn_pipeline.pyspark_custom_imputation_pipeline(df=testing_df, \n",
    "                                                                                               output_schema=pyspark_custom_imputation_schema,\n",
    "                                                                                               analysis_group=analysis_group)\n",
    "\n",
    "testing_custom_imputation_pipeline.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. PySpark: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----+--------+\n",
      "| GlucoseDisplayTime|           PatientId|Value|y_Binary|\n",
      "+-------------------+--------------------+-----+--------+\n",
      "|2022-01-31 17:35:00|8W/rpnb48OMm47W2x...|328.0|       1|\n",
      "+-------------------+--------------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=training_custom_imputation_pipeline, \n",
    "                                                                          lower=daily_stats_features_lower, \n",
    "                                                                          upper=daily_stats_features_upper)\n",
    "\n",
    "training_df_added_binary_labels.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+--------+\n",
      "| GlucoseDisplayTime|           PatientId|    Value|y_Binary|\n",
      "+-------------------+--------------------+---------+--------+\n",
      "|2022-02-08 16:55:00|8W/rpnb48OMm47W2x...|164.20488|       0|\n",
      "+-------------------+--------------------+---------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=testing_custom_imputation_pipeline, \n",
    "                                                                          lower=daily_stats_features_lower, \n",
    "                                                                          upper=daily_stats_features_upper)\n",
    "\n",
    "testing_df_added_binary_labels.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PySpark: Features: Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+------------------+\n",
      "|           PatientId|Chunk| GlucoseDisplayTime|Value|y_Binary|index|y_summary_binary|              Mean|           Std Dev|Median|  Min|  Max|CountBelow|CountAbove|PercentageBelow|   PercentageAbove|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+------------------+\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-01-31 18:25:00|289.0|       1|   11|               1|310.27272727272725|15.868780098614446| 313.0|289.0|331.0|         0|        11|            0.0|0.9166666666666666|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=training_df_added_binary_labels,\n",
    "                                                                                 daily_stats_features_lower=daily_stats_features_lower,\n",
    "                                                                                 daily_stats_features_upper=daily_stats_features_upper)\n",
    "\n",
    "training_features_summary_stats.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+-----------------+-----------------+---------+---------+-----+----------+----------+---------------+------------------+\n",
      "|           PatientId|Chunk| GlucoseDisplayTime|Value|y_Binary|index|y_summary_binary|             Mean|          Std Dev|   Median|      Min|  Max|CountBelow|CountAbove|PercentageBelow|   PercentageAbove|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+-----------------+-----------------+---------+---------+-----+----------+----------+---------------+------------------+\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-02-08 17:45:00|246.0|       1|   11|               1|209.5662980513139|52.66031632565954|164.20488|164.20488|277.0|         0|         5|            0.0|0.4166666666666667|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+-----------------+-----------------+---------+---------+-----+----------+----------+---------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=testing_df_added_binary_labels,\n",
    "                                                                                 daily_stats_features_lower=daily_stats_features_lower,\n",
    "                                                                                 daily_stats_features_upper=daily_stats_features_upper)\n",
    "\n",
    "testing_features_summary_stats.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PySpark: Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. PySpark: Lag Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/21 06:19:18 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+------------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+------------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "|           PatientId|Chunk| GlucoseDisplayTime|Value|y_Binary|index|y_summary_binary|              Mean|           Std Dev|Median|  Min|  Max|CountBelow|CountAbove|PercentageBelow|   PercentageAbove|value_lag_1|        mean_lag_1|     std_dev_lag_1|med_lag_1|min_lag_1|max_lag_1|cnt_bel_lag_1|cnt_abv_lag_1|perc_belw_lag_1|    perc_abv_lag_1|value_lag_2|        mean_lag_2|     std_dev_lag_2|med_lag_2|min_lag_2|max_lag_2|cnt_bel_lag_2|cnt_abv_lag_2|perc_belw_lag_2|    perc_abv_lag_2|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+------------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+------------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-01-31 17:45:00|329.0|       1|    3|               1|310.27272727272725|15.868780098614446| 313.0|289.0|331.0|         0|        11|            0.0|0.9166666666666666|      331.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|      328.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-01-31 17:50:00|321.0|       1|    4|               1|310.27272727272725|15.868780098614446| 313.0|289.0|331.0|         0|        11|            0.0|0.9166666666666666|      329.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|      331.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-01-31 17:55:00|315.0|       1|    5|               1|310.27272727272725|15.868780098614446| 313.0|289.0|331.0|         0|        11|            0.0|0.9166666666666666|      321.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|      329.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-01-31 18:00:00|313.0|       1|    6|               1|310.27272727272725|15.868780098614446| 313.0|289.0|331.0|         0|        11|            0.0|0.9166666666666666|      315.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|      321.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-01-31 18:05:00|304.0|       1|    7|               1|310.27272727272725|15.868780098614446| 313.0|289.0|331.0|         0|        11|            0.0|0.9166666666666666|      313.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|      315.0|310.27272727272725|15.868780098614446|    313.0|    289.0|    331.0|            0|           11|            0.0|0.9166666666666666|\n",
      "+--------------------+-----+-------------------+-----+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+------------------+-----------+------------------+------------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+------------------+------------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_lag_features_creation=create_lag_features.pyspark_lag_features(df=training_features_summary_stats,\n",
    "                                                                       time_series_lag_values_created=time_series_lag_values_created)\n",
    "training_lag_features_creation.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+---------+--------+-----+----------------+-----------------+-----------------+---------+---------+-----+----------+----------+---------------+------------------+-----------+-----------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+-----------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "|           PatientId|Chunk| GlucoseDisplayTime|    Value|y_Binary|index|y_summary_binary|             Mean|          Std Dev|   Median|      Min|  Max|CountBelow|CountAbove|PercentageBelow|   PercentageAbove|value_lag_1|       mean_lag_1|    std_dev_lag_1|med_lag_1|min_lag_1|max_lag_1|cnt_bel_lag_1|cnt_abv_lag_1|perc_belw_lag_1|    perc_abv_lag_1|value_lag_2|       mean_lag_2|    std_dev_lag_2|med_lag_2|min_lag_2|max_lag_2|cnt_bel_lag_2|cnt_abv_lag_2|perc_belw_lag_2|    perc_abv_lag_2|\n",
      "+--------------------+-----+-------------------+---------+--------+-----+----------------+-----------------+-----------------+---------+---------+-----+----------+----------+---------------+------------------+-----------+-----------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+-----------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-02-08 17:05:00|164.20488|       0|    3|               1|209.5662980513139|52.66031632565954|164.20488|164.20488|277.0|         0|         5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-02-08 17:10:00|164.20488|       0|    4|               1|209.5662980513139|52.66031632565954|164.20488|164.20488|277.0|         0|         5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-02-08 17:15:00|164.20488|       0|    5|               1|209.5662980513139|52.66031632565954|164.20488|164.20488|277.0|         0|         5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-02-08 17:20:00|164.20488|       0|    6|               1|209.5662980513139|52.66031632565954|164.20488|164.20488|277.0|         0|         5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|\n",
      "|8W/rpnb48OMm47W2x...|    0|2022-02-08 17:25:00|    277.0|       1|    7|               1|209.5662980513139|52.66031632565954|164.20488|164.20488|277.0|         0|         5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|  164.20488|209.5662980513139|52.66031632565954|164.20488|164.20488|    277.0|            0|            5|            0.0|0.4166666666666667|\n",
      "+--------------------+-----+-------------------+---------+--------+-----+----------------+-----------------+-----------------+---------+---------+-----+----------+----------+---------------+------------------+-----------+-----------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+-----------+-----------------+-----------------+---------+---------+---------+-------------+-------------+---------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_lag_features_creation=create_lag_features.pyspark_lag_features(df=testing_features_summary_stats,\n",
    "                                                                       time_series_lag_values_created=time_series_lag_values_created)\n",
    "testing_lag_features_creation.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PySpark: Sklearn Categorical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. PySpark: Sklearn Numerical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_numerical_stages=feature_transformations.numerical_scaling(df=training_lag_features_creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. PySpark: XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You enabled use_gpu in spark local mode. Please make sure your local node has at least 2 GPUs\n",
      "[06:19:43] task 1 got new rank 0                                    (0 + 2) / 2]\n",
      "[06:19:43] task 0 got new rank 1\n",
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:19:45] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You enabled use_gpu in spark local mode. Please make sure your local node has at least 2 GPUs\n"
     ]
    }
   ],
   "source": [
    "xgboost_model=create_pyspark_xgboost.xgboost_classifier(ml_df=training_lag_features_creation,\n",
    "                                                        stages=training_numerical_stages,\n",
    "                                                        model_storage_location=model_storage_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. PySpark: Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. PySpark: Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:19:51] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+--------+----------+--------------------+--------------------+\n",
      "|           PatientId| GlucoseDisplayTime|    Value|y_binary|prediction|       rawPrediction|         probability|\n",
      "+--------------------+-------------------+---------+--------+----------+--------------------+--------------------+\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:05:00|164.20488|       0|       0.0|[3.85823082923889...|[0.97933089733123...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:10:00|164.20488|       0|       0.0|[3.85823082923889...|[0.97933089733123...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:15:00|164.20488|       0|       0.0|[3.85823082923889...|[0.97933089733123...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:20:00|164.20488|       0|       0.0|[3.85823082923889...|[0.97933089733123...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:25:00|    277.0|       1|       0.0|[3.85823082923889...|[0.97933089733123...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:30:00|    270.0|       1|       1.0|[-10.903190612792...|[1.83582305908203...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:35:00|    268.0|       1|       1.0|[-7.4308743476867...|[5.92350959777832...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:40:00|    259.0|       1|       1.0|[-7.4308743476867...|[5.92350959777832...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:45:00|    246.0|       1|       1.0|[-7.4308743476867...|[5.92350959777832...|\n",
      "|8W/rpnb48OMm47W2x...|2022-02-08 17:50:00|    232.0|       1|       1.0|[-7.4308743476867...|[5.92350959777832...|\n",
      "+--------------------+-------------------+---------+--------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testing_predictions=model_predictions.create_predictions_with_model(test_df=testing_lag_features_creation, \n",
    "                                                                    model=xgboost_model)\n",
    "testing_predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. PySpark: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[06:20:48] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "[06:20:49] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "[06:20:50] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "[06:20:51] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "[06:20:52] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "testing_predictions=testing_predictions \\\n",
    "    .withColumn(\"y_binary\", testing_predictions[\"y_binary\"].cast(\"double\")) \\\n",
    "    .withColumn(\"prediction\", testing_predictions[\"prediction\"].cast(\"double\"))\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "eval_accuracy = MulticlassClassificationEvaluator(labelCol=\"y_binary\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "eval_precision = MulticlassClassificationEvaluator(labelCol=\"y_binary\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "eval_recall = MulticlassClassificationEvaluator(labelCol=\"y_binary\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "eval_f1 = MulticlassClassificationEvaluator(labelCol=\"y_binary\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "\n",
    "eval_auc = BinaryClassificationEvaluator(labelCol=\"y_binary\", rawPredictionCol=\"prediction\")\n",
    "\n",
    "accuracy = eval_accuracy.evaluate(testing_predictions)\n",
    "precision = eval_precision.evaluate(testing_predictions)\n",
    "recall = eval_recall.evaluate(testing_predictions)\n",
    "f1score = eval_f1.evaluate(testing_predictions)\n",
    "\n",
    "auc = eval_accuracy.evaluate(testing_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9760138050043141\n",
      "0.9786324786324786\n",
      "0.9817090597313518\n",
      "0.9760038290057009\n",
      "0.9760138050043141\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1score)\n",
    "\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_and_labels = testing_predictions.select(['prediction_label','y_binary']).withColumn('label', F.col('y_binary').cast(FloatType())).orderBy('prediction_label')\n",
    "\n",
    "#select only prediction and label columns\n",
    "preds_and_labels = preds_and_labels.select(['prediction_label','label'])\n",
    "\n",
    "metrics = MulticlassMetrics(preds_and_labels.rdd.map(tuple))\n",
    "\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_model.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(zip(xgboost_model.getInputCols()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_classification_plot.read_model_plot_variance(model_storage_location=model_storage_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_df=reading_data.read_in_pandas()\n",
    "pandas_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pat=pandas_df[pandas_df['PatientId']=='tHu8WPnIffml5CL+AbOBkXcbFApQnP06KdrHbjinta4=']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_custom_imputation_pipeline=pandas_sklearn_pipeline.pandas_custom_imputation_pipeline(df=pandas_df)\n",
    "pandas_custom_imputation_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_imputation_pipeline=pandas_sklearn_pipeline.pandas_custom_imputation_pipeline(df=pandas_df)\n",
    "pandas_custom_imputation_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Aggregate Data at Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_added_binary_labels=create_binary_labels.pandas_binary_labels(df=pandas_custom_imputation_pipeline, \n",
    "                                                                        lower=daily_stats_features_lower, \n",
    "                                                                        upper=daily_stats_features_upper)\n",
    "pandas_df_added_binary_labels.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Features: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_features_summary_stats=summary_stats_features.pandas_compressDailyValues(data=pandas_df_added_binary_labels, \n",
    "                                                                                lower=daily_stats_features_lower, \n",
    "                                                                                upper=daily_stats_features_upper)\n",
    "pandas_features_summary_stats.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Wrapper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Sklearn Categorical Pipeline in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df=pandas_features_summary_stats[['PatientId', 'Value', 'GlucoseDisplayTime', 'GlucoseDisplayDate', 'inserted', \n",
    "        'missing', 'y_Binary', 'Median', 'Mean', 'Std Dev', 'Max', 'Min', 'AreaBelow', 'AreaAbove']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_id in df['PatientId'].unique():\n",
    "    # Categorical Features\n",
    "    categorical_features=['inserted', 'missing']\n",
    "    categorical_transformer=Pipeline([('imputer_cat', SimpleImputer(strategy='constant', fill_value=np.nan)),\n",
    "                                        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocessor_2=ColumnTransformer([('categorical', categorical_transformer, categorical_features)],\n",
    "                                    remainder = 'passthrough')\n",
    "\n",
    "    cat_pipe_pipeline=Pipeline([('preprocessing_2', preprocessor_2)])\n",
    "\n",
    "    transformed_data1=cat_pipe_pipeline.fit_transform(df)\n",
    "\n",
    "    transformed_data_df=pd.DataFrame(transformed_data1)\n",
    "\n",
    "    transformed_data_df['combine_inserted']=transformed_data_df[[0,1]].values.tolist()\n",
    "    transformed_data_df['combine_missing']=transformed_data_df[[2,3]].values.tolist()\n",
    "    transformed_data_df=transformed_data_df.drop(transformed_data_df.iloc[:, 0:4],axis = 1)\n",
    "\n",
    "    transformed_data_df.columns=['PatientId', 'Value', 'GlucoseDisplayTime', 'GlucoseDisplayDate', \n",
    "                                    'y_Binary', 'Median', 'Mean', 'Std Dev', 'Max', 'Min', 'AreaBelow', \n",
    "                                    'AreaAbove', 'inserted', 'missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_categorical_pipeline=pandas_sklearn_pipeline.pandas_transform_categorical_features(df=pandas_features_summary_stats)\n",
    "pandas_custom_categorical_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Sklearn Numerical Pipeline in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Pandas\n",
    "pandas_custom_numerical_pipeline=pandas_sklearn_pipeline.pandas_transform_numerical_features(df=pandas_custom_categorical_pipeline)\n",
    "pandas_custom_numerical_pipeline.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas: Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e282f9bbb605e17ec33c138684786f2c6a0e45a19b318784bb5671e7a7934052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
