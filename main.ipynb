{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "chmod 600 ~/.ssh/id_ed25519### Current Process\n",
    "1. Read in data --> Done\n",
    "\n",
    "2. Custom Imputation --> Done\n",
    "\n",
    "3. Add Binary Class --> Done, Should Add Binary Class Later\n",
    "\n",
    "4. Summary Statistics Features --> Done\n",
    "\n",
    "5. Wrapper Functions --> Done, Need to Test Though\n",
    "\n",
    "6. Sklearn Pipeline Categorical Features --> One Hot Encoding Done\n",
    "\n",
    "7. Sklearn Pipeline Numerical Features --> StandardScaler Done\n",
    "\n",
    "8. Create Lagged Features --> Done\n",
    "\n",
    "9. Modeling --> Currently XgBoost, (Maybe Try: TensorFlow Decision Tree, TensorFlow Probability Model)\n",
    "\n",
    "10. Model Evaluation --> Accuracy, Precision, Recall, F1, Confusion Matrix (Need to add Variable Importance Based on Variance)\n",
    "\n",
    "11. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Need to Run These in Notebook Version For Pandas UDF\n",
    "! pip install pyarrow\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install pyspark\n",
    "! pip install xgboost\n",
    "! pip install kaleido\n",
    "! pip install EntropyHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Input_Variables.read_vars import train_data_storage, validation_data_storage, test_data_storage, \\\n",
    "                                      inter_train_location, inter_test_location, inter_val_location,\\\n",
    "                                      one_hot_encoding_data, \\\n",
    "                                      analysis_group, \\\n",
    "                                      daily_stats_features_lower, daily_stats_features_upper, \\\n",
    "                                      model_storage_location, random_seed, \\\n",
    "                                      time_series_lag_values_created, \\\n",
    "                                      evaluation_metrics_output_storage, \\\n",
    "                                      feature_importance_storage_location, \\\n",
    "                                      overall_feature_importance_plot_location\n",
    "\n",
    "from Data_Schema.schema import Pandas_UDF_Data_Schema\n",
    "from Read_In_Data.read_data import Reading_Data\n",
    "from Data_Pipeline.imputation_pipeline import Date_And_Value_Imputation\n",
    "\n",
    "\n",
    "from Feature_Generation.create_binary_labels import Create_Binary_Labels\n",
    "from Feature_Generation.summary_stats import Summary_Stats_Features\n",
    "from Feature_Generation.lag_features import Create_Lagged_Features\n",
    "from Feature_Generation.time_series_feature_creation import TS_Features\n",
    "from Feature_Generation.difference_features import Difference_Features\n",
    "\n",
    "from Data_Pipeline.encoding_scaling_pipeline import Feature_Transformations\n",
    "\n",
    "from Model_Creation.pyspark_xgboost import Create_PySpark_XGBoost\n",
    "\n",
    "from Model_Predictions.pyspark_model_preds import Model_Predictions\n",
    "\n",
    "from Model_Evaluation.pyspark_model_eval import Evaluate_Model\n",
    "\n",
    "from Feature_Importance.model_feature_importance import Feature_Importance\n",
    "\n",
    "from Model_Plots.xgboost_classification_plots import XGBoost_Classification_Plot\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 06:19:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# PySpark UDF Schema Activation\n",
    "pandas_udf_data_schema=Pandas_UDF_Data_Schema()\n",
    "\n",
    "# Data Location\n",
    "reading_data=Reading_Data()\n",
    "\n",
    "# Create Binary y Variables\n",
    "create_binary_labels=Create_Binary_Labels()\n",
    "\n",
    "# Imputation\n",
    "date_and_value_imputation=Date_And_Value_Imputation()\n",
    "\n",
    "# Features Daily Stats Module\n",
    "summary_stats_features=Summary_Stats_Features()\n",
    "\n",
    "# Features Complex\n",
    "ts_features=TS_Features()\n",
    "\n",
    "# Features Lagged Value\n",
    "create_lag_features=Create_Lagged_Features()\n",
    "\n",
    "# Features Differences\n",
    "difference_features=Difference_Features()\n",
    "\n",
    "# PySpark XGBoost Model Module\n",
    "create_pyspark_xgboost=Create_PySpark_XGBoost()\n",
    "\n",
    "# Classification Evaluation\n",
    "evaluate_model=Evaluate_Model()\n",
    "\n",
    "# Model Plots Feature Importance\n",
    "xgboost_classification_plot=XGBoost_Classification_Plot()\n",
    "\n",
    "# Feature Transformations\n",
    "feature_transformations=Feature_Transformations()\n",
    "\n",
    "\n",
    "pyspark_custom_imputation_schema=pandas_udf_data_schema.custom_imputation_pyspark_schema()\n",
    "\n",
    "\n",
    "model_predictions=Model_Predictions()\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance=Feature_Importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. PySpark: Reading In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======================================================> (28 + 1) / 29]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "|NumId|           PatientId|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|\n",
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "| 6563|++3L3PAkDvSTkWnWe...|272.0|2022-02-16 16:10:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|273.0|2022-02-16 16:15:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|270.0|2022-02-16 16:20:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|268.0|2022-02-16 16:25:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|264.0|2022-02-16 16:30:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|262.0|2022-02-16 16:35:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|261.0|2022-02-16 16:40:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|259.0|2022-02-16 16:45:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|253.0|2022-02-16 16:50:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|249.0|2022-02-16 16:55:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|243.0|2022-02-16 17:00:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|237.0|2022-02-16 17:05:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|237.0|2022-02-16 17:10:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|232.0|2022-02-16 17:15:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|231.0|2022-02-16 17:20:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|233.0|2022-02-16 17:25:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|233.0|2022-02-16 17:30:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|244.0|2022-02-16 17:35:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|246.0|2022-02-16 17:40:00|                 null|              null|\n",
      "| 6563|++3L3PAkDvSTkWnWe...|260.0|2022-02-16 17:45:00|                 null|              null|\n",
      "+-----+--------------------+-----+-------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df=reading_data.read_in_pyspark_data(data_location=train_data_storage)\n",
    "training_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_df=reading_data.read_in_pyspark_data(data_location=validation_data_storage)\n",
    "validation_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_df=reading_data.read_in_pyspark_data(data_location=test_data_storage)\n",
    "testing_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. PySpark: Custom Imputation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======================================================> (75 + 2) / 77]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+----------+\n",
      "| GlucoseDisplayTime|NumId|Value|IsFilledIn|\n",
      "+-------------------+-----+-----+----------+\n",
      "|2022-01-31 18:42:00|    0|153.0|       0.0|\n",
      "|2022-01-31 18:47:00|    0|156.0|       0.0|\n",
      "+-------------------+-----+-----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "interpolation_complete = os.path.exists('/cephfs/interpolation/train')\n",
    "\n",
    "if interpolation_complete == False:\n",
    "    date_and_value_imputation.interpolation_creation('train')\n",
    "    \n",
    "training_custom_imputation_pipeline = date_and_value_imputation.read_interpolation('/cephfs/interpolation/train/')\n",
    "\n",
    "training_custom_imputation_pipeline.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interpolation_complete = os.path.exists('/cephfs/interpolation/test')\n",
    "\n",
    "if interpolation_complete == False:\n",
    "    date_and_value_imputation.interpolation_creation('test')\n",
    "    \n",
    "testing_custom_imputation_pipeline = date_and_value_imputation.read_interpolation('/cephfs/interpolation/test/')\n",
    "\n",
    "testing_custom_imputation_pipeline.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. PySpark: Adding Binary Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+----------+--------+--------+--------+\n",
      "| GlucoseDisplayTime|NumId|Value|IsFilledIn|y_Binary|is_above|is_below|\n",
      "+-------------------+-----+-----+----------+--------+--------+--------+\n",
      "|2022-01-31 15:19:00|   26|179.0|       0.0|       0|       0|       0|\n",
      "+-------------------+-----+-----+----------+--------+--------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=training_custom_imputation_pipeline)\n",
    "\n",
    "training_df_added_binary_labels.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_df_added_binary_labels=create_binary_labels.pyspark_binary_labels(df=testing_custom_imputation_pipeline)\n",
    "\n",
    "testing_df_added_binary_labels.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. PySpark: Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+----------+---------+-------+\n",
      "| GlucoseDisplayTime|NumId|Value|IsFilledIn|FirstDiff|SecDiff|\n",
      "+-------------------+-----+-----+----------+---------+-------+\n",
      "|2022-01-31 15:19:00|   26|179.0|       0.0|      0.0|    0.0|\n",
      "|2022-01-31 15:24:00|   26|140.0|       1.0|    -39.0|  -39.0|\n",
      "|2022-01-31 15:25:00|   26|177.0|       0.0|     37.0|   76.0|\n",
      "|2022-01-31 15:29:00|   26|177.0|       0.0|      0.0|  -37.0|\n",
      "|2022-01-31 15:34:00|   26|180.0|       0.0|      3.0|    3.0|\n",
      "+-------------------+-----+-----+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df_differences = difference_features.add_difference_features(training_custom_imputation_pipeline)\n",
    "training_df_differences.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+----------+---------+-------+-----+-----+\n",
      "| GlucoseDisplayTime|NumId|Value|IsFilledIn|FirstDiff|SecDiff|index|Chunk|\n",
      "+-------------------+-----+-----+----------+---------+-------+-----+-----+\n",
      "|2022-01-31 15:19:00|   26|179.0|       0.0|      0.0|    0.0|    1|    0|\n",
      "|2022-01-31 15:24:00|   26|140.0|       1.0|    -39.0|  -39.0|    2|    0|\n",
      "|2022-01-31 15:25:00|   26|177.0|       0.0|     37.0|   76.0|    3|    0|\n",
      "|2022-01-31 15:29:00|   26|177.0|       0.0|      0.0|  -37.0|    4|    0|\n",
      "|2022-01-31 15:34:00|   26|180.0|       0.0|      3.0|    3.0|    5|    0|\n",
      "+-------------------+-----+-----+----------+---------+-------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "training_df_chunks = summary_stats_features.create_chunk_col(training_df_differences, chunk_val = 288)\n",
    "training_df_chunks.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------------+----------------+-------------+\n",
      "|NumId|Chunk|ShortTermVariance|LongTermVariance|VarianceRatio|\n",
      "+-----+-----+-----------------+----------------+-------------+\n",
      "|   26|    0|        31.438166|       4.5052047|     6.978188|\n",
      "|   26|    1|        31.703512|       1.4018252|    22.615881|\n",
      "|   26|    2|        42.562332|        1.128366|    37.720325|\n",
      "|   26|    3|        113.79796|       5.9704657|    19.060148|\n",
      "|   26|    4|        56.827408|       1.9084752|     29.77634|\n",
      "+-----+-----+-----------------+----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+--------+--------+--------+\n",
      "|NumId|Chunk|    Entropy|Entropy2|Entropy3|Entropy4|\n",
      "+-----+-----+-----------+--------+--------+--------+\n",
      "|   26|    0|  0.3791744|    null|    null|    null|\n",
      "|   26|    1|  0.1712082|    null|    null|    null|\n",
      "|   26|    2| 0.24108508|    null|    null|    null|\n",
      "|   26|    3|0.059399597|    null|    null|    null|\n",
      "|   26|    4| 0.06988227|    null|    null|    null|\n",
      "+-----+-----+-----------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# training_df_poincare = training_df_chunks.groupby(['NumId', 'Chunk']).apply(ts_features.poincare)\n",
    "# training_df_poincare.show(5)\n",
    "\n",
    "# training_df_entropy = training_df_chunks.groupby(['NumId', 'Chunk']).apply(ts_features.entropy)\n",
    "# training_df_entropy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "[Stage 44:>               (6 + 6) / 192][Stage 45:>               (0 + 0) / 192]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2796.126s][warning][gc,alloc] Executor task launch worker for task 11.0 in stage 44.0 (TID 2247): Retried waiting for GCLocker too often allocating 524288 words\n",
      "23/05/18 07:06:27 WARN TaskMemoryManager: Failed to allocate a page (4194288 bytes), try again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:269: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:226: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/opt/conda/lib/python3.10/site-packages/numpy/core/_methods.py:261: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n",
      "[Stage 44:=>             (17 + 6) / 192][Stage 45:>               (0 + 0) / 192]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 07:08:18 ERROR Executor: Exception in task 21.0 in stage 44.0 (TID 2257)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n",
      "\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:165)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:132)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:137)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:142)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:758)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/18 07:08:18 ERROR Executor: Exception in task 20.0 in stage 44.0 (TID 2256)\n",
      "org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n",
      "\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:165)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:132)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:137)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:142)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:758)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/05/18 07:08:18 WARN TaskSetManager: Lost task 21.0 in stage 44.0 (TID 2257) (jupyter-kkanjaria-40ucsd-2eedu executor driver): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n",
      "\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:165)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:132)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:137)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:142)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n",
      "\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:758)\n",
      "\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "23/05/18 07:08:18 ERROR TaskSetManager: Task 21 in stage 44.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=>             (17 + 7) / 192][Stage 45:>               (0 + 0) / 192]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 07:08:19 WARN TaskSetManager: Lost task 24.0 in stage 44.0 (TID 2260) (jupyter-kkanjaria-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/18 07:08:19 WARN TaskSetManager: Lost task 23.0 in stage 44.0 (TID 2259) (jupyter-kkanjaria-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/18 07:08:19 WARN TaskSetManager: Lost task 22.0 in stage 44.0 (TID 2258) (jupyter-kkanjaria-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/18 07:08:19 WARN TaskSetManager: Lost task 0.0 in stage 45.0 (TID 2261) (jupyter-kkanjaria-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=====>                                                 (18 + 2) / 192]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 07:08:19 WARN TaskSetManager: Lost task 18.0 in stage 44.0 (TID 2254) (jupyter-kkanjaria-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/20/temp_shuffle_53da192e-5457-45b4-ad99-addf412d07c2\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2f/temp_shuffle_b7e841c1-87cd-478b-a0b4-86fa441f331b\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2e/temp_shuffle_0384e3c0-f664-439b-ba07-9bd4b4cbba28\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/07/temp_shuffle_aba1a57c-db10-47db-8b38-66fdb820eeca\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1c/temp_shuffle_2ab6a0a1-3908-4c24-9527-c43d41340708\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/11/temp_shuffle_f963afd9-5b4f-4377-946c-2d9b6832af63\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/35/temp_shuffle_4daf2d0c-d0de-4954-9f53-a562be1c44d3\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0c/temp_shuffle_c32f7e10-5398-4122-990b-9d8969d28df9\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/03/temp_shuffle_76fb3030-2297-412a-9453-47b9fba1d2d8\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/39/temp_shuffle_4c4ab1f5-c61a-4ff4-94b4-988acc80c3d2\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/38/temp_shuffle_d3cfbfba-ab18-4b5b-a00b-3862d44bb8f1\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/25/temp_shuffle_8aa8740a-b4a3-43a3-aa5f-3f10cec409a3\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1f/temp_shuffle_992923d3-6c69-44cd-ba57-8152ea47002d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2e/temp_shuffle_d81315a8-0b19-4f0e-83dd-2246845e8d04\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_4d378b14-a276-43aa-8986-7f3e54b35cdd\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/14/temp_shuffle_cef7fa84-1295-4827-962a-db63707fd310\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/38/temp_shuffle_e9cb0422-f78b-44a5-bd7d-9fafa5f44bb6\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/19/temp_shuffle_754178fd-5ab1-4649-a70b-da55ce8a2840\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/04/temp_shuffle_2f8b2da7-8f22-44ed-aad6-86188413e561\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1d/temp_shuffle_e7c5fe22-27af-4bf0-98e5-c0352f61f167\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_eb9c8069-67e9-4ce4-b9f8-2282be3a7fa5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/35/temp_shuffle_89ee9d9e-420e-49cd-bd5b-4cfe1d5abdf5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2b/temp_shuffle_6686f0d9-c851-4ca0-9068-b44109170c91\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1d/temp_shuffle_277b96b4-971c-4e74-ae49-9671da2ef17c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/36/temp_shuffle_59be5eb5-a1ae-4398-b4ff-b3a312d8b2a0\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1e/temp_shuffle_6f7d8316-9d4a-418e-b589-5de5822d505a\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2b/temp_shuffle_85aca231-21bf-4c1e-8baa-935f92f8f3eb\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2a/temp_shuffle_eaa095e3-1ffd-4c69-876b-2a0808028982\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/28/temp_shuffle_9effbe10-7471-4021-a312-5d791bbb49c4\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/21/temp_shuffle_366892a9-bf9a-4c39-962e-7e722fadb03f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/21/temp_shuffle_e6462433-29dc-462e-98ae-3e3032f7463a\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/39/temp_shuffle_8abe9969-546a-4ba6-999c-6ac058b4e286\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/16/temp_shuffle_80958ba2-7cb1-4929-89d6-1bdfe9fd629d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/05/temp_shuffle_0d7691d5-de73-49f6-9717-a04577f53d2f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/30/temp_shuffle_8e2bbfe1-3016-4758-8335-89614f886652\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/08/temp_shuffle_1c029140-2ec2-4e7c-8522-6021fc22aea8\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/21/temp_shuffle_345d59f3-54b9-48b0-a357-d31cfb76802e\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/28/temp_shuffle_24569046-3b96-4b97-90da-9a82e0d724fc\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/02/temp_shuffle_290398e1-bcbe-4c2e-ab32-5f6b3dbf1647\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_1a5d797b-83e0-4d36-9a76-ad3706ba4e21\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/06/temp_shuffle_8ef0580e-48dd-4220-848f-86a059515d51\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/39/temp_shuffle_2f210bcc-31c2-4c33-9407-eec7e33b5e02\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/09/temp_shuffle_42bdfb1f-8657-4b93-b946-93e0f1c700e9\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/04/temp_shuffle_33671637-a19a-4fa6-96aa-0b634cc302ed\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/15/temp_shuffle_e2024933-04cc-4b98-a1cf-0eeb7024e64f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3f/temp_shuffle_6586b415-5b2c-475e-b316-52df98ba1440\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/18/temp_shuffle_0e32d587-a2ea-4d66-9e4a-0f43dd8948f5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/18/temp_shuffle_639d5533-1484-4af4-8309-ad1ed522752c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/18/temp_shuffle_5c0742a5-8c86-4be7-ae7e-baf32d7497c5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_886d91bc-f221-4909-b3ac-dca0120d9384\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3b/temp_shuffle_5be9a402-0797-43a4-b0c7-4fa577554b21\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3b/temp_shuffle_33c0c4a8-123a-41d5-83a9-f98562a57f26\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0d/temp_shuffle_95059689-0455-4ca9-94bd-84c529445238\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/23/temp_shuffle_759b4440-8736-4966-b6ea-53bded602049\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/38/temp_shuffle_6ade5351-11a5-4826-aa6d-84da7c311416\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/21/temp_shuffle_dc89deed-a9d3-4e3e-9913-726daab15166\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/33/temp_shuffle_966d59cf-4d9f-49c6-a2b4-2d3da1b149b3\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/39/temp_shuffle_4df3a87b-02a4-42e2-b679-0b8f20fd5d75\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1c/temp_shuffle_a8a4ade4-19c5-40bb-8e6d-1c2e1c12e904\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3f/temp_shuffle_24273a30-41e1-4fec-a011-66a57c91c3ea\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3b/temp_shuffle_886265cc-fc8f-44ba-805d-e9b04a85b426\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/07/temp_shuffle_90c5a8b9-e607-4983-a0f2-90b8ada43202\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/13/temp_shuffle_02c39aad-6dbd-4e84-a5b3-d3d55c32b507\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3b/temp_shuffle_ff4b6eae-8ca3-4a03-a583-60a33a7d0262\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/32/temp_shuffle_850e7c70-2aa1-4a2e-bfff-eacdbb5c6f5c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3d/temp_shuffle_7f941b21-5eb2-4b77-ad5f-52cd83b2b034\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2f/temp_shuffle_f75881cd-e847-4bc2-8d53-c5d971660a56\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/14/temp_shuffle_6913d169-8c15-4eec-85b1-748f8599ce4d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/11/temp_shuffle_3772b1ed-cdfe-47a7-a7c4-8a33343bd73f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1b/temp_shuffle_979c96fa-ae05-41dd-86d6-8b4eed18964f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0e/temp_shuffle_c4816c93-b56e-4270-ba7c-a51b5118b58c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/39/temp_shuffle_49960a3a-3d12-48ee-8e87-4f1ab0a6ed74\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/32/temp_shuffle_a1c22e81-86b1-4438-8fc1-74662fa3d616\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/32/temp_shuffle_8303404a-5379-457f-a7b3-445843add187\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2f/temp_shuffle_99ec72e4-801b-4b9e-b5ac-6ba9d9378467\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0e/temp_shuffle_b719b3f9-776c-4d87-bb86-05919ec2bf33\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1d/temp_shuffle_8d498051-6ac2-4f7c-8230-1cfe44c72e79\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/05/temp_shuffle_86759fc1-4e44-4466-bb2a-d4496f67a3fa\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2f/temp_shuffle_a71e7397-742e-4dd2-ba31-49ced813b275\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/19/temp_shuffle_015f40e8-0e2a-46e9-a2fd-d51a6b3947f4\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/16/temp_shuffle_ce7869dd-0980-4223-8315-367a7df36131\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_72091ce1-cede-4a6f-ba70-8d5e57dbda0a\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/20/temp_shuffle_022e84f2-9c4b-45e2-a06f-72cd625fe3ae\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1b/temp_shuffle_4f51910f-cfe4-4c9f-a3a7-f4eb3f9236a3\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0c/temp_shuffle_fdea2206-39f8-4d7d-bde9-8e8776d17b5f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/32/temp_shuffle_c6071d95-fdc0-48d5-972c-59af628224a9\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1a/temp_shuffle_953eab66-9f2c-4f35-b72e-ee470139a9a7\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/02/temp_shuffle_bb443180-25a9-4ace-9c85-7d54e2a6f22b\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2b/temp_shuffle_6b14c338-baab-4ce4-a721-a7eb8b16537d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/37/temp_shuffle_18ddac5f-9bca-4a21-b090-122e527accad\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2e/temp_shuffle_a3a9ab3c-b729-47e5-9df9-f081cf783768\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/13/temp_shuffle_cc56b956-b9d9-4b19-ad0d-95638744c0f7\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0f/temp_shuffle_4ca47440-9352-4f95-b488-23b75e53c937\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/20/temp_shuffle_d10cc573-76f1-47fd-a0f0-553be3d04471\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3a/temp_shuffle_a0fa9355-94a3-4f00-b544-f9ee6fa78d4b\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/09/temp_shuffle_ea5ba783-f8c0-40ee-9383-f4673bbb1d38\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/28/temp_shuffle_b8fe0695-5098-4db8-98e8-7c1d5b1c2f59\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2a/temp_shuffle_83d05256-e473-45a1-a673-6092b060534f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3c/temp_shuffle_928f619d-a686-4f13-8c95-de4cbcd3e474\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2a/temp_shuffle_55c143b9-79f0-48dc-9c45-62c0cc44602d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/04/temp_shuffle_581d469b-ca24-44f5-9e17-73854a79d683\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/04/temp_shuffle_f3c9e8e5-8f07-4df0-9afd-5fcc4857aaa8\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3d/temp_shuffle_9e0c4c4d-fd14-4890-a7be-a7a6c4406320\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1d/temp_shuffle_93e47b70-e2b7-44ba-b543-b03aba96c07f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1b/temp_shuffle_b09548a7-5b7e-4662-8bb4-58bdb6050979\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/07/temp_shuffle_8dac7218-df7e-4078-a146-f10c2ea371ff\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/37/temp_shuffle_a1947879-0547-44ce-9fc6-97dea65310fe\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/09/temp_shuffle_ae2a33a3-acca-46f5-8001-613aa5df7e7e\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2f/temp_shuffle_eddb45e9-c7d5-446d-a775-3cd36ee0bc43\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2d/temp_shuffle_08bf14e1-d8e4-4e35-a4ed-692ac7f759c1\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_b048ba92-21b4-4603-ba82-a39f41808a6b\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/34/temp_shuffle_ec979116-a2f5-4d52-b91a-04988a29ff8a\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/35/temp_shuffle_10603e25-896b-4f6e-8d55-dac037e1724c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/21/temp_shuffle_5f674e4a-5391-4a75-80fd-60dddf49a11a\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1a/temp_shuffle_b78c338f-1191-4b48-afa0-101033f9127f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/23/temp_shuffle_9b9d3394-398e-4fba-885f-28a3784b4d21\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/06/temp_shuffle_ded27731-b6d6-431a-8429-93b24f9dfa88\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0f/temp_shuffle_a6371ba2-89be-4e3b-acb2-825e9789fcb7\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2c/temp_shuffle_be4e7b0b-80f2-49ee-ba65-5b04a3571638\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/33/temp_shuffle_615c1f9f-1eeb-4a9b-99c0-2ab424c1467c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0d/temp_shuffle_47cda657-6b71-4b3c-9ddb-f673b5c1de8b\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1f/temp_shuffle_30340bef-f930-446c-92c2-f19908df4dad\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/14/temp_shuffle_56aa2838-5018-4ed9-9872-572390f29ff8\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2a/temp_shuffle_b668198e-a459-421e-9bf9-cbbc0a2d60e7\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3c/temp_shuffle_3fb74b3c-0448-445a-95e5-7d1664a81829\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3f/temp_shuffle_75d07814-f7c1-49b2-9b8d-d7041568bcab\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3d/temp_shuffle_710091e6-765e-49d7-bcc5-0a93cb9d2358\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3e/temp_shuffle_8da28673-ab31-49a1-a486-399228ed532e\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/18/temp_shuffle_b1fbd0e3-4554-4c06-85f9-40e73505a30f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/11/temp_shuffle_4037e1e7-d463-4270-86de-394a3ff72990\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/36/temp_shuffle_c0fe7fbf-552d-4661-8bc4-0f3a0ce26d59\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/00/temp_shuffle_48537fd4-4e25-45c6-b05b-6cbaac524ef3\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/37/temp_shuffle_efe3cfc2-b2e0-452a-861e-6d576760998a\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0c/temp_shuffle_734d5b0d-d095-4ffd-98a7-60bdd18ce8a5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1d/temp_shuffle_4ebdc873-d525-4f16-bf37-f998cc855603\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3b/temp_shuffle_49aeb434-53b0-4893-bcdc-78228b42b1bf\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/34/temp_shuffle_f59fa660-de85-48c7-921f-fe8f09bd396d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3d/temp_shuffle_2a6989ec-b9b7-4af9-93fe-0727547fbe47\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/11/temp_shuffle_52c70841-7cd1-4bfa-9f69-01caf6e42c83\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/10/temp_shuffle_627b2824-6c2b-4a6f-b5e4-e1ca1c8140ab\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3b/temp_shuffle_97bcdcfd-8684-43ea-a865-c85aad2749dc\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_10a44f01-3d70-452e-9822-cc2bb1e5e699\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2d/temp_shuffle_835b4b0c-f776-4fc6-ac96-39612ba38a4e\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1a/temp_shuffle_3635586e-cb82-4fca-8498-57a0141ac434\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/16/temp_shuffle_89706509-ae17-4f75-8d27-ff5f764354aa\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/17/temp_shuffle_fcee827d-be6e-470c-a4e5-91c13612170a\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3d/temp_shuffle_49b7c681-408f-4c27-a8da-bf2df847e73f\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/15/temp_shuffle_83311541-9590-4d21-86e3-bca8e9ec54e6\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_b4625264-872c-46bc-87c9-6be3bd4c6ad5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1c/temp_shuffle_12eedcb3-d7af-4601-b129-7eebe77e6d6b\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3f/temp_shuffle_eb1afac4-373f-4f82-abc4-0207ea5a27ca\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/32/temp_shuffle_629b18de-8b45-4872-8ad4-c7456e716096\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/08/temp_shuffle_a1ba7594-fafe-4481-a6ae-c8f0f6bf9b30\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/39/temp_shuffle_5fbdbb81-f6f8-44ca-ba5a-12664cd72e03\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/23/temp_shuffle_fbefd542-4dfc-480d-9c74-f645aeff3c85\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/16/temp_shuffle_a1be7112-1bd5-4003-9840-b4b8a2e14921\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/09/temp_shuffle_21cdb421-ea5f-42bc-8571-29fab55491bb\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/25/temp_shuffle_1b8d3654-0399-438a-bb69-cccbb2e7a092\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/13/temp_shuffle_d2f0e5fd-771c-41be-835b-deec91e133e8\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2c/temp_shuffle_bbd53563-d9b0-4329-9af1-57d370d1bfbb\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1f/temp_shuffle_8590b211-2203-4b4d-8765-3a1fbe24c72c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/17/temp_shuffle_e64679cb-da12-459f-b2a7-513bf45e35da\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/37/temp_shuffle_c6e454d1-c6a2-4bbe-9b0f-d0cca46b9eb5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/33/temp_shuffle_3e5a3db2-1a7c-4e60-8254-3dcc04d69db7\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0e/temp_shuffle_aac10e00-fc14-4607-812e-f322851e7ed5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/37/temp_shuffle_19f62896-ae00-4bc2-8e51-3dff4bba5ee2\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/36/temp_shuffle_704b3e19-81aa-42f8-93d2-9c17e7644d16\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1b/temp_shuffle_b36ae85f-fa64-4640-999e-56d10ee098ae\n",
      "23/05/18 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/Feature_Generation/time_series_feature_creation.py:55: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ratio = short_term_variation / long_term_variation\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o211.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 21 in stage 44.0 failed 1 times, most recent failure: Lost task 21.0 in stage 44.0 (TID 2257) (jupyter-kkanjaria-40ucsd-2eedu executor driver): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:165)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:132)\n\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:137)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:142)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:758)\n\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:165)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:132)\n\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:137)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:142)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:758)\n\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m training_df_complex_features \u001b[38;5;241m=\u001b[39m training_df_poincare\u001b[38;5;241m.\u001b[39mjoin(training_df_entropy,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChunk\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtraining_df_complex_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o211.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 21 in stage 44.0 failed 1 times, most recent failure: Lost task 21.0 in stage 44.0 (TID 2257) (jupyter-kkanjaria-40ucsd-2eedu executor driver): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:165)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:132)\n\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:137)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:142)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:758)\n\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:158)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:165)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:132)\n\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:137)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:142)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:174)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:102)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator.isEmpty(Iterator.scala:387)\n\tat scala.collection.Iterator.isEmpty$(Iterator.scala:387)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.isEmpty(WholeStageCodegenExec.scala:758)\n\tat org.apache.spark.sql.execution.python.FlatMapGroupsInPandasExec.$anonfun$doExecute$1(FlatMapGroupsInPandasExec.scala:81)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0b/temp_shuffle_94c4a5e0-1cf3-4772-91c0-934432f06860\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1d/temp_shuffle_218f9d2a-2cec-4d0e-8fbe-44a66607b8e9\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/37/temp_shuffle_758e6fdb-d640-491a-89b3-a425816d1040\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/02/temp_shuffle_835b1425-4625-484e-8108-1f2fb5518d4d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/24/temp_shuffle_743a9bb8-a32e-4ad7-bbeb-fd7d2d7b5176\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2f/temp_shuffle_981ea4ef-e174-4ef6-8178-57f8eebd67ae\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3d/temp_shuffle_b9703557-515b-4304-85f0-d62b42c9b496\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1d/temp_shuffle_6929117d-611c-44f0-9ec9-f58634d37857\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/09/temp_shuffle_841f3b69-554e-4ed3-a3f6-203a33627f1b\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2a/temp_shuffle_a6760b2f-9c60-4b14-949e-e66d60779dde\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3a/temp_shuffle_b2a40922-39ca-4dc0-9150-4da48429d599\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2c/temp_shuffle_00882ba6-4c0b-448f-8a34-ce7c63e21367\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3b/temp_shuffle_6088a7e8-5385-476c-80ac-e97483ba3422\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/21/temp_shuffle_71a5c306-f7e1-421e-9665-f0dea7fb593e\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/12/temp_shuffle_01ab45f1-90e9-4780-94b5-d7e009c2d528\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/01/temp_shuffle_df00d755-cbb6-46e0-8fd3-59117260a20d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1e/temp_shuffle_a47f1e47-3a98-4b8a-a95d-7dc48c71ab2c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/26/temp_shuffle_4ea9bd0e-9b59-43de-b806-104a2d4763aa\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/20/temp_shuffle_956a5978-ecd0-4950-9c3a-c1086c49dabb\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/2d/temp_shuffle_ec088d0c-f1d8-4318-a972-2c4879b8272c\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/13/temp_shuffle_0bae90f7-6dfd-4eb1-93c4-11dfbffed714\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/1f/temp_shuffle_c388f576-76f4-4e4b-b7b4-266de7eca1a6\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0f/temp_shuffle_0f95e4eb-16cb-4ab5-9daa-74607e17a8f5\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/08/temp_shuffle_63ce19a1-154e-4570-87eb-a823647ef9ee\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/02/temp_shuffle_be4de2a2-fbbd-4967-a6e5-d9b38afbd266\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/21/temp_shuffle_bafa8d92-71fa-45fa-868a-2cf7b03a4f21\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/37/temp_shuffle_c68c5232-8152-406a-bd0e-c087f15cbdf1\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/0b/temp_shuffle_80806cfe-a005-43df-b1da-ab20eb44bf74\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/3b/temp_shuffle_4174a793-cd22-42c7-9a77-facb3c43392d\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/08/temp_shuffle_1634fb66-4688-4e2d-bfac-c2399d7bcae6\n",
      "23/05/18 07:08:19 WARN DiskBlockObjectWriter: Error deleting /tmp/blockmgr-cf5c049c-0b2b-4a9b-9181-8ea0eba743c9/08/temp_shuffle_17e9de60-5619-4e54-a07f-df93f4169b5f\n",
      "23/05/18 07:08:19 WARN TaskSetManager: Lost task 19.0 in stage 44.0 (TID 2255) (jupyter-kkanjaria-40ucsd-2eedu executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "# training_df_complex_features = training_df_poincare.join(training_df_entropy,['NumId', 'Chunk'])\n",
    "# training_df_complex_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_df_sleep = ts_features.process_for_sleep(df=training_df_differences)\n",
    "# training_df_sleep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary_stats_complete = os.path.exists('/cephfs/summary_stats/train')\n",
    "\n",
    "if summary_stats_complete == False:\n",
    "    training_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=training_df_chunks)\n",
    "else:\n",
    "    training_features_summary_stats=reading_data.read_in_pyspark_data_for_summary_stats('/cephfs/summary_stats/train')\n",
    "\n",
    "# merge complex features and summary stats and demographics and sleep features\n",
    "# merge in one hot encoded cohort file info demographics\n",
    "    # '/cephfs/data/cohort_encoded.parquet' (gender, treatment, age category)\n",
    "    # groupby patientId and chunk\n",
    "\n",
    "training_features_summary_stats.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #add target variable\n",
    "# training_features_final_summary = summary_stats_features\\\n",
    "#                                     .add_lag_out_of_range(df=training_features_summary_stats, chunk_lag=1)\n",
    "\n",
    "# training_features_final_summary.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge these together\n",
    "# training_features_summary_stats\n",
    "# training_df_complex_features\n",
    "# one-hot-encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complex Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_df_differences = difference_features.add_difference_features(testing_df_added_binary_labels)\n",
    "testing_df_differences.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_df_chunks = summary_stats_features.create_chunk_col(testing_df_differences, chunk_val = 288)\n",
    "testing_df_chunks.show(5)\n",
    "\n",
    "# testing_df_poincare = testing_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.poincare)\n",
    "# testing_df_poincare.show(5)\n",
    "\n",
    "# testing_df_entropy = testing_df_chunks.groupby(['PatientId', 'Chunk']).apply(ts_features.entropy)\n",
    "# testing_df_entropy.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing_df_complex_features = testing_df_poincare.join(testing_df_entropy,['PatientId', 'Chunk'])\n",
    "# testing_df_complex_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_df_sleep = ts_features.process_for_sleep(df=testing_df_added_binary_labels)\n",
    "# training_df_sleep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_features_summary_stats=summary_stats_features.pyspark_summary_statistics(df=testing_df_chunks)\n",
    "\n",
    "# merge complex features and summary stats and demographics and sleep features\n",
    "# merge in one hot encoded cohort file info demographics\n",
    "    # '/cephfs/data/cohort_encoded.parquet' (gender, treatment, age category)\n",
    "    # groupby patientId and chunk\n",
    "\n",
    "testing_features_summary_stats.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge these together\n",
    "# testing_features_summary_stats\n",
    "# training_df_complex_features\n",
    "# one-hot-encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PySpark: Sklearn Regression Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_df=reading_data.read_in_one_hot_encoded_data(one_hot_encoding_location=one_hot_encoding_data)\n",
    "one_hot_encoded_df=one_hot_encoded_df.select('UserId', \n",
    "                                             'Sex_Encoded', \n",
    "                                             'Treatment_Encoded', \n",
    "                                             'AgeGroup_Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_hot_encoded_df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(outputCol=\"features\")\n",
    "vecAssembler.setInputCols([\"Sex_Encoded\", \"Treatment_Encoded\", \"AgeGroup_Encoded\"])\n",
    "\n",
    "test = vecAssembler.transform(one_hot_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "model=pipeline.fit(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_encoded=training_features_summary_stats.join(one_hot_encoded_df,\n",
    "                                                       training_features_summary_stats.PatientId==one_hot_encoded_df.UserId,\n",
    "                                                       \"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_encoded.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge training_features_summary with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_encoded=testing_features_summary_stats.join(one_hot_encoded_df,\n",
    "                                                       testing_features_summary_stats.PatientId==one_hot_encoded_df.UserId,\n",
    "                                                       \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing_encoded.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. PySpark: Sklearn Numerical Pipeline in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.sql.types import DoubleType, FloatType, LongType\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_encoded.select('PatientId').show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "patient_1=training_encoded.filter(training_encoded.PatientId=='8W/rpnb48OMm47W2x4FSkc7+9u2mol061DQuJoMdiK0=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "double_cols=[f.name for f in patient_1.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "float_cols=[f.name for f in patient_1.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "long_cols=[f.name for f in patient_1.schema.fields if isinstance(f.dataType, LongType)]\n",
    "\n",
    "# all_numerical=list(set(double_cols+float_cols))\n",
    "# all_numerical_lags=[x for x in all_numerical if \"lag\" in x]\n",
    "all_numerical=list(set(double_cols+float_cols+long_cols))\n",
    "all_numerical.remove('target')\n",
    "\n",
    "# featureArr = [('scaled_' + f) for f in all_numerical_lags]\n",
    "featureArr = [('scaled_' + f) for f in all_numerical]+['Sex_Encoded', 'Treatment_Encoded', 'AgeGroup_Encoded']\n",
    "# featureArr = [('scaled_' + f) for f in all_numerical]\n",
    "\n",
    "va1 = [VectorAssembler(inputCols=[f], outputCol=('vec_' + f)) for f in all_numerical]\n",
    "ss = [StandardScaler(inputCol='vec_' + f, outputCol='scaled_' + f, withMean=True, withStd=True) for f in all_numerical]\n",
    "\n",
    "va2 = VectorAssembler(inputCols=featureArr, outputCol=\"features\")\n",
    "\n",
    "stages = va1 + ss + [va2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "model=pipeline.fit(training_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.transform(training_encoded).select('features').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.transform(training_encoded).select('features').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "model=pipeline.fit(patient_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.transform(patient_1).select('features').show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_numerical_stages=feature_transformations.numerical_scaling(df=training_encoded)\n",
    "training_numerical_stages=feature_transformations.numerical_scaling(df=training_features_summary_stats)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. PySpark: XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# xgboost_model=create_pyspark_xgboost.xgboost_classifier(ml_df=training_encoded,\n",
    "#                                                         stages=training_numerical_stages,\n",
    "#                                                         model_storage_location=model_storage_location,\n",
    "#                                                         random_seed=random_seed)\n",
    "\n",
    "xgboost_model=create_pyspark_xgboost.xgboost_classifier(ml_df=training_features_summary_stats,\n",
    "                                                        stages=training_numerical_stages,\n",
    "                                                        model_storage_location=model_storage_location,\n",
    "                                                        random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. PySpark: Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. PySpark: Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testing_predictions=model_predictions.create_predictions_with_model(test_df=testing_encoded, \n",
    "#                                                                     model=xgboost_model)\n",
    "# testing_predictions.show(10)\n",
    "\n",
    "testing_predictions=model_predictions.create_predictions_with_model(test_df=training_features_summary_stats, \n",
    "                                                                    model=xgboost_model)\n",
    "testing_predictions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. PySpark: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluation=evaluate_model.regression_evaluation(testing_predictions=testing_predictions, \n",
    "                                                          eval_csv_location=evaluation_metrics_output_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. PySpark: XGBoost Classification Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance_df=feature_importance.\\\n",
    "                        feature_importance_accuracy_gain(xgboost_model=xgboost_model, \n",
    "                                                         feature_importance_storage_location=feature_importance_storage_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_importance_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. PySpark: Feature Importance Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_feature_plot=xgboost_classification_plot.feature_overall_importance_plot(feature_importance_df=feature_importance_df,\n",
    "                                                                                 overall_importance_plot_location=overall_feature_importance_plot_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_feature_plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.PySpark: Local Level Feature Importance --> Shap Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add to reqs if this works\n",
    "! pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgboost_model.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(xgboost_model.stages[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e282f9bbb605e17ec33c138684786f2c6a0e45a19b318784bb5671e7a7934052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
