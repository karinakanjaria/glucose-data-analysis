{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Input_Variables.read_vars import raw_data_storage, \\\n",
    "                                      mfdfa_wins, mfdfa_q_list, mfdfa_rev_seg, mfdfa_pol_order, \\\n",
    "                                      fpcs_min_time\n",
    "\n",
    "from Data_Schema.schema import Pandas_UDF_Data_Schema\n",
    "from Read_In_Data.read_data import Reading_Data\n",
    "from Data_Pipeline.sklearn_pipeline import Sklearn_Pipeline\n",
    "from Time_Series_Features.time_series_feature_creation import TS_Feature_Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark UDF Schema Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_udf_data_schema=Pandas_UDF_Data_Schema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading In Data Files in PySpark and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 18:53:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/02/14 18:53:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: PostDate, IngestionDate, PostId, PostTime, PatientId, Stream, SequenceNumber, TransmitterNumber, ReceiverNumber, RecordedSystemTime, RecordedDisplayTime, RecordedDisplayTimeRaw, TransmitterId, TransmitterTime, GlucoseSystemTime, GlucoseDisplayTime, GlucoseDisplayTimeRaw, Value, Status, TrendArrow, TrendRate, IsBackFilled, InternalStatus, SessionStartTime\n",
      " Schema: PostDate, IngestionDate, PostID, PostTime, PatientId, Stram, SequenceNumber, TransmitterNumber, ReceiverNumber, RecordedSystemTime, RecordedDisplayTime, RecordedDisplayTimeRaw, TransmitterId, TransmitterTime, GlucoseSystemTime, GlucoseDisplayTime, GlucoseDisplayTimeRaw, Value, Status, TrendArrow, TrendRate, IsBackFilled, InternalStatus, SessionStartTime\n",
      "Expected: Stram but found: Stream\n",
      "CSV file: file:///Users/carlosmonsivais/Desktop/capstone_data/ahr414_glucose_sample%20-%20ahr414_glucose_sample.csv\n",
      "[Row(PostDate=datetime.datetime(2022, 9, 14, 0, 0), IngestionDate=datetime.datetime(2022, 9, 14, 0, 0), PostID='fyQ0wOxwB8sthzC75TWKpqdrCMCLq+W1wBg9s7MHVcE=', PostTime=datetime.datetime(2022, 9, 14, 0, 26), PatientId='tHu8WPnIffml5CL+AbOBkXcbFApQnP06KdrHbjinta4=', Stram='Phone7', SequenceNumber='1663115129', TransmitterNumber='XFgG633aV9tw5Gclf1WB6nmt9Wgv4nHJjXfRQIAOfsg=', ReceiverNumber=None, RecordedSystemTime=datetime.datetime(2022, 9, 13, 23, 16, 1), RecordedDisplayTime=datetime.datetime(2022, 9, 13, 19, 16, 1), RecordedDisplayTimeRaw=datetime.datetime(2022, 9, 13, 16, 16, 1, 9000), TransmitterId='XFgG633aV9tw5Gclf1WB6nmt9Wgv4nHJjXfRQIAOfsg=', TransmitterTime='5302874', GlucoseSystemTime=datetime.datetime(2022, 9, 13, 23, 15, 45), GlucoseDisplayTime=datetime.datetime(2022, 9, 13, 19, 15, 45), GlucoseDisplayTimeRaw=datetime.datetime(2022, 9, 13, 16, 15, 45), Value=111.0, Status=None, TrendArrow='Flat', TrendRate=0.20000000298023224, IsBackFilled='FALSE', InternalStatus='6', SessionStartTime='5137335')]\n",
      "    PostDate IngestionDate                                        PostId  \\\n",
      "0 2022-09-14    2022-09-14  fyQ0wOxwB8sthzC75TWKpqdrCMCLq+W1wBg9s7MHVcE=   \n",
      "\n",
      "             PostTime                                     PatientId  Stream  \\\n",
      "0 2022-09-14 00:26:00  tHu8WPnIffml5CL+AbOBkXcbFApQnP06KdrHbjinta4=  Phone7   \n",
      "\n",
      "  SequenceNumber                             TransmitterNumber ReceiverNumber  \\\n",
      "0     1663115129  XFgG633aV9tw5Gclf1WB6nmt9Wgv4nHJjXfRQIAOfsg=            NaN   \n",
      "\n",
      "   RecordedSystemTime  ...   GlucoseSystemTime  GlucoseDisplayTime  \\\n",
      "0 2022-09-13 23:16:01  ... 2022-09-13 23:15:45 2022-09-13 19:15:45   \n",
      "\n",
      "      GlucoseDisplayTimeRaw  Value Status TrendArrow TrendRate  IsBackFilled  \\\n",
      "0 2022-09-13 19:15:45-04:00  111.0    NaN       Flat       0.2         FALSE   \n",
      "\n",
      "  InternalStatus SessionStartTime  \n",
      "0              6          5137335  \n",
      "\n",
      "[1 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "reading_data=Reading_Data(data_location=raw_data_storage)\n",
    "\n",
    "####### PySpark\n",
    "pyspark_df=reading_data.read_in_pyspark()\n",
    "print(pyspark_df.head(1))\n",
    "\n",
    "####### Pandas\n",
    "pandas_df=reading_data.read_in_pandas()\n",
    "print(pandas_df.head(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Pipeline in PySpark and Pure Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 18:53:44 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /Users/carlosmonsivais/Desktop/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/pyspark/sql/pandas/group_ops.py:98: UserWarning:It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/02/14 18:53:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: PostDate, IngestionDate, PostId, PostTime, PatientId, Stream, SequenceNumber, TransmitterNumber, ReceiverNumber, RecordedSystemTime, RecordedDisplayTime, RecordedDisplayTimeRaw, TransmitterId, TransmitterTime, GlucoseSystemTime, GlucoseDisplayTime, GlucoseDisplayTimeRaw, Value, Status, TrendArrow, TrendRate, IsBackFilled, InternalStatus, SessionStartTime\n",
      " Schema: PostDate, IngestionDate, PostID, PostTime, PatientId, Stram, SequenceNumber, TransmitterNumber, ReceiverNumber, RecordedSystemTime, RecordedDisplayTime, RecordedDisplayTimeRaw, TransmitterId, TransmitterTime, GlucoseSystemTime, GlucoseDisplayTime, GlucoseDisplayTimeRaw, Value, Status, TrendArrow, TrendRate, IsBackFilled, InternalStatus, SessionStartTime\n",
      "Expected: Stram but found: Stream\n",
      "CSV file: file:///Users/carlosmonsivais/Desktop/capstone_data/ahr414_glucose_sample%20-%20ahr414_glucose_sample.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Value=-0.7103890776634216, TrendRate=0.2827904522418976, PatientId='tHu8WPnIffml5CL+AbOBkXcbFApQnP06KdrHbjinta4=', GlucoseDisplayTimeRaw=datetime.datetime(2022, 9, 13, 16, 15, 45), TrendArrow=[0, 1, 0, 0, 0, 0, 0])]\n",
      "      Value TrendRate                                     PatientId  \\\n",
      "0 -0.710389   0.28279  tHu8WPnIffml5CL+AbOBkXcbFApQnP06KdrHbjinta4=   \n",
      "\n",
      "      GlucoseDisplayTimeRaw                           TrendArrow  \n",
      "0 2022-09-13 19:15:45-04:00  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pandas_sklearn_pipeline=Sklearn_Pipeline()\n",
    "\n",
    "####### PySpark\n",
    "pyspark_transform_schema=pandas_udf_data_schema.sklearn_pyspark_schema()\n",
    "pyspark_transformations=pandas_sklearn_pipeline.pyspark_sklearn_pipeline(df=pyspark_df, \n",
    "                                                                         output_schema=pyspark_transform_schema)\n",
    "print(pyspark_transformations.head(1))\n",
    "\n",
    "####### Pandas\n",
    "pandas_transformations=pandas_sklearn_pipeline.pandas_transform_features(df=pandas_df)\n",
    "print(pandas_transformations.head(1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multifractal Detrended Fluctuation Analysis, Poincare Analysis, Functional Principal Component Analysis, Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SD1    SD2  SD_ratio\n",
      "1  0.037  0.098     0.373\n",
      "2  0.268  0.281     0.957\n",
      "3  0.052  0.083     0.630\n",
      "    Entropy\n",
      "0  0.267132\n"
     ]
    }
   ],
   "source": [
    "ts_feature_creation=TS_Feature_Creation()\n",
    "\n",
    "# # Issue with fcpaWrapper --> Need to make it loop through every PatientID, also fathon library not working on Mac M1 or M2 chips according to documentation\n",
    "# mfdfa=ts_feature_creation.do_mfdfa(data=pandas_df.Value.to_numpy(), \n",
    "#                                    win_sizes=mfdfa_wins, \n",
    "#                                    q_list=mfdfa_q_list, \n",
    "#                                    rev_seg=mfdfa_rev_seg, \n",
    "#                                    pol_order=mfdfa_pol_order)\n",
    "\n",
    "# Function Works\n",
    "poincare=ts_feature_creation.poincare_wrapper(data=pandas_transformations)\n",
    "print(poincare)\n",
    "\n",
    "# # Issue with fcpaWrapper --> Need to make it loop through every PatientID and how to define minTime variable\n",
    "# functional_principal_component_analysis=ts_feature_creation.fpcaWrapper(rawData=pandas_transformations, minTime=fpcs_min_time)\n",
    "\n",
    "# Fuction Works\n",
    "entropy=ts_feature_creation.entropy_calculation(data=pandas_transformations)\n",
    "print(entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e282f9bbb605e17ec33c138684786f2c6a0e45a19b318784bb5671e7a7934052"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
