{"cells":[{"cell_type":"markdown","id":"eb06510f","metadata":{},"source":["### Install"]},{"cell_type":"code","execution_count":1,"id":"53eb2019","metadata":{"scrolled":true},"outputs":[],"source":["# Install the following if you have not done so, otherwise leave commented\n","# ! pip install EntropyHub"]},{"cell_type":"markdown","id":"42bc83ae","metadata":{},"source":["### Libraries"]},{"cell_type":"code","execution_count":2,"id":"9762b1c1","metadata":{},"outputs":[],"source":["import pyspark\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, StringType, FloatType, TimestampType, ArrayType, IntegerType\n","from pyspark.sql.functions import pandas_udf, PandasUDFType, lit\n","\n","import pandas as pd\n","import numpy as np\n","import EntropyHub as EH\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder"]},{"cell_type":"markdown","id":"ce74fb57","metadata":{},"source":["### Reading Data"]},{"cell_type":"code","execution_count":3,"id":"cfecb4c7","metadata":{},"outputs":[],"source":["class Reading_Data:\n","    \"\"\"\n","    A class used to read in data from a location into a PySpark DataFrame\n","\n","    ...\n","\n","    Attributes\n","    ----------\n","    data_location : str\n","        string location of where data csv is stored\n","\n","    Methods\n","    -------\n","    data_schema()\n","        Returns a Spark Dataframe with the correct schema for our input file\n","    \"\"\"\n","    \n","    \n","    def __init__(self, data_location):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        data_location : str\n","            string location of where data csv is stored\n","        \"\"\"\n","    \n","        self.data_location=data_location\n","\n","        \n","    def data_schema(self):\n","        \"\"\"\n","        Returns the data schema we will use on the input csv\n","\n","        Parameters\n","        ----------\n","        None\n","\n","        Returns\n","        ------\n","        StructType\n","            This is a PySpark data schema\n","        \"\"\"\n","        \n","        glucose_data_schema=StructType([StructField('PostDate', TimestampType(),True),\n","                                        StructField('IngestionDate', TimestampType(),True),\n","                                        StructField('PostID', StringType(),True),\n","                                        StructField('PostTime', TimestampType(), True),\n","                                        StructField('PatientID', StringType(), True),\n","                                        StructField('Stram', StringType(), True),\n","                                        StructField('SequenceNumber', StringType(), True),\n","                                        StructField('TransmitterNumber', StringType(), True),\n","                                        StructField('ReceiverNumber', StringType(), True),                                       \n","                                        StructField('RecordedSystemTime', TimestampType(), True),\n","                                        StructField('RecordedDisplayTime', TimestampType(), True),\n","                                        StructField('RecordedDisplayTimeRaw', TimestampType(), True),\n","                                        StructField('TransmitterId', StringType(), True),\n","                                        StructField('TransmitterTime', StringType(), True),\n","                                        StructField('GlucoseSystemTime', TimestampType(), True),\n","                                        StructField('GlucoseDisplayTime', TimestampType(), True),\n","                                        StructField('GlucoseDisplayTimeRaw', TimestampType(), True),\n","                                        StructField('Value', FloatType(), True),\n","                                        StructField('Status', StringType(), True),\n","                                        StructField('TrendArrow', StringType(), True),\n","                                        StructField('TrendRate', FloatType(), True),\n","                                        StructField('IsBackFilled', StringType(), True),\n","                                        StructField('InternalStatus', StringType(), True),\n","                                        StructField('SessionStartTime', StringType(), True)])\n","        return glucose_data_schema\n","            \n","        \n","    def read_in_data(self):\n","        \"\"\"\n","        Returns the PySpark dataframe that will be used throughout the project\n","\n","        Parameters\n","        ----------\n","        None\n","\n","        Returns\n","        ------\n","        pyspark.sql.dataframe.DataFrame\n","            This is a PySpark Dataframe created based on the data schema and input file specified\n","        \"\"\"        \n","        \n","        spark=SparkSession.builder.master(\"local\"). \\\n","                           appName('Resd_Glucose_Data'). \\\n","                           getOrCreate()\n","        \n","        glucose_data=spark.read.csv(self.data_location, \n","                                    header=True,\n","                                    sep=',', \n","                                    schema=self.data_schema())\n","        \n","        return glucose_data\n","        \n","    "]},{"cell_type":"markdown","id":"d65a30d7","metadata":{},"source":["### Wrapper Functions"]},{"cell_type":"code","execution_count":4,"id":"53d1a3ae","metadata":{},"outputs":[],"source":["# Can convert this into a PySpark process like the Data Transformation class below but need to install packages\n","# in the console directly, will do that in the requirements file, needs to be located in the same place\n","# as the udf()\n","class Statistical_Time_Series_Methods:\n","    \"\"\"\n","    A class used to calculate entropy, fucntion2, function3, function4\n","\n","    ...\n","\n","    Attributes\n","    ----------\n","    glucose_data : pandas.DataFrame\n","        Pandas dataframe with the glucose data\n","\n","    Methods\n","    -------\n","    entropy_calculation()\n","        Returns a pandas dataframe with the entropy value calculated based on the input data.\n","    \"\"\"\n","    \n","    def __init__(self, glucose_data):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        glucose_data : pandas.DataFrame\n","            Pandas dataframe with the glucose data\n","        \"\"\"\n","        \n","        self.glucose_data=glucose_data\n","    \n","    \n","    def entropy_calculation(self):\n","        \"\"\"\n","        Returns a pandas dataframe wit the entropy calculation value\n","\n","        Parameters\n","        ----------\n","        None\n","\n","        Returns\n","        ------\n","        pandas.DataFrame\n","            This is a pandas dataframe with the entropy value\n","        \"\"\"  \n","        \n","        entropy=EH.SampEn(self.glucose_data['Value'].values, m=4)[0][-1]\n","        ent_df=pd.DataFrame()\n","        ent_df['Entropy']=[entropy]\n","\n","        return ent_df\n","\n","    \n","    ### Add other functions here, will create a dataframe out of them once added, also will wrap them in a PySpark \n","    ### pandas_udf wrapper to do the whole thing in PySpark instead of swiching from Pandas to PySpark and so on.\n","    \n","    def calculation2(self):\n","        \n","        return None\n","\n","    \n","    def calculation3(self):\n","        \n","        return None\n","    \n","    \n","    def calculation4(self):\n","        \n","        return None"]},{"cell_type":"markdown","id":"1adcbc2b","metadata":{},"source":["### Data Transformation PySpark --> Sklearn --> PySpark"]},{"cell_type":"code","execution_count":5,"id":"935b6409","metadata":{},"outputs":[],"source":["class Data_Transformations:\n","    \"\"\"\n","    A class used to create the sklearn data pipeline transformation in PySpark while still using a non Pyspark\n","    library (sklearn), we are completing this process fully in PySpark by using the @pandas_udf() PySpark\n","    wrapper. This will be useful when we are able to complete this process using groups such as gender, sex,\n","    etc.. However for now I have assigned a dummy group to let the process run.\n","\n","    ...\n","\n","    Attributes\n","    ----------\n","    glucose_data : pyspark.sql.dataframe.DataFrame\n","        PySpark dataframe with our glucose data from the Reading_Data class above\n","        \n","    transform_schema : StructType\n","        The data schema that will be used on the @pandas_udf() wrapper function when outputting our PySpark\n","        data from the transformed variables pipeline\n","\n","    Methods\n","    -------\n","    sklearn_pipeline()\n","        Returns a PySpark dataframe with transformed values using the sklearn library, however process is distributed\n","        in PySpark resources.\n","    \"\"\"\n","    \n","    def __init__(self, glucose_data):\n","        \"\"\"\n","        Parameters\n","        ----------\n","        glucose_data : pyspark.sql.dataframe.DataFrame\n","            PySpark dataframe with our glucose data from the Reading_Data class above\n","            \n","        transform_schema : StructType\n","            The data schema that will be used on the @pandas_udf() wrapper function when outputting our PySpark\n","            data from the transformed variables pipeline\n","        \"\"\"\n","        \n","        self.glucose_data=glucose_data\n","        self.transform_schema=StructType([StructField('Value', FloatType(),True),\n","                                          StructField('TrendRate', FloatType(),True),\n","                                          StructField('PatientID', StringType(),True),\n","                                          StructField('GlucoseDisplayTimeRaw', TimestampType(),True),\n","                                          StructField('TrendArrow', ArrayType(IntegerType()),True)])\n","    \n","    def sklearn_pipeline(self):\n","        \"\"\"\n","        Returns a PySpark dataframe with the transformed values. The values are transformed in sklearn by using\n","        OneHotEncoding, Imputations, and StandardScaler methods in sklearn, however we are applying this to a \n","        PySpark dataframe without having to convert it into a pandas dataframe. We are doing this by creating a \n","        dummy group, this group function will come into use when we are grouping our data by sex, gender, etc.\n","\n","        Parameters\n","        ----------\n","        None\n","\n","        Returns\n","        ------\n","        pyspark.sql.dataframe.DataFrame\n","            This is a PySpark Dataframe with the transformed values\n","        \"\"\"  \n","        @pandas_udf(self.transform_schema, PandasUDFType.GROUPED_MAP)\n","        def transform_features(pdf):\n","            df=pdf[['PatientID','Value','GlucoseDisplayTimeRaw','TrendArrow','TrendRate']]\n","   \n","            categorical_features=['TrendArrow']\n","            categorical_transformer=Pipeline([('imputer_cat', SimpleImputer(strategy='constant', fill_value=np.nan)),\n","                                              ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n","\n","            numeric_features=['Value', 'TrendRate']\n","            numeric_transformer=Pipeline([('imputer_num', SimpleImputer(strategy='median')),\n","                                          ('scaler', StandardScaler())])\n","\n","            preprocessor=ColumnTransformer([('categorical', categorical_transformer, categorical_features),\n","                                            ('numerical', numeric_transformer, numeric_features)],\n","                                            remainder = 'passthrough')\n","\n","            pipeline=Pipeline([('preprocessing', preprocessor)])\n","\n","            transformed_data_array=pipeline.fit_transform(df)\n","            transformed_data_df=pd.DataFrame(transformed_data_array)\n","\n","            transformed_data_df['combine']=transformed_data_df[[0,1,2,3,4,5,6]].values.tolist()\n","            transformed_data_df=transformed_data_df.drop(transformed_data_df.iloc[:, 0:7],axis = 1)\n","            transformed_data_df.columns=['Value', 'TrendRate', 'PatientID', 'GlucoseDisplayTimeRaw', 'TrendArrow']\n","            \n","            return transformed_data_df\n","        \n","        self.glucose_data=self.glucose_data.withColumn('Group', lit(1))\n","        transformed_data=self.glucose_data.groupby('Group').apply(transform_features)\n","        \n","        return transformed_data"]},{"cell_type":"markdown","id":"4c8f24bf","metadata":{},"source":["### Run Modules"]},{"cell_type":"code","execution_count":6,"id":"74e5ee9c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------------------+-------------------+--------------------+-------------------+--------------------+------+--------------+--------------------+--------------+-------------------+-------------------+----------------------+--------------------+---------------+-------------------+-------------------+---------------------+-----+------+----------+---------+------------+--------------+----------------+\n","|           PostDate|      IngestionDate|              PostID|           PostTime|           PatientID| Stram|SequenceNumber|   TransmitterNumber|ReceiverNumber| RecordedSystemTime|RecordedDisplayTime|RecordedDisplayTimeRaw|       TransmitterId|TransmitterTime|  GlucoseSystemTime| GlucoseDisplayTime|GlucoseDisplayTimeRaw|Value|Status|TrendArrow|TrendRate|IsBackFilled|InternalStatus|SessionStartTime|\n","+-------------------+-------------------+--------------------+-------------------+--------------------+------+--------------+--------------------+--------------+-------------------+-------------------+----------------------+--------------------+---------------+-------------------+-------------------+---------------------+-----+------+----------+---------+------------+--------------+----------------+\n","|2022-09-14 00:00:00|2022-09-14 00:00:00|fyQ0wOxwB8sthzC75...|2022-09-14 00:26:00|tHu8WPnIffml5CL+A...|Phone7|    1663115129|XFgG633aV9tw5Gclf...|          null|2022-09-13 23:16:01|2022-09-13 19:16:01|  2022-09-13 23:16:...|XFgG633aV9tw5Gclf...|        5302874|2022-09-13 23:15:45|2022-09-13 19:15:45|  2022-09-13 23:15:45|111.0|  null|      Flat|      0.2|       FALSE|             6|         5137335|\n","|2022-09-14 00:00:00|2022-09-14 00:00:00|fyQ0wOxwB8sthzC75...|2022-09-14 00:26:00|tHu8WPnIffml5CL+A...|Phone7|    1663115129|XFgG633aV9tw5Gclf...|          null|2022-09-13 23:20:57|2022-09-13 19:20:57|  2022-09-13 23:20:...|XFgG633aV9tw5Gclf...|        5303174|2022-09-13 23:20:45|2022-09-13 19:20:45|  2022-09-13 23:20:45|109.0|  null|      Flat|      0.1|       FALSE|             6|         5137335|\n","|2022-09-14 00:00:00|2022-09-14 00:00:00|fyQ0wOxwB8sthzC75...|2022-09-14 00:26:00|tHu8WPnIffml5CL+A...|Phone7|    1663115129|XFgG633aV9tw5Gclf...|          null|2022-09-13 23:25:57|2022-09-13 19:25:57|  2022-09-13 23:25:...|XFgG633aV9tw5Gclf...|        5303474|2022-09-13 23:25:45|2022-09-13 19:25:45|  2022-09-13 23:25:45|111.0|  null|      Flat|      0.1|       FALSE|             6|         5137335|\n","+-------------------+-------------------+--------------------+-------------------+--------------------+------+--------------+--------------------+--------------+-------------------+-------------------+----------------------+--------------------+---------------+-------------------+-------------------+---------------------+-----+------+----------+---------+------------+--------------+----------------+\n","only showing top 3 rows\n","\n"]}],"source":["# Reading Data Class\n","data_bucket_loc='gs://glucose_data_dse/ahr414_glucose_sample - ahr414_glucose_sample.csv'\n","reading_data=Reading_Data(data_location=data_bucket_loc)\n","glucose_df=reading_data.read_in_data()\n","\n","glucose_df.show(3)"]},{"cell_type":"code","execution_count":7,"id":"3437b7f1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Entropy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.140083</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Entropy\n","0  0.140083"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# Statistical_Time_Series_Methods Class \n","statistical_time_series_methods=Statistical_Time_Series_Methods(glucose_data=glucose_df.toPandas())\n","entropy=statistical_time_series_methods.entropy_calculation()\n","\n","entropy"]},{"cell_type":"code","execution_count":8,"id":"38a78fd6","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/spark/python/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n","  warnings.warn(\n","23/01/24 21:10:26 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","[Stage 4:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+-----------+----------+--------------------+---------------------+--------------------+\n","|      Value| TrendRate|           PatientID|GlucoseDisplayTimeRaw|          TrendArrow|\n","+-----------+----------+--------------------+---------------------+--------------------+\n","| -0.7103891|0.28279045|tHu8WPnIffml5CL+A...|  2022-09-13 23:15:45|[0, 1, 0, 0, 0, 0...|\n","|-0.73639596|0.15356635|tHu8WPnIffml5CL+A...|  2022-09-13 23:20:45|[0, 1, 0, 0, 0, 0...|\n","| -0.7103891|0.15356635|tHu8WPnIffml5CL+A...|  2022-09-13 23:25:45|[0, 1, 0, 0, 0, 0...|\n","+-----------+----------+--------------------+---------------------+--------------------+\n","only showing top 3 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Data_Transformations Class\n","data_transformations=Data_Transformations(glucose_data=glucose_df)\n","dt_1=data_transformations.sklearn_pipeline()\n","dt_1.show(3)"]},{"cell_type":"code","execution_count":null,"id":"8a4f15ec","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}