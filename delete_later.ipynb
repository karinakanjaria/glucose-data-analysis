{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1ff94-8b89-410f-b778-d83999af578d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install xgboost\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb009577-2a0a-46f1-8ecd-9d3a9ec8f48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType, FloatType, LongType\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable \n",
    "\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c1de89-771a-40b7-b71b-914fbe2cb9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 00:58:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"check_files\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bbdd69f-dd5a-42f1-bb62-05465476bb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_location1='/cephfs/summary_stats/train/summary_stats_parquet_0_25.parquet'\n",
    "data_location2='/cephfs/summary_stats/train/summary_stats_parquet_159_25.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d2d459d-4b12-4343-ac03-20a51cebf971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------------+------------------+------+-----+-----+------------------+--------------------+-----------------+------------------+----------+----------+---------------+------+\n",
      "|NumId|Chunk|              Mean|            StdDev|Median|  Min|  Max|      AvgFirstDiff|          AvgSecDiff|     StdFirstDiff|        StdSecDiff|CountAbove|CountBelow|TotalOutOfRange|target|\n",
      "+-----+-----+------------------+------------------+------+-----+-----+------------------+--------------------+-----------------+------------------+----------+----------+---------------+------+\n",
      "|   63|    1|191.84027777777777|23.943651386837775| 190.0|146.0|282.0|0.1597222222222222|-0.00347222222222...|4.784066461561062|3.1072560509995135|        23|         0|             23|     6|\n",
      "+-----+-----+------------------+------------------+------+-----+-----+------------------+--------------------+-----------------+------------------+----------+----------+---------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.read.format('parquet').load(data_location1)\n",
    "df1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7891457d-fcd9-4894-b0eb-c638a42c4bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColumnScaler(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def _transform(self, df):\n",
    "        double_cols=[f.name for f in df.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "        float_cols=[f.name for f in df.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "        long_cols=[f.name for f in df.schema.fields if isinstance(f.dataType, LongType)]\n",
    "\n",
    "        all_numerical=list(set(double_cols+float_cols+long_cols))\n",
    "        all_numerical.remove('target')\n",
    "        \n",
    "        for num_column in all_numerical:\n",
    "            input_col = f\"{num_column}\"\n",
    "            output_col = f\"scaled_{num_column}\"\n",
    "\n",
    "            w = Window.partitionBy('NumId')\n",
    "\n",
    "            mu = mean(input_col).over(w)\n",
    "            sigma = stddev(input_col).over(w)\n",
    "\n",
    "            df=df.withColumn(output_col, (col(input_col) - mu)/(sigma))\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0736ebb8-ba65-4b25-ac0d-37fe2fd67275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "double_cols=[f.name for f in df1.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "float_cols=[f.name for f in df1.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "long_cols=[f.name for f in df1.schema.fields if isinstance(f.dataType, LongType)]\n",
    "\n",
    "all_numerical=list(set(double_cols+float_cols+long_cols))\n",
    "all_numerical.remove('target')\n",
    "\n",
    "featureArr = [('scaled_' + f) for f in all_numerical]\n",
    "\n",
    "columns_scaler=ColumnScaler()\n",
    "\n",
    "va2 = VectorAssembler(inputCols=featureArr, outputCol=\"features\", handleInvalid='skip')\n",
    "\n",
    "stages=[columns_scaler]+[va2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88f2293-4900-4a50-8318-fdedf91ef39c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 00:58:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[00:59:02] task 0 got new rank 0                                    (0 + 1) / 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:59:04] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/glucose-data-analysis/glucose_venv/lib/python3.10/site-packages/xgboost/sklearn.py:782: UserWarning: Loading a native XGBoost model with Scikit-Learn interface.\n",
      "  warnings.warn(\"Loading a native XGBoost model with Scikit-Learn interface.\")\n"
     ]
    }
   ],
   "source": [
    "features_col=\"features\"\n",
    "label_name=\"target\"\n",
    "\n",
    "xgb_regression=SparkXGBRegressor(features_col=features_col, \n",
    "                                  label_col=label_name,\n",
    "                                  random_state=123,\n",
    "                                  use_gpu=False)\n",
    "\n",
    "\n",
    "stages.append(xgb_regression)\n",
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "model=pipeline.fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c586c7b8-5dbc-462d-b8a0-9fe07471a8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x7f4c5a6ce3b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76835dc8-58a9-43d7-81aa-deebfe864064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efeefd-9e62-4677-9fbf-85d017bc2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=spark.read.format('parquet').load(data_location2)\n",
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc157bbb-8478-49c0-9acd-1b39e2ee0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_cols=[f.name for f in df2.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "float_cols=[f.name for f in df2.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "long_cols=[f.name for f in df2.schema.fields if isinstance(f.dataType, LongType)]\n",
    "\n",
    "all_numerical=list(set(double_cols+float_cols+long_cols))\n",
    "all_numerical.remove('target')\n",
    "\n",
    "featureArr = [('scaled_' + f) for f in all_numerical]\n",
    "\n",
    "columns_scaler=ColumnScaler()\n",
    "\n",
    "va2 = VectorAssembler(inputCols=featureArr, outputCol=\"features\", handleInvalid='skip')\n",
    "\n",
    "stages=[columns_scaler]+[va2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
