{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a1ff94-8b89-410f-b778-d83999af578d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached xgboost-1.7.5-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Installing collected packages: numpy, scipy, xgboost\n",
      "Successfully installed numpy-1.24.3 scipy-1.10.1 xgboost-1.7.5\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.0.1 tzdata-2023.3\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 threadpoolctl-3.1.0\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-12.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.9 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.24.3)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-12.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost\n",
    "! pip install pandas\n",
    "! pip install scikit-learn\n",
    "! pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b8d380-1690-42b2-8d5d-2fb4939abb19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cf219d-8bc5-4a4e-bf02-4e63a5392dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08dd8a67-400c-40f5-b669-dfd447737c54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'SparkXGBRegressorModel' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipelineModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetStages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'SparkXGBRegressorModel' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab31a798-1728-4f9d-b220-98a128def056",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de5177-da0d-4c88-92f8-390b6b060666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee7c33-42a3-4e1d-a714-5b9c9a59c865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3bec0-bf9e-4611-886d-cd03f513d104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b6211-a2f9-41bc-855a-c0460a021e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "bst = xgb.Booster()\n",
    "# Loading the model saved in previous snippet\n",
    "bst.load_model(\"/tmp/xgboost-pyspark-model/model/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842c312-1dc1-4d6b-b6be-a214a808d714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb009577-2a0a-46f1-8ecd-9d3a9ec8f48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType, FloatType, LongType\n",
    "from pyspark.sql.functions import col, mean, stddev\n",
    "from pyspark.sql import Window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable \n",
    "\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24c1de89-771a-40b7-b71b-914fbe2cb9ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 02:01:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"check_files\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bbdd69f-dd5a-42f1-bb62-05465476bb0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_location1='/cephfs/summary_stats/train/summary_stats_parquet_42_26.parquet'\n",
    "data_location2='/cephfs/summary_stats/train/summary_stats_parquet_159_25.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0bdd5ab-bab6-4903-a0c4-e3c730652313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_og='/cephfs/interpolation/train/parquet_42_26.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd9900-7ef9-4ea3-93d6-dd23bb05f2ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047a47c-93a3-4972-a1e0-211ef5ca133a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d2d459d-4b12-4343-ac03-20a51cebf971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+------+------+---+---+------------+----------+------------+----------+----------+----------+---------------+------+\n",
      "|NumId|Chunk|Mean|StdDev|Median|Min|Max|AvgFirstDiff|AvgSecDiff|StdFirstDiff|StdSecDiff|CountAbove|CountBelow|TotalOutOfRange|target|\n",
      "+-----+-----+----+------+------+---+---+------------+----------+------------+----------+----------+----------+---------------+------+\n",
      "+-----+-----+----+------+------+---+---+------------+----------+------------+----------+----------+----------+---------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.read.format('parquet').load(data_location1)\n",
    "df1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7891457d-fcd9-4894-b0eb-c638a42c4bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ColumnScaler(Transformer, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    def _transform(self, df):\n",
    "        double_cols=[f.name for f in df.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "        float_cols=[f.name for f in df.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "        long_cols=[f.name for f in df.schema.fields if isinstance(f.dataType, LongType)]\n",
    "\n",
    "        all_numerical=list(set(double_cols+float_cols+long_cols))\n",
    "        all_numerical.remove('target')\n",
    "        \n",
    "        for num_column in all_numerical:\n",
    "            input_col = f\"{num_column}\"\n",
    "            output_col = f\"scaled_{num_column}\"\n",
    "\n",
    "            w = Window.partitionBy('NumId')\n",
    "\n",
    "            mu = mean(input_col).over(w)\n",
    "            sigma = stddev(input_col).over(w)\n",
    "\n",
    "            df=df.withColumn(output_col, (col(input_col) - mu)/(sigma))\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0736ebb8-ba65-4b25-ac0d-37fe2fd67275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "double_cols=[f.name for f in df1.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "float_cols=[f.name for f in df1.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "long_cols=[f.name for f in df1.schema.fields if isinstance(f.dataType, LongType)]\n",
    "\n",
    "all_numerical=list(set(double_cols+float_cols+long_cols))\n",
    "all_numerical.remove('target')\n",
    "\n",
    "featureArr = [('scaled_' + f) for f in all_numerical]\n",
    "\n",
    "columns_scaler=ColumnScaler()\n",
    "\n",
    "va2 = VectorAssembler(inputCols=featureArr, outputCol=\"features\", handleInvalid='skip')\n",
    "\n",
    "stages=[columns_scaler]+[va2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e88f2293-4900-4a50-8318-fdedf91ef39c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[03:15:50] task 0 got new rank 0                                    (0 + 1) / 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03:15:51] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_col=\"features\"\n",
    "label_name=\"target\"\n",
    "\n",
    "xgb_regression=SparkXGBRegressor(features_col=features_col, \n",
    "                                  label_col=label_name,\n",
    "                                  random_state=123,\n",
    "                                  use_gpu=False)\n",
    "\n",
    "\n",
    "stages.append(xgb_regression)\n",
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "model=pipeline.fit(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e88f5e25-68f1-45f3-8782-0b346b3d1673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.write().overwrite().save('Saved_Models/model_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76835dc8-58a9-43d7-81aa-deebfe864064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08efeefd-9e62-4677-9fbf-85d017bc2e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------------+------------------+------+----+-----+--------------------+--------------------+------------------+------------------+----------+----------+---------------+------+\n",
      "|NumId|Chunk|              Mean|            StdDev|Median| Min|  Max|        AvgFirstDiff|          AvgSecDiff|      StdFirstDiff|        StdSecDiff|CountAbove|CountBelow|TotalOutOfRange|target|\n",
      "+-----+-----+------------------+------------------+------+----+-----+--------------------+--------------------+------------------+------------------+----------+----------+---------------+------+\n",
      "|  168|    1|106.13888888888889|14.927285360656931| 106.0|81.0|140.0|-0.00694444444444...|0.017361111111111112|2.1085408789060285|1.5914768571721178|         0|         0|              0|     6|\n",
      "+-----+-----+------------------+------------------+------+----+-----+--------------------+--------------------+------------------+------------------+----------+----------+---------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=spark.read.format('parquet').load(data_location2)\n",
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc157bbb-8478-49c0-9acd-1b39e2ee0276",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_cols=[f.name for f in df2.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "float_cols=[f.name for f in df2.schema.fields if isinstance(f.dataType, FloatType)]\n",
    "long_cols=[f.name for f in df2.schema.fields if isinstance(f.dataType, LongType)]\n",
    "\n",
    "all_numerical=list(set(double_cols+float_cols+long_cols))\n",
    "all_numerical.remove('target')\n",
    "\n",
    "featureArr = [('scaled_' + f) for f in all_numerical]\n",
    "\n",
    "columns_scaler=ColumnScaler()\n",
    "\n",
    "va2 = VectorAssembler(inputCols=featureArr, outputCol=\"features\", handleInvalid='skip')\n",
    "\n",
    "stages=[columns_scaler]+[va2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4f6a88-a816-4f98-ba8b-9c2d009432a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[02:04:52] WARNING: ../src/learner.cc:553:                          (0 + 1) / 1]\n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n",
      "[02:04:54] task 0 got new rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:04:56] WARNING: ../src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "features_col=\"features\"\n",
    "label_name=\"target\"\n",
    "\n",
    "xgb_classifier=SparkXGBRegressor(features_col=features_col, \n",
    "                                 label_col=label_name,\n",
    "                                  random_state=123,\n",
    "                                  use_gpu=False,\n",
    "                                  xgb_model=model.stages[-1].get_booster())\n",
    "\n",
    "\n",
    "\n",
    "stages.append(xgb_classifier)\n",
    "pipeline=Pipeline(stages=stages)\n",
    "\n",
    "model=pipeline.fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2685996f-3c8c-44a2-8671-cb2d1cc37f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 02:05:04 WARN TaskSetManager: Stage 28 contains a task of very large size (1159 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "model.write().overwrite().save('Saved_Models/model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a73bc162-aff6-4e10-b2ee-daed11154357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1%20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f3b200-8b20-4756-b33e-a2d9a5393db2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "40%20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a13c3-1a62-47a8-b1c0-3dbce644df71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
