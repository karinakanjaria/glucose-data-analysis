{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8409e87-c7a1-427d-83b1-7b96c6bfba40",
   "metadata": {},
   "source": [
    "#### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f1bdfe-a8a7-401a-814a-f5a0a572c54a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, \\\n",
    "StringType, IntegerType, TimestampType, DateType, FloatType\n",
    "import time\n",
    "import pathlib\n",
    "from pyspark.sql.functions import col, to_date, sum, avg, max, min, \\\n",
    "stddev, percentile_approx,\\\n",
    "pandas_udf, PandasUDFType, lit, udf, collect_list, sqrt, monotonically_increasing_id, map_from_entries,\\\n",
    "rank, dense_rank, count, when\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08f4f330-522a-41ad-b2e2-e3012d603121",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/19 21:22:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf().setAll([\\\n",
    "            ('spark.master', 'local[*]'),\\\n",
    "            ('spark.app.name', 'Glucose_Analysis_Spark')])\\\n",
    "            .set('spark.sql.shuffle.partitions', '1500')\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .getOrCreate()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78224625-863f-4ac1-a89f-bf9d5f296ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PatientId: string (nullable = true)\n",
      " |-- Value: float (nullable = true)\n",
      " |-- GlucoseDisplayDate: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "  StructField('PatientId', StringType(), True),\n",
    "  StructField('Value', FloatType(), True),\n",
    "  StructField('GlucoseDisplayDate', DateType(), True)\n",
    "  ])\n",
    "\n",
    "emptyRDD = spark.sparkContext.emptyRDD()\n",
    "df = spark.createDataFrame(emptyRDD,schema)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e7fa6eb-5a1a-4943-b706-a8a8e1aa149e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/cephfs/stepped_glucose_data/step0_load/parquet_0_to_10/part-00000-532ee45d-8e0d-44c4-8f3b-884b22175e0f-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa6f93d3-cc66-4759-bf85-69b70d8cd854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.withColumn('y_binary', lit(1))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "799e3b76-b1fc-43f5-b627-1c06ed0a6b68",
   "metadata": {
    "tags": []
   },
   "source": [
    "import numpy as np\n",
    "    \n",
    "# def calculate_poincare(values):\n",
    "#     glucose_differentials = np\n",
    "#.diff(values.collect())\n",
    "\n",
    "#     st_dev_differentials = np.std(np.diff(glucose_differentials))\n",
    "#     st_dev_values = np.std(glucose_differentials)\n",
    "\n",
    "#     # measures the width of poincare cloud\n",
    "#     short_term_variation = (1 / np.sqrt(2)) * st_dev_differentials\n",
    "\n",
    "#     # measures the length of the poincare cloud\n",
    "#     long_term_variation = np.sqrt((2 * st_dev_values ** 2) - (0.5 * st_dev_differentials ** 2))\n",
    "#     return round(short_term_variation, 3), \\\n",
    "#            round(long_term_variation, 3), \\\n",
    "#            round(short_term_variation / long_term_variation, 3)\n",
    "\n",
    "def calculate_poincare(values):\n",
    "    glucose_differentials = values\n",
    "\n",
    "    st_dev_differentials = stddev(glucose_differentials.diff())\n",
    "    st_dev_values = stddev(glucose_differentials)\n",
    "\n",
    "    # measures the width of poincare cloud\n",
    "    short_term_variation = (1 / sqrt(2)) * st_dev_differentials\n",
    "\n",
    "    # measures the length of the poincare cloud\n",
    "    long_term_variation = sqrt((2 * st_dev_values ** 2) - (0.5 * st_dev_differentials ** 2))\n",
    "    return round(short_term_variation, 3), \\\n",
    "           round(long_term_variation, 3), \\\n",
    "           round(short_term_variation / long_term_variation, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ba92081-2c36-4480-8b27-584a71e3b6d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def pyspark_summary_statistics(self, df, spark):\n",
    "# @pandas_udf(StructType([StructField('Entropy', FloatType())]), PandasUDFType.GROUPED_MAP)\n",
    "# def entropy_grouped(df):\n",
    "#     return feat_create_obj.entropy_extraction(df.Value)\n",
    "\n",
    "def entropy_udf(vals):\n",
    "    feat_create_obj = feat_create()\n",
    "    return udf(feat_create_obj.entropy_extraction(vals), FloatType())\n",
    "\n",
    "def poincare_udf(vals):\n",
    "    feat_create_obj = feat_create()\n",
    "    return udf(calculate_poincare(vals),\\\n",
    "               StructType([\\\n",
    "                   StructField('First', FloatType()),\\\n",
    "                   StructField('Second', FloatType()),\\\n",
    "                   StructField('Third', FloatType())\\\n",
    "               ]))\n",
    "\n",
    "# def chunk_by_index():\n",
    "#     return udf(collect_list\n",
    "\n",
    "def create_partition_date(df, chunk_val):\n",
    "    window = Window.partitionBy(df['PatientId']).orderBy(df['GlucoseDisplayTime'])\n",
    "    df = df.select('*', rank().over(window).alias('index'))\n",
    "    df = df.withColumn(\"Chunk\", (df.index/chunk_val).cast(IntegerType()))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def pyspark_summary_statistics(df, \\\n",
    "                               daily_stats_features_lower,\\\n",
    "                               daily_stats_features_upper, \\\n",
    "                               chunk_val = 12):  \n",
    "\n",
    "    df_added = create_partition_date(df, chunk_val)\n",
    "    \n",
    "    group_cols = [\"PatientId\", \"Chunk\"]\n",
    "\n",
    "    summary_df = df_added.groupby(group_cols)\\\n",
    "        .agg(max('y_binary').alias('y_summary_binary'),\\\n",
    "             avg(\"Value\").alias(\"Mean\"),\\\n",
    "             stddev(\"Value\").alias(\"Std Dev\"),\\\n",
    "             percentile_approx(\"Value\", .5).alias(\"Median\"), \\\n",
    "             min(\"Value\").alias(\"Min\"),\\\n",
    "             max(\"Value\").alias(\"Max\"),\\\n",
    "             count(when(col(\"Value\") < daily_stats_features_lower, 1)).alias(\"CountBelow\"),\\\n",
    "             count(when(col(\"Value\") > daily_stats_features_upper, 1)).alias(\"CountAbove\"),\\\n",
    "             (count(when(col(\"Value\") < daily_stats_features_lower, 1))/chunk_val).alias(\"PercentageBelow\"),\\\n",
    "             (count(when(col(\"Value\") > daily_stats_features_upper, 1))/chunk_val).alias(\"PercentageAbove\")\n",
    "            )\n",
    "\n",
    "    df_added = df_added.join(summary_df, ['PatientId', 'Chunk'])\n",
    "    \n",
    "    return df_added"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7976f6a7-b88b-4d21-9823-75f7e21c3d64",
   "metadata": {
    "tags": []
   },
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d847f755-6765-4f01-a54e-63d5f5f7caee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:====>             (1 + 3) / 4][Stage 13:>                 (0 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[336.457s][warning][gc,alloc] Executor task launch worker for task 0.0 in stage 13.0 (TID 27): Retried waiting for GCLocker too often allocating 4194306 words\n",
      "23/04/19 21:27:36 WARN TaskMemoryManager: Failed to allocate a page (33554432 bytes), try again.\n",
      "23/04/19 21:27:37 WARN TaskMemoryManager: Failed to allocate a page (33554432 bytes), try again.\n",
      "[336.604s][warning][gc,alloc] Executor task launch worker for task 0.0 in stage 13.0 (TID 27): Retried waiting for GCLocker too often allocating 4194306 words\n",
      "23/04/19 21:27:37 WARN TaskMemoryManager: Failed to allocate a page (33554432 bytes), try again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+-------------------+---------------------+------------------+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+---------------+\n",
      "|           PatientId|Chunk|Value| GlucoseDisplayTime|GlucoseDisplayTimeRaw|GlucoseDisplayDate|y_binary|index|y_summary_binary|              Mean|           Std Dev|Median|  Min|  Max|CountBelow|CountAbove|PercentageBelow|PercentageAbove|\n",
      "+--------------------+-----+-----+-------------------+---------------------+------------------+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+---------------+\n",
      "|++3L3PAkDvSTkWnWe...|    8|169.0|2022-02-19 01:56:07| 2022-02-19T01:56:...|        2022-02-19|       1|   96|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|170.0|2022-02-19 02:01:07| 2022-02-19T02:01:...|        2022-02-19|       1|   97|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|170.0|2022-02-19 02:06:07| 2022-02-19T02:06:...|        2022-02-19|       1|   98|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|170.0|2022-02-19 02:11:06| 2022-02-19T02:11:...|        2022-02-19|       1|   99|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|169.0|2022-02-19 02:16:07| 2022-02-19T02:16:...|        2022-02-19|       1|  100|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|168.0|2022-02-19 02:21:07| 2022-02-19T02:21:...|        2022-02-19|       1|  101|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|169.0|2022-02-19 02:26:07| 2022-02-19T02:26:...|        2022-02-19|       1|  102|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|170.0|2022-02-19 02:31:07| 2022-02-19T02:31:...|        2022-02-19|       1|  103|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|172.0|2022-02-19 02:36:07| 2022-02-19T02:36:...|        2022-02-19|       1|  104|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|172.0|2022-02-19 02:41:07| 2022-02-19T02:41:...|        2022-02-19|       1|  105|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|165.0|2022-02-19 02:46:07| 2022-02-19T02:46:...|        2022-02-19|       1|  106|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|    8|162.0|2022-02-19 02:51:07| 2022-02-19T02:51:...|        2022-02-19|       1|  107|               1|168.83333333333334| 2.823065172768233| 169.0|162.0|172.0|         0|         0|            0.0|            0.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|220.0|2022-02-19 07:56:08| 2022-02-19T07:56:...|        2022-02-19|       1|  168|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|236.0|2022-02-19 08:01:07| 2022-02-19T08:01:...|        2022-02-19|       1|  169|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|246.0|2022-02-19 08:06:07| 2022-02-19T08:06:...|        2022-02-19|       1|  170|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|248.0|2022-02-19 08:11:08| 2022-02-19T08:11:...|        2022-02-19|       1|  171|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|240.0|2022-02-19 08:16:08| 2022-02-19T08:16:...|        2022-02-19|       1|  172|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|231.0|2022-02-19 08:21:07| 2022-02-19T08:21:...|        2022-02-19|       1|  173|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|228.0|2022-02-19 08:26:08| 2022-02-19T08:26:...|        2022-02-19|       1|  174|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|231.0|2022-02-19 08:31:08| 2022-02-19T08:31:...|        2022-02-19|       1|  175|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|253.0|2022-02-19 08:36:07| 2022-02-19T08:36:...|        2022-02-19|       1|  176|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|245.0|2022-02-19 08:41:08| 2022-02-19T08:41:...|        2022-02-19|       1|  177|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|235.0|2022-02-19 08:46:08| 2022-02-19T08:46:...|        2022-02-19|       1|  178|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   14|238.0|2022-02-19 08:51:07| 2022-02-19T08:51:...|        2022-02-19|       1|  179|               1|237.58333333333334|  9.41428637119075| 236.0|220.0|253.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   45|270.0|2022-05-21 16:31:07| 2022-05-21T16:31:...|        2022-05-21|       1|  540|               1| 271.6666666666667|20.672042311618185| 270.0|235.0|297.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   45|283.0|2022-05-21 16:36:07| 2022-05-21T16:36:...|        2022-05-21|       1|  541|               1| 271.6666666666667|20.672042311618185| 270.0|235.0|297.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   45|294.0|2022-05-21 16:41:07| 2022-05-21T16:41:...|        2022-05-21|       1|  542|               1| 271.6666666666667|20.672042311618185| 270.0|235.0|297.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   45|297.0|2022-05-21 16:46:06| 2022-05-21T16:46:...|        2022-05-21|       1|  543|               1| 271.6666666666667|20.672042311618185| 270.0|235.0|297.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   45|292.0|2022-05-21 16:51:07| 2022-05-21T16:51:...|        2022-05-21|       1|  544|               1| 271.6666666666667|20.672042311618185| 270.0|235.0|297.0|         0|        12|            0.0|            1.0|\n",
      "|++3L3PAkDvSTkWnWe...|   45|287.0|2022-05-21 16:56:07| 2022-05-21T16:56:...|        2022-05-21|       1|  545|               1| 271.6666666666667|20.672042311618185| 270.0|235.0|297.0|         0|        12|            0.0|            1.0|\n",
      "+--------------------+-----+-----+-------------------+---------------------+------------------+--------+-----+----------------+------------------+------------------+------+-----+-----+----------+----------+---------------+---------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[PatientId: string, Chunk: int, Value: float, GlucoseDisplayTime: timestamp, GlucoseDisplayTimeRaw: string, GlucoseDisplayDate: date, y_binary: int, index: int, y_summary_binary: int, Mean: double, Std Dev: double, Median: float, Min: float, Max: float, CountBelow: bigint, CountAbove: bigint, PercentageBelow: double, PercentageAbove: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark_summary_statistics(df, 70, 180, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b14176-2502-4731-8336-ea4c361f480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "added_daily_features=df.groupby(analysis_group).apply(transform_features)\n",
    "\n",
    "    return added_daily_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a6384-4f88-4020-aa90-cc5a0305fb5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb304db0-ff73-4ec0-b390-ac970bf72bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glucose-venv",
   "language": "python",
   "name": "glucose-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
